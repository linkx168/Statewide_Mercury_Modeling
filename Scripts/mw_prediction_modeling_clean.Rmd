---
title: "mw_prediction_modeling"
author: "Denver Link"
date: "2025-12-05"
output: html_document
editor_options: 
  chunk_output_type: console
---

# library
```{r}
library(tidyverse)
library(arrow)
library(LAGOSNE)
library(ggrepel)
library(mwlaxeref)
library(mnsentinellakes)
library(corrplot)
library(performance)
library(brms)
library(lme4)
library(Matrix)
library(car)
library(tidybayes)
library(rstanarm)
library(bayesplot)
library(loo)
library(arrow)
library(sf)
library(maps)
library(viridis)
library(tigris)
library(emmeans)
```

# data
```{r}
mw_hg <- read_csv(file.path("G:",
                            "Shared drives",
                            "Hansen Lab",
                            "RESEARCH PROJECTS",
                            "Statewide mercury modeling",
                            "Predictive Modeling",
                            "Data",
                            "hg_data_comb_12DEC2025.csv"))
```

# landcover exploration 
```{r}
#development?
mw_hg %>% 
  distinct(lagos_id, .keep_all = T) %>% 
  mutate(pct_developed = nlcd_devopen21_pct +
                nlcd_devlow22_pct +
                nlcd_devmed23_pct +
                nlcd_devhi24_pct) %>% 
  filter(!is.na(pct_developed)) %>% 
  summarise(n_10 = sum(pct_developed <=10),
            n_0 = sum(pct_developed == 0), 
            n = n())
#almost 80% of lakes have under 10% development - lets not worry about it

#keep forests seperate?
forest_vars <- mw_hg %>%
   distinct(lagos_id, .keep_all = T) %>% 
   filter(!is.na(nlcd_devhi24_pct)) %>% 
   select(nlcd_fordec41_pct, nlcd_forcon42_pct, nlcd_formix43_pct)
cor(forest_vars)
#they aren't strongly correlated so lets keep them different
rm(forest_vars)


vars_to_check <- c(
  "nlcd_fordec41_pct",
  "nlcd_forcon42_pct",
  "nlcd_formix43_pct",
  "nlcd_wetwood90_pct",
  "nlcd_wetemerg95_pct",
  "nlcd_cultcrop82_pct",
  "nlcd_past81_pct",
  "nlcd_grass71_pct",
  "nlcd_shrub52_pct"
)

mw_hg %>% 
  distinct(lagos_id, .keep_all = TRUE) %>% 
  select(all_of(vars_to_check)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
  filter(!is.na(value)) %>%
  group_by(variable) %>%
  summarise(
    n_10 = sum(value <= 10),
    n_5  = sum(value <= 5),
    n_0  = sum(value == 0),
    n    = n(),
    .groups = "drop"
  )
#forest and wetlands have a good count for the wetlands and will keep
#ag, grass, pasture, shrub all are pretty low and not ecologically relevent 
rm(vars_to_check)
```

# data prep
-select columns needed for modeling and filtering 
-taking lagos ID because that is the level for which is the landcover data
-keep forests separate to capture the diversity across our sampled lakes
```{r}
mw_hg_prep <- mw_hg %>% 
  select(lagos_id,
         lake_area_ha,
         watershed_lake_area_ratio,
         nlcd_fordec41_pct,
         nlcd_forcon42_pct,
         nlcd_formix43_pct,
         nlcd_wetwood90_pct,
         nlcd_wetemerg95_pct,
         state,
         year,
         species,
         sample_type,
         num_fish_in_sample,
         length_in,
         hg_ppm) 
```
Sample types generally all look comparable and samples with more than one fish are smaller species that we should probably keep (at least for now)

# filtering
-most recent years (2000+)
-take half of any sample 0.01 ppm (presumed LOD)
-filter some really big ws lake ratios
-filter for species of interest 
-filter for lakes with landcover data
```{r}
mw_hg_prep <- mw_hg_prep %>% 
  #year filter
  filter(year > 1999) %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(hg_ppm = case_when(hg_ppm <= 0.01 ~ hg_ppm/2,
                           TRUE ~ hg_ppm)) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(watershed_lake_area_ratio < 100) %>% 
  #species of interest (had enough sample size in MN)
  filter(species %in% c("walleye",
                      "northern_pike",
                      "black_crappie",
                      "bluegill",
                      "white_sucker",
                      "yellow_perch",
                      "largemouth_bass")) %>% 
  #only lakes with landcover data
  filter(!is.na(nlcd_fordec41_pct)) %>% 
  #no NA lengths or hg
  filter(!is.na(length_in) & !is.na(hg_ppm))
```

# leave out data
-leave out 15% of species-lake combinations
```{r}
set.seed(10)

#filtering out spec-lake combinations
loo_spec_lakes <- mw_hg_prep %>% 
  distinct(species, lagos_id) %>% 
  sample_frac(0.15)

hg.data.loo.mw <- mw_hg_prep %>% 
  anti_join(loo_spec_lakes, by = c("species", "lagos_id"))

#summary of samples that were left out
mw_hg_prep %>% 
  semi_join(loo_spec_lakes, by = c("species", "lagos_id")) %>% 
  summarise(avg_hg = mean(hg_ppm),
            wae = sum(species == "walleye"),
            nop = sum(species == "northern_pike"),
            spec_else = sum(species != "walleye" & species != "northern_pike"),
            n = n())

mw_hg_prep %>% 
  semi_join(loo_spec_lakes, by = c("species", "lagos_id")) %>% 
  distinct(lagos_id, .keep_all = T) %>% 
  summarise(avg_size = mean(lake_area_ha),
            avg_wetland = mean(nlcd_wetemerg95_pct),
            avg_wetwood = mean(nlcd_wetwood90_pct),
            avg_for_dec = mean(nlcd_fordec41_pct),
            avg_for_con = mean(nlcd_forcon42_pct),
            avg_for_mix = mean(nlcd_formix43_pct),
            n_lakes = n_distinct(lagos_id))

#are there any lakes totally removed?
#if multiple species were removed making removed completely, the lake will show up n times
mw_hg_prep %>% 
  distinct(lagos_id, species) %>% 
  anti_join(hg.data.loo.mw, by = "lagos_id") %>% 
  arrange(lagos_id) %>% 
  print(n = nrow(.))

mw_hg_prep %>% 
  distinct(lagos_id, species) %>% 
  anti_join(hg.data.loo.mw, by = "lagos_id") %>% 
  arrange(lagos_id) %>%
  distinct(lagos_id) %>% 
  print(n = nrow(.))

#how many times was a species removed but at least one still remained?
mw_hg_prep %>%
  distinct(lagos_id, species) %>%
  anti_join(hg.data.loo.mw, by = c("species", "lagos_id")) %>% 
  semi_join(hg.data.loo.mw, by = "lagos_id") 

#write_csv(hg.data.loo.mw, "hg_model_data_loo_mw.csv")
rm(loo_spec_lakes)
```

# saved leave out data
-read in saved fish data with some left out
-anti join to who fish data set that has been prepped
-this generates a dataset of the fish left out of the model fit
```{r}
# write_csv(hg.data.loo.mw, file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Data", "hg_model_data_loo.csv"))

hg.data.loo.mw <- read_csv(file.path("G:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Data", "hg_model_data_loo_mw.csv"))

hg.data.loo_anti <- read_csv(file.path("G:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Data", "hg_model_data_loo_mw.csv")) %>% 
  select(species, year, lagos_id, hg_ppm, length_in)

left_out_data <- mw_hg_prep %>% 
  anti_join(hg.data.loo_anti)

rm(hg.data.loo_anti)
```

# transforming, scaling, and formating variables for modeling 
-scale and make sure scalers are saved for prediction 
-scale length by species
-scale lake-level covarites
   (make sure that we scale for each unique lake, not at the fish level)
-filtering out any fish that are +/- 3 SD 
```{r}
#transform variables
hg.data.loo.mw <- hg.data.loo.mw %>% 
  #first, lets provide 0's in our landcover data with a very small number so they can be transformed to logit scale
  mutate(across(starts_with("nlcd"),
                ~ifelse(.x ==0, 0.001, .x))) %>%
  #now apply logit transformations to landcover and log to others
  mutate(logit_wetwood = car::logit(nlcd_wetwood90_pct),
         logit_wetemerge = car::logit(nlcd_wetemerg95_pct),
         logit_formix = car::logit(nlcd_formix43_pct),
         logit_forcon = car::logit(nlcd_forcon42_pct),
         logit_fordec = car::logit(nlcd_fordec41_pct),
         log_area = log(lake_area_ha),
         log_ws_area_ratio = log(watershed_lake_area_ratio),
         log_hgppm = log(hg_ppm))

#scale lake level data (have to get to one lake per row and then apply to fish data)
lake_level <- hg.data.loo.mw %>% 
  distinct(lagos_id, 
           logit_wetwood,
           logit_wetemerge, 
           logit_formix, 
           logit_forcon,
           logit_fordec,
           log_area,
           log_ws_area_ratio)

lake_scalers <- lake_level %>% 
  summarise(
    mean_log_area = mean(log_area),
    sd_log_area   = sd(log_area),

    mean_logit_wetwood = mean(logit_wetwood),
    sd_logit_wetwood  = sd(logit_wetwood),
    
    mean_logit_wetemerge = mean(logit_wetemerge),
    sd_logit_wetemerge  = sd(logit_wetemerge),

    mean_logit_formix = mean(logit_formix),
    sd_logit_formix   = sd(logit_formix),
    
    mean_logit_forcon = mean(logit_forcon),
    sd_logit_forcon   = sd(logit_forcon),
    
    mean_logit_fordec = mean(logit_fordec),
    sd_logit_fordec   = sd(logit_fordec),

    mean_lw_ratio = mean(log_ws_area_ratio),
    sd_lw_ratio   = sd(log_ws_area_ratio),
  )

#length scalers
length_scalers <- hg.data.loo.mw %>% 
  group_by(species) %>% 
  summarise(
    length_mean = mean(length_in),
    length_sd   = sd(length_in)
  )

######################apply scaling#################################
#lake level
hg.data.loo.mw <- hg.data.loo.mw %>%
    mutate(scaled_log_area = (log_area - lake_scalers$mean_log_area) /lake_scalers$sd_log_area,
           
           scaled_logit_wetwood = (logit_wetwood - lake_scalers$mean_logit_wetwood) / lake_scalers$sd_logit_wetwood,
           
           scaled_logit_wetemerge = (logit_wetemerge - lake_scalers$mean_logit_wetemerge) / lake_scalers$sd_logit_wetemerge,
           
           scaled_logit_formix = (logit_formix - lake_scalers$mean_logit_formix) / lake_scalers$sd_logit_formix,
           
           scaled_logit_forcon = (logit_forcon - lake_scalers$mean_logit_forcon) / lake_scalers$sd_logit_forcon,
           
           scaled_logit_fordec = (logit_fordec - lake_scalers$mean_logit_fordec) / lake_scalers$sd_logit_fordec,
           
           scaled_log_ws_area_ratio = (log_ws_area_ratio - lake_scalers$mean_lw_ratio) / lake_scalers$sd_lw_ratio) 

#length scaler
hg.data.loo.mw <- hg.data.loo.mw %>% 
  left_join(length_scalers, by = "species") %>% 
  mutate(scaled_length = (length_in - length_mean)/ length_sd) %>% 
  select(-length_mean,
         -length_sd)
rm(lake_level)

#now prep left out data to match the scaling used in prediction 
left_out_scaled <- left_out_data %>%
  mutate(across(starts_with("nlcd"),
                ~ifelse(.x ==0, 0.001, .x))) %>%
  mutate(logit_wetwood = car::logit(nlcd_wetwood90_pct),
         logit_wetemerge = car::logit(nlcd_wetemerg95_pct),
         logit_formix = car::logit(nlcd_formix43_pct),
         logit_forcon = car::logit(nlcd_forcon42_pct),
         logit_fordec = car::logit(nlcd_fordec41_pct),
         log_area = log(lake_area_ha),
         log_ws_area_ratio = log(watershed_lake_area_ratio),
         log_hgppm = log(hg_ppm)) %>% 
  left_join(length_scalers, by = "species") %>% 
  mutate(
    
    # fish-level scaling 
    scaled_length = (length_in - length_mean) / length_sd,

    # lake-level predictors
           scaled_log_area = (log_area - lake_scalers$mean_log_area) /lake_scalers$sd_log_area,
           
           scaled_logit_wetwood = (logit_wetwood - lake_scalers$mean_logit_wetwood) / lake_scalers$sd_logit_wetwood,
           
           scaled_logit_wetemerge = (logit_wetemerge - lake_scalers$mean_logit_wetemerge) / lake_scalers$sd_logit_wetemerge,
           
           scaled_logit_formix = (logit_formix - lake_scalers$mean_logit_formix) / lake_scalers$sd_logit_formix,
           
           scaled_logit_forcon = (logit_forcon - lake_scalers$mean_logit_forcon) / lake_scalers$sd_logit_forcon,
           
           scaled_logit_fordec = (logit_fordec - lake_scalers$mean_logit_fordec) / lake_scalers$sd_logit_fordec,
           
           scaled_log_ws_area_ratio = (log_ws_area_ratio - lake_scalers$mean_lw_ratio) / lake_scalers$sd_lw_ratio
  ) 
rm(left_out_data)

hg.data.loo.mw <- hg.data.loo.mw %>% 
  mutate(lagos_id = as.character(lagos_id))

left_out_scaled <- left_out_scaled %>% 
  mutate(lagos_id = as.character(lagos_id))


# filtering out fish +/- 3 SD
hg.data.loo.mw <- hg.data.loo.mw %>% 
  filter(scaled_length >= -3 & scaled_length <= 3)

left_out_scaled <- left_out_scaled %>% 
  filter(scaled_length >= -3 & scaled_length <= 3) %>% 
  select(-length_mean, -length_sd)
```
*We loose 19 spec-lake combos in the training data (hg.data.loo.mw) when trimming for scaled length*
*We loose 2 spec-lake combos in the training data (hg.data.loo.mw) when trimming for scaled length*

# map of data
```{r}
#quick combination for mapping
analysis_total <- bind_rows(left_out_scaled %>% 
                              mutate(data = "Left Out"), 
                            hg.data.loo.mw %>%
                              mutate(data = "Training"))

analysis_total <- analysis_total %>% 
  mutate(data = as.factor(data),
         data = factor(data, levels = c("Training", "Left Out")))

data_for_map <- analysis_total %>% 
  group_by(lagos_id) %>% 
  summarise(n_samples = n(),
            n_spec = n_distinct(species))

lat_lon <-mw_hg %>% 
  distinct(lagos_id, lat_decdeg, lon_decdeg)

data_for_map <- data_for_map %>% 
  left_join(lat_lon %>% 
              mutate(lagos_id = as.character(lagos_id)))
rm(lat_lon)

# making map
states <- st_as_sf(map("state", plot = FALSE, fill = TRUE)) %>%
  filter(ID %in% c("minnesota", "wisconsin"))

data_for_map <- data_for_map %>%
  st_as_sf(coords = c("lon_decdeg", "lat_decdeg"), 
           crs = 4326) 

ggplot() +
  geom_sf(data = states, fill = "white", color = "black") +
  geom_sf(data = data_for_map, aes(size = n_samples, color = n_spec)) +
  scale_size(name = "Number of Samples") +
  scale_color_viridis_c(option = "plasma", name = "Number of Species") +
  theme_minimal() +
  theme(legend.position = "bottom",
        legend.title.align =  0.5,
        legend.key.height = unit(0.3, "cm"),
        legend.key.width  = unit(0.3, "cm"),
        legend.text = element_text(size = 7),
        legend.title = element_text(size = 8))
#ggsave("map_pred_samples.jpg", height = 6, width = 6, dpi = 600)

###############all samples (left out and traning)#############################
data_for_map <- analysis_total %>% 
  mutate(species = case_when(species == "black_crappie" ~ "Black Crappie",
                             species == "bluegill" ~ "Bluegill",
                             species == "largemouth_bass" ~ "Largemouth Bass",
                             species == "northern_pike" ~ "Northern Pike",
                             species == "walleye" ~ "Walleye",
                             species == "yellow_perch" ~ "Yellow Perch",
                             species == "white_sucker" ~ "White Sucker")) %>% 
  group_by(lagos_id, species, data) %>% 
  summarise(n_samples = n())

lat_lon <-mw_hg %>% 
  distinct(lagos_id, lat_decdeg, lon_decdeg)

data_for_map <- data_for_map %>% 
  left_join(lat_lon %>% 
              mutate(lagos_id = as.character(lagos_id)))
rm(lat_lon)

data_for_map <- data_for_map %>%
  st_as_sf(coords = c("lon_decdeg", "lat_decdeg"), 
           crs = 4326) 

ggplot() +
  geom_sf(data = states, fill = "white", color = "black") +
  geom_sf(data = subset(data_for_map, data == "Training"), aes(size = n_samples, color = data), alpha = .7) +
  geom_sf(data = subset(data_for_map, data == "Left Out"), aes(size = n_samples, color = data), alpha = .7) +
  scale_color_manual("Data", values = c("Training" = "navy", 
                                "Left Out" = "orange")) +
  scale_size(name = "Number of Samples") +
  facet_wrap(~species) +
  guides(color = guide_legend(direction = "horizontal"),
         size = guide_legend(direction = "horizontal")) +
  theme_minimal() +
  theme(legend.position = c(.69, .15),
        legend.box = "horiztonal",
    legend.key.size = unit(0.6, "lines"),
    legend.title = element_text(size = 10),
    legend.text  = element_text(size = 8),
    legend.spacing.x = unit(0.4, "cm"),
    legend.spacing.y = unit(0.4, "cm"),
    axis.text.x = element_text(angle = 30, hjust = 1)) 
ggsave("samples_by_spec.jpg", height = 5, width = 5, dpi = 600)
rm(states, data_for_map)
```

# summary of data for modeling
1. how many total lakes were left out
2. partial lakes with no walleye remaining
3. partial lakes with walleye or nop remaining
4. how many lakes in the training data
5. how many spec-lake in the training data
```{r}
#lakes in the training data?
train_lakes <- unique(hg.data.loo.mw$lagos_id)
#see lake count in the environment 

#how many lakes left out?
left_out_scaled <- left_out_scaled %>% 
  mutate(lake_status = case_when(lagos_id %in% train_lakes ~ "partial",
                                 TRUE ~ "new"))

train_species_by_lake <- hg.data.loo.mw %>% 
  group_by(lagos_id) %>% 
  summarise(species_present = list(unique(species)), .groups = "drop")

left_out_scaled <- left_out_scaled %>% 
  left_join(train_species_by_lake, by = "lagos_id") %>% 
  mutate(species_present = replace_na(species_present, list(character())))

left_out_sum <- left_out_scaled %>% 
  group_by(species, lake_status) %>% 
  summarise(n_lakes = n_distinct(lagos_id),
            n = n())
left_out_sum
write_csv(left_out_sum, "left_out_sum.csv")
rm(left_out_sum)

left_out_scaled %>% 
  group_by(lake_status) %>% 
  summarise(n_lakes = n_distinct(lagos_id),
            n = n())

left_out_scaled %>% 
  summarise(n_lakes = n_distinct(lagos_id),
            n = n())

#how many total lake-spec in training data
hg.data.loo.mw %>% 
  distinct(lagos_id, species) %>% 
  count()

left_out_scaled %>%
  distinct(lagos_id, species, .keep_all = T) %>% 
  count()

left_out_scaled %>% 
  distinct(lagos_id) %>% 
  count()

#quick overall summary
counts <- analysis_total %>% 
  group_by(species, data) %>% 
  summarise(n_lakes = n_distinct(lagos_id),
            n_fish = n()) %>% 
  arrange(species)
write_csv(counts, "counts.csv")
rm(counts)

analysis_total %>% 
  group_by(data) %>% 
  summarise(n_lakes = n_distinct(lagos_id),
            n_fish = n())

analysis_total %>% 
  summarise(n_lakes = n_distinct(lagos_id),
            n_fish = n())
  

rm(train_lakes, train_species_by_lake, analysis_total)
```

# fit model
```{r}
# fit <- brm(log_hgppm ~
#                  scaled_length*species +
#                  species +
#                  scaled_logit_wetwood +
#              scaled_logit_wetemerge +
#                  scaled_logit_formix +
#              scaled_logit_forcon +
#              scaled_logit_fordec +
#                  scaled_log_ws_area_ratio +
#                  scaled_log_area +
#                  (1|lagos_id),
#                       data = hg.data.loo.mw,
#                       iter = 10000,
#                       warmup = 500,
#                      chains = 3,
#                       family = gaussian())

# saveRDS(fit, file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output", "mw_random_leave_out.rds"))

fit <- readRDS(file.path("G:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output", "mw_random_leave_out.rds"))
```

# check model fit
```{r}
summary(fit)
#plot(fit)
#conditional_effects(fit)
```


# predictions 
```{r}
predictions <- left_out_scaled %>% 
  add_predicted_draws(fit, ndraws = 500, allow_new_levels = T)

#write_csv(predictions, file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output",  "predictions_left_out_mw.csv"))

predictions <- read_csv(file.path("G:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output",  "predictions_left_out_mw.csv"))
rm(fit)
```

# summary from predictions 
1. We do this on the log scale
2. We do this on the original scale (backtransformed)
```{r}
#summary of the draws for each prediction (on log scale)
predictions_summary <- predictions %>% 
  group_by(lagos_id, hg_ppm, length_in, species, .row) %>%
  summarise(.pred_sd = sd(.prediction),
            .lower = quantile(.prediction, probs = c(0.025)),
            .upper = quantile(.prediction, probs = c(0.975)),
            .prediction = median(.prediction)) %>% 
  mutate(lagos_id = as.character(lagos_id))

#connect these predictions back to the original data
predictions_summary <- left_out_scaled %>% 
  #this is needed due to maybe duplicate spec-length-hgppm-year-lake
  distinct(lagos_id, species, hg_ppm, length_in, species, .keep_all = T) %>% 
  left_join(predictions_summary, by = c("lagos_id", "hg_ppm", "length_in", "species"))

##############################################################################

#summary of predictions in the original scale (back transformed)
predictions_summary_original <- predictions %>% 
  mutate(.prediction = exp(.prediction)) %>% 
  group_by(lagos_id, hg_ppm, length_in, species, .row) %>%
  summarise(.pred_sd = sd(.prediction),
            .lower = quantile(.prediction, probs = c(0.025)),
            .upper = quantile(.prediction, probs = c(0.975)),
            .prediction = median(.prediction)) %>% 
  mutate(lagos_id = as.character(lagos_id))

predictions_summary_original <- left_out_scaled %>% 
  #this is needed due to there being some duplicate spec-length-hgppm-year-lake
  distinct(lagos_id, species, hg_ppm, length_in, species, .keep_all = T) %>% 
  left_join(predictions_summary_original, by = c("lagos_id", "hg_ppm", "length_in", "species"))
```

# predicitons at the lake-species level - ignores fish level variation 
Lets compare how well we predict the observed mean within a species-lake:
1. calculate the mean values within a species-lake (we will use the same length of fish for predictions so we won't have to worry about that)
2. from the predicted values, calculate the mean of value of each spec-lake. This would mean averaging across species, lake, .draw. (grouping by draw gets us each observation within lake-species and thus a mean mercury value for the grouping) - we will get 500 draws of each mean
3. compare the observed mean to the predicted mean
```{r}
# we group by .draw here so that we get an average of all fish in a lake-spec within a draw
#we essentially get 500 estimates of each lake-spec average value across all lengths
lake_species_preds <- predictions %>% 
  group_by(lagos_id, species, .draw) %>% 
  summarise(mean_pred = mean(.prediction))  %>% 
  ungroup() %>% 
  group_by(lagos_id, species) %>% 
  median_qi(mean_pred) %>% 
  select(-.width,
         -.point,
         -.interval) %>% 
  mutate(lagos_id = as.character(lagos_id))

lake_species_obs <- left_out_scaled %>% 
  group_by(lagos_id, species) %>% 
  summarise(mean_obs = mean(log_hg))

lake_species_compare <- left_join(lake_species_preds, lake_species_obs, by = c("lagos_id", "species"))
rm(lake_species_preds, lake_species_obs)


lake_species_metrics <- lake_species_compare %>%
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )
lake_species_metrics

#what about by various groupings?

loo_lakes <- left_out_scaled %>% 
  distinct(lagos_id, species, lake_status, species_present, species_case)

lake_species_compare <- lake_species_compare %>% 
  left_join(loo_lakes)

lake_species_compare %>%
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>%
  group_by(lake_status) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )

lake_species_compare %>%
  filter(lake_status != "new") %>% 
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>% 
  group_by(species_case) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )

lake_species_compare %>% 
ggplot(aes(x = mean_obs, y = mean_pred)) +
  geom_point(alpha = 0.7, size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(
    x = "Observed log(Hg)",
    y = "Predicted log(Hg)",
  ) +
  theme_minimal()

lake_species_compare %>%
  group_by(species) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = log(0.22), linetype = "dashed") +
  facet_wrap(~species, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")

lake_species_compare %>%
  group_by(species) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = log(0.22), linetype = "dashed") +
  facet_grid(species~lake_status, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")

#lets filter for where the partial miss to see how viz how badly 
lake_species_compare %>%
  filter(mean_obs <= .lower | mean_obs >= .upper) %>% 
  group_by(species) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = log(0.22), linetype = "dashed") +
  facet_grid(species~lake_status, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")

###############################################################################
#lets do this on the back transformed scale as well
lake_species_preds <- predictions %>% 
  group_by(lagos_id, species, .draw) %>% 
  summarise(mean_pred = mean(exp(.prediction)))  %>% 
  ungroup() %>% 
  group_by(lagos_id, species) %>% 
  median_qi(mean_pred) %>% 
  select(-.width,
         -.point,
         -.interval) %>% 
  mutate(lagos_id = as.character(lagos_id))

lake_species_obs <- left_out_scaled %>% 
  group_by(lagos_id, species) %>% 
  summarise(mean_obs = mean(hg_ppm))

lake_species_compare <- left_join(lake_species_preds, lake_species_obs, by = c("lagos_id", "species"))
rm(lake_species_preds, lake_species_obs)


lake_species_metrics <- lake_species_compare %>%
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )
lake_species_metrics

#what about by various groupings?
loo_lakes <- left_out_scaled %>% 
  distinct(lagos_id, species, lake_status, species_present, species_case)

lake_species_compare <- lake_species_compare %>% 
  left_join(loo_lakes)

#confusion matrix - overall
lake_species_compare %>% 
  mutate(obs_harmful = case_when(mean_obs >= 0.22 ~ "y",
                                 TRUE ~ "n"),
         pred_harmful = case_when(mean_pred >= .22 ~ "y",
                                  TRUE ~ "n")) %>% 
  group_by(obs_harmful,
           pred_harmful) %>% 
  count()

lake_species_compare %>% 
  filter(lake_status == "new") %>% 
  mutate(obs_harmful = case_when(mean_obs >= 0.22 ~ "y",
                                 TRUE ~ "n"),
         pred_harmful = case_when(mean_pred >= .22 ~ "y",
                                  TRUE ~ "n")) %>% 
  group_by(obs_harmful,
           pred_harmful) %>% 
  count()

lake_species_compare %>% 
  filter(species_case == "partial_no_wae_nop") %>% 
  mutate(obs_harmful = case_when(mean_obs >= 0.22 ~ "y",
                                 TRUE ~ "n"),
         pred_harmful = case_when(mean_pred >= .22 ~ "y",
                                  TRUE ~ "n")) %>% 
  group_by(obs_harmful,
           pred_harmful) %>% 
  count()


metrics <- lake_species_compare %>%
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>%
  group_by(lake_status) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )
metrics

metrics <- lake_species_compare %>%
  filter(lake_status != "new") %>% 
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>% 
  group_by(species_case) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE),
    med_interval_width = median(interval_width)
  )
metrics

lake_species_compare %>% 
ggplot(aes(x = mean_obs, y = mean_pred, color = lake_status)) +
  geom_point(alpha = 0.7, size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(values = c("orange", "navyblue")) +
  labs(
    x = "Observed Hg",
    y = "Predicted Hg",
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
#ggsave("lake_average_scatter.jpg")

lake_species_compare %>%
  group_by(species) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = 0.22, linetype = "dashed") +
  facet_wrap(~species, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")

lake_species_compare %>%
  group_by(species) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = 0.22, linetype = "dashed") +
  facet_grid(species~lake_status, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentration") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")
#ggsave("lake_average_loo_compare.jpg", height = 5, width = 6)

#lets filter for where the partial miss to see how viz how badly 
lake_species_compare %>%
  filter(mean_obs <= .lower | mean_obs >= .upper) %>% 
  group_by(species) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = 0.22, linetype = "dashed") +
  facet_grid(species~lake_status, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")
#ggsave("lake_average_loo_misses.jpg", height = 5, width = 6)

#how many times do we estimate it is over?
#here we look at cases where the actual mean wasn't in the intervals
# we look at when the actual value was over and under the interval
# we look at cases where the predicted value was over 0.22 but it was obs under
# we look at cases where the obs
miss <- lake_species_compare %>%
  summarise(sum_under = sum(mean_obs <= .lower),
            sum_over = sum(mean_obs >= .upper),
            pred_error_over = sum(mean_pred >= 0.22 & mean_obs <= 0.22),
            pred_error_under = sum(mean_pred <= 0.22 & mean_obs >= 0.22),
            n = n(),
            prop_error_over = pred_error_over/n,
            prop_error_under = pred_error_under/n)
miss

#clean up work space
rm(metrics, miss, predictions_summary, predictions_summary_original, lake_species_compare, lake_species_metrics)
```

# hot spot map
1. create dataframe for lakes that we want to predict to 
    -read in dnr covaraite data and join other dataframes
    -filter out any NA values 
    -scale covary to what was modeled
```{r}
########dataframe for lakes we would like to predict########################
#lake link
lake_link <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "lake_link.csv")) %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

#crosswalk for lagos id to nhdhr
xwalk <- read_csv(file.path("D:",
                          "Shared drives",
                          "Hansen Lab",
                          "LAGOS data extraction",
                          "lake_link.csv"))
glimpse(xwalk)

#land use land cover data
lagos_watershed_lulc <- read_csv(file.path("D:",
                          "Shared drives",
                          "Hansen Lab",
                          "LAGOS data extraction",
                          "lagos_watershed_lulc.csv"))
glimpse(lagos_watershed_lulc)

most_recent_landcover <- lagos_watershed_lulc %>% 
  filter(year == 2016) %>% 
  select(-spatial_division,
         -year,
         -datacoveragepct,
         -precision) %>% 
  rename(lagoslakeid = zoneid)
rm(lagos_watershed_lulc)

#watershed attributes data
lagos_watershed <- read_csv(file.path("D:",
                          "Shared drives",
                          "Hansen Lab",
                          "LAGOS data extraction",
                          "lake_watersheds.csv"))
glimpse(lagos_watershed)

#############join lagos data#####################
covary <- lagos_watershed %>% 
  left_join(most_recent_landcover) %>% 
  mutate(state_keep = case_when(str_detect(ws_states, "MN") ~ "y",
                                str_detect(ws_states, "WI") ~ "y",
                                TRUE ~ "n")) %>% 
  filter(state_keep == "y")
rm(lagos_watershed,
   most_recent_landcover)

#lets just grab the lagos id, nhdhr, name, and coords for cross walking
xwalk <- xwalk %>% 
  distinct(lagoslakeid,
         lake_nhdid,
         lake_namegnis,
         lake_namelagos,
         lake_lat_decdeg,
         lake_lon_decdeg)

#grabbing cross walk information via lagos id (really interested in the nhdhr)
covary <- covary %>% 
  left_join(xwalk, relationship = "one-to-one")
rm(xwalk)
#############################################################################
pred_data <- covary %>% 
  select(lagos_id = lagoslakeid,
         ws_focallakewaterarea_ha,
         ws_lake_arearatio,
         nlcd_forcon42_pct,
         nlcd_formix43_pct,
         nlcd_fordec41_pct,
         nlcd_wetemerg95_pct,
         nlcd_wetwood90_pct,
         ws_lat_decdeg,
         ws_lon_decdeg)
rm(covary)
##############################################################################
#use scalers to put lakes in original data scale
pred_data <- pred_data %>% 
  filter(ws_lake_arearatio < 100) %>% 
  filter(!is.na(ws_lake_arearatio)) %>%
  filter(!is.na(nlcd_forcon42_pct)) %>% 
  rename(lake_area_ha = ws_focallakewaterarea_ha,
         watershed_lake_area_ratio = ws_lake_arearatio) %>% 
  mutate(
    #first, lets provide 0's in our landcover data with a very small number so they can be transfomred to logit scale
  mutate(across(starts_with("nlcd"),
                ~ifelse(.x ==0, 0.001, .x))) %>%
  mutate(across(starts_with("nlcd"),
                ~ifelse(.x ==100, 0.999, .x))) %>%  
  #now apply logit transformations to landcover and log to others
  mutate(logit_wetwood = car::logit(nlcd_wetwood90_pct),
         logit_wetemerge = car::logit(nlcd_wetemerg95_pct),
         logit_formix = car::logit(nlcd_formix43_pct),
         logit_forcon = car::logit(nlcd_forcon42_pct),
         logit_fordec = car::logit(nlcd_fordec41_pct),
         log_area = log(lake_area_ha),
         log_ws_area_ratio = log(watershed_lake_area_ratio))
  ) %>% 
  mutate(scaled_log_area = (log_area - lake_scalers$mean_log_area) /lake_scalers$sd_log_area,
           
           scaled_logit_wetwood = (logit_wetwood - lake_scalers$mean_logit_wetwood) / lake_scalers$sd_logit_wetwood,
           
           scaled_logit_wetemerge = (logit_wetemerge - lake_scalers$mean_logit_wetemerge) / lake_scalers$sd_logit_wetemerge,
           
           scaled_logit_formix = (logit_formix - lake_scalers$mean_logit_formix) / lake_scalers$sd_logit_formix,
           
           scaled_logit_forcon = (logit_forcon - lake_scalers$mean_logit_forcon) / lake_scalers$sd_logit_forcon,
           
           scaled_logit_fordec = (logit_fordec - lake_scalers$mean_logit_fordec) / lake_scalers$sd_logit_fordec,
           
           scaled_log_ws_area_ratio = (log_ws_area_ratio - lake_scalers$mean_lw_ratio) / lake_scalers$sd_lw_ratio)

pred_spec <- tibble(species = c("walleye",
                             "northern_pike",
                             "black_crappie",
                             "largemouth_bass",
                             "bluegill",
                             "white_sucker",
                             "yellow_perch"),
                    length = c(15,
                               24,
                               8.1,
                               12,
                               6.1,
                               12,
                               8.1)) %>% 
  left_join(length_scalers,by = "species") %>% 
  mutate(scaled_length = (length - length_mean)/length_sd)
pred_spec

#cross the datasets
pred_full <- crossing(pred_data, pred_spec)
rm(pred_data, pred_spec)
#############################################################################
fit <- readRDS(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output", "mw_random_leave_out.rds"))

#predicting at the lake-spec level 
#####here we can change the number of draws###########
lake_spec_pred <- pred_full %>% 
  add_epred_draws(fit, ndraws = 500, allow_new_levels = TRUE)
rm(fit, pred_full)

#here we have 500 draw for each row because there is only one length within each lake-spec
lake_spec_sum <- lake_spec_pred %>% 
  #prediction in original scale
  mutate(back_transformed = exp(.epred)) %>% 
  group_by(lagos_id, species) %>% 
  summarise(
    log_hgppm = median(.epred),
    log_lower = quantile(.epred, 0.05),
    log_upper = quantile(.epred, 0.95),
    hgppm = median(back_transformed),
    lower = quantile(back_transformed, 0.05),
    upper = quantile(back_transformed, 0.95),
    prob_over = mean(back_transformed > 0.22),
    .groups = "drop"
  ) 
rm(lake_spec_pred)

lake_spec_sum <- lake_spec_sum %>% 
  left_join(lake_link %>% 
              select(lagos_id = lagoslakeid,
                     lake_lat_decdeg,
                     lake_lon_decdeg))
rm(lake_link)

###################making map#############################

lake_sf <- lake_spec_sum %>%
  st_as_sf(coords = c("lake_lon_decdeg", "lake_lat_decdeg"), crs = 4326) %>% 
  mutate(species = case_when(species == "black_crappie" ~ "Black Crappie",
                             species == "bluegill" ~ "Bluegill",
                             species == "largemouth_bass" ~ "Largemouth Bass",
                             species == "northern_pike" ~ "Northern Pike",
                             species == "walleye" ~ "Walleye",
                             species == "yellow_perch" ~ "Yellow Perch",
                             species == "white_sucker" ~ "White Sucker")) 

states <- st_as_sf(map("state", plot = FALSE, fill = TRUE)) %>%
  filter(ID %in% c("minnesota", "wisconsin"))

#map with prob over 0.22
ggplot() +
  geom_sf(data = states, fill = "white", color = "black") +
  geom_sf(
    data = lake_sf,
    aes(
      color = prob_over,
      alpha = .5
    )
  ) +
  facet_wrap(~ species) +
  scale_color_viridis_c(option = "plasma",
                        name = "Probability Over 0.22 ppm") +
  theme_minimal() +
  theme(legend.position = c(.65, .01),
        axis.text.x = element_text(angle = 30, hjust = 1))
#ggsave("prob_map.jpg", height = 5, width = 5, dpi = 600)


#create four grouping for "risk" based on posterior prob over 0.22
lake_spec_sum <- lake_spec_sum %>% 
  mutate(risk = case_when(prob_over >= 0.9 ~ "high",
                          prob_over >= 0.5 & prob_over < 0.9 ~ "worth a check",
                          prob_over >= 0.25 & prob_over < 0.5 ~ "yellow flag",
                          prob_over < 0.25 ~ "low",
                          TRUE ~ NA)) 


###############################################################################
#summary stuff
lake_spec_sum %>% 
  group_by(species) %>% 
  summarise(high = sum(risk == "high"),
            high_med = sum(risk == "worth a check"),
            low_med = sum(risk == "yellow flag"),
            low = sum(risk == "low"),
            n = n())

############################################################################
#map
lake_sf <- lake_spec_sum %>%
  st_as_sf(coords = c("lake_lon_decdeg", "lake_lat_decdeg"), crs = 4326) %>% 
  mutate(risk = as.factor(risk),
         risk = factor(risk, levels = c("high", "worth a check", "yellow flag", "low"))) %>% 
  mutate(species = case_when(species == "black_crappie" ~ "Black Crappie",
                             species == "bluegill" ~ "Bluegill",
                             species == "largemouth_bass" ~ "Largemouth Bass",
                             species == "northern_pike" ~ "Northern Pike",
                             species == "walleye" ~ "Walleye",
                             species == "yellow_perch" ~ "Yellow Perch",
                             species == "white_sucker" ~ "White Sucker")) 

states <- st_as_sf(map("state", plot = FALSE, fill = TRUE)) %>%
  filter(ID %in% c("minnesota", "wisconsin"))

#map with color by grouping
ggplot() +
  geom_sf(
    data = lake_sf,
    aes(
      color = risk,
      shape = risk
    ),
    alpha = .6
  ) +
  scale_shape_manual("Risk for Consumption", values = c(15, 16, 17, 18),
                     labels = c("High", "Worth a Check", "Yellow Flag", "Low")) +
  scale_color_manual("Risk for Consumption", 
                     values = c("#CC79A7", "#D55E00", "#0072B2", "#009E73"),
                     labels = c("High", "Worth a Check", "Yellow Flag", "Low")) +
  geom_sf(data = states, fill = NA, color = "black", alpha = .5) +
  facet_wrap(~ species) +
  theme_minimal() +
  theme(legend.box = "horizontal",
    legend.position = c(.65, .15),
    legend.key.size = unit(0.6, "lines"),
    legend.title = element_text(size = 10),
    legend.text  = element_text(size = 8),
    legend.spacing.x = unit(0.4, "cm"),
    legend.spacing.y = unit(0.4, "cm"),
    axis.text.x = element_text(angle = 30, hjust = 1)
    )
#ggsave("prediciton_map_levels.jpg", height = 5, width = 5, dpi = 600)

############################################################################
lake_spec_sum %>% 
  group_by(species, risk) %>% 
  summarise(n_lakes = n()) %>% 
  mutate(species = case_when(species == "black_crappie" ~ "Black Crappie",
                             species == "bluegill" ~ "Bluegill",
                             species == "largemouth_bass" ~ "Largemouth Bass",
                             species == "northern_pike" ~ "Northern Pike",
                             species == "walleye" ~ "Walleye",
                             species == "yellow_perch" ~ "Yellow Perch",
                             species == "white_sucker" ~ "White Sucker")) %>%  
  mutate(risk = as.factor(risk),
         risk = factor(risk, levels = c("high", "worth a check", "yellow flag", "low"))) %>% 
  ggplot() +
  geom_col(aes(x = risk, y = n_lakes, fill = risk)) +
  labs(y = NULL,
       x = NULL) +
  scale_fill_manual("Consumption Risk",
                    values = c("#CC79A7", "#D55E00", "#0072B2", "#009E73"),
                    labels = c("High", "Worth a Check", "Yellow Flag", "Low")) +
  facet_wrap(~species) +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))
#ggsave("predicted_risk_count.jpg", height = 5, width = 5, dpi = 600)

lake_spec_sum %>% 
  group_by(species, risk) %>% 
  summarise(n_lakes = n()) %>% 
  mutate(species = case_when(species == "black_crappie" ~ "Black Crappie",
                             species == "bluegill" ~ "Bluegill",
                             species == "largemouth_bass" ~ "Largemouth Bass",
                             species == "northern_pike" ~ "Northern Pike",
                             species == "walleye" ~ "Walleye",
                             species == "yellow_perch" ~ "Yellow Perch",
                             species == "white_sucker" ~ "White Sucker")) %>% 
  mutate(risk = as.factor(risk),
         risk = factor(risk, levels = c("high", "worth a check", "yellow flag", "low"))) %>% 
  ggplot() +
  geom_col(aes(x = species, y = n_lakes, fill = risk)) +
  scale_fill_manual("Consumption Risk",
                    values = c("#CC79A7", "#D55E00", "#0072B2", "#009E73"),
                    labels = c("High", "Worth a Check", "Yellow Flag", "Low")) +
  labs(x = NULL,
       y = NULL) +
  theme_minimal() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 25, hjust = 1),
        legend.key.size = unit(0.6, "lines"),
    legend.title = element_text(size = 10),
    legend.text  = element_text(size = 8),
    legend.spacing.x = unit(0.4, "cm"),
    legend.spacing.y = unit(0.4, "cm"))
#ggsave("risk_by_species.jpg", height = 5, width = 5, dpi = 600)
```

#addtional analysis ideas
1. post hoc ZM stuff
  -ZM lakes that were sampled and unsampled, do we predict high risk? 
2. compare predicted levels to state level guidance