---
title: "mw_prediction_modeling"
author: "Denver Link"
date: "2025-12-05"
output: html_document
editor_options: 
  chunk_output_type: console
---

# library
```{r}
library(tidyverse)
library(arrow)
library(LAGOSNE)
library(ggrepel)
library(mwlaxeref)
library(mnsentinellakes)
library(corrplot)
library(performance)
library(brms)
library(lme4)
library(Matrix)
library(car)
library(tidybayes)
library(rstanarm)
library(bayesplot)
library(loo)
library(arrow)
library(sf)
library(maps)
library(viridis)
library(tigris)
library(emmeans)
```

# data
```{r}
mw_hg <- read_csv(file.path("D:",
                            "Shared drives",
                            "Hansen Lab",
                            "RESEARCH PROJECTS",
                            "Statewide mercury modeling",
                            "Predictive Modeling",
                            "Data",
                            "hg_data_comb_12DEC2025.csv"))
```

# landcover exploration 
```{r}
#development?
mw_hg %>% 
  distinct(lagos_id, .keep_all = T) %>% 
  mutate(pct_developed = nlcd_devopen21_pct +
                nlcd_devlow22_pct +
                nlcd_devmed23_pct +
                nlcd_devhi24_pct) %>% 
  filter(!is.na(pct_developed)) %>% 
  summarise(n_10 = sum(pct_developed <=10),
            n_0 = sum(pct_developed == 0), 
            n = n())
#almost 80% of lakes have under 10% development - lets not worry about it

#keep forests seperate?
forest_vars <- mw_hg %>%
   distinct(lagos_id, .keep_all = T) %>% 
   filter(!is.na(nlcd_devhi24_pct)) %>% 
   select(nlcd_fordec41_pct, nlcd_forcon42_pct, nlcd_formix43_pct)
cor(forest_vars)
#they aren't strongly correlated so lets keep them different
rm(forest_vars)


vars_to_check <- c(
  "nlcd_fordec41_pct",
  "nlcd_forcon42_pct",
  "nlcd_formix43_pct",
  "nlcd_wetwood90_pct",
  "nlcd_wetemerg95_pct",
  "nlcd_cultcrop82_pct",
  "nlcd_past81_pct",
  "nlcd_grass71_pct",
  "nlcd_shrub52_pct"
)

mw_hg %>% 
  distinct(lagos_id, .keep_all = TRUE) %>% 
  select(all_of(vars_to_check)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
  filter(!is.na(value)) %>%
  group_by(variable) %>%
  summarise(
    n_10 = sum(value <= 10),
    n_5  = sum(value <= 5),
    n_0  = sum(value == 0),
    n    = n(),
    .groups = "drop"
  )
#forest and wetlands have a good count for the wetlands and will keep
#ag, grass, pasture, shrub all are pretty low and not ecologically relevent 
rm(vars_to_check)
```

# data prep
-select columns needed for modeling and filtering 
-taking lagos ID because that is the level for which is the landcover data
-keep forests separate to capture the diversity across our sampled lakes
```{r}
mw_hg_prep <- mw_hg %>% 
  select(lagos_id,
         lake_area_ha,
         watershed_lake_area_ratio,
         nlcd_fordec41_pct,
         nlcd_forcon42_pct,
         nlcd_formix43_pct,
         nlcd_wetwood90_pct,
         nlcd_wetemerg95_pct,
         state,
         year,
         species,
         sample_type,
         num_fish_in_sample,
         length_in,
         hg_ppm) 
```
Sample types generally all look comparable and samples with more than one fish are smaller species that we should probably keep (at least for now)

# filtering
-most recent years (2000+)
-take half of any sample 0.01 ppm (presumed LOD)
-filter some really big ws lake ratios
-filter for species of interest 
-filter for lakes with landcover data
```{r}
mw_hg_prep <- mw_hg_prep %>% 
  #year filter
  filter(year > 1999) %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(hg_ppm = case_when(hg_ppm <= 0.01 ~ hg_ppm/2,
                           TRUE ~ hg_ppm)) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(watershed_lake_area_ratio < 100) %>% 
  #species of interest (had enough sample size in MN)
  filter(species %in% c("walleye",
                      "northern_pike",
                      "black_crappie",
                      "bluegill",
                      "white_sucker",
                      "yellow_perch",
                      "largemouth_bass")) %>% 
  #only lakes with landcover data
  filter(!is.na(nlcd_fordec41_pct)) %>% 
  #no NA lengths or hg
  filter(!is.na(length_in) & !is.na(hg_ppm))
```

# leave out data
-leave out 15% of species-lake combinations
```{r}
set.seed(10)

#filtering out spec-lake combinations
loo_spec_lakes <- mw_hg_prep %>% 
  distinct(species, lagos_id) %>% 
  sample_frac(0.15)

hg.data.loo.mw <- mw_hg_prep %>% 
  anti_join(loo_spec_lakes, by = c("species", "lagos_id"))

#summary of samples that were left out
mw_hg_prep %>% 
  semi_join(loo_spec_lakes, by = c("species", "lagos_id")) %>% 
  summarise(avg_hg = mean(hg_ppm),
            wae = sum(species == "walleye"),
            nop = sum(species == "northern_pike"),
            spec_else = sum(species != "walleye" & species != "northern_pike"),
            n = n())

mw_hg_prep %>% 
  semi_join(loo_spec_lakes, by = c("species", "lagos_id")) %>% 
  distinct(lagos_id, .keep_all = T) %>% 
  summarise(avg_size = mean(lake_area_ha),
            avg_wetland = mean(nlcd_wetemerg95_pct),
            avg_wetwood = mean(nlcd_wetwood90_pct),
            avg_for_dec = mean(nlcd_fordec41_pct),
            avg_for_con = mean(nlcd_forcon42_pct),
            avg_for_mix = mean(nlcd_formix43_pct),
            n_lakes = n_distinct(lagos_id))

#are there any lakes totally removed?
#if multiple species were removed making removed completely, the lake will show up n times
mw_hg_prep %>% 
  distinct(lagos_id, species) %>% 
  anti_join(hg.data.loo.mw, by = "lagos_id") %>% 
  arrange(lagos_id) %>% 
  print(n = nrow(.))

mw_hg_prep %>% 
  distinct(lagos_id, species) %>% 
  anti_join(hg.data.loo.mw, by = "lagos_id") %>% 
  arrange(lagos_id) %>%
  distinct(lagos_id) %>% 
  print(n = nrow(.))

#how many times was a species removed but at least one still remained?
mw_hg_prep %>%
  distinct(lagos_id, species) %>%
  anti_join(hg.data.loo.mw, by = c("species", "lagos_id")) %>% 
  semi_join(hg.data.loo.mw, by = "lagos_id") 

#write_csv(hg.data.loo.mw, "hg_model_data_loo_mw.csv")
rm(loo_spec_lakes)
```

# saved leave out data
-read in saved fish data with some left out
-anti join to who fish data set that has been prepped
-this generates a dataset of the fish left out of the model fit
```{r}
# write_csv(hg.data.loo.mw, file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Data", "hg_model_data_loo.csv"))

hg.data.loo.mw <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Data", "hg_model_data_loo_mw.csv"))

hg.data.loo_anti <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Data", "hg_model_data_loo_mw.csv")) %>% 
  select(species, year, lagos_id, hg_ppm, length_in)

left_out_data <- mw_hg_prep %>% 
  anti_join(hg.data.loo_anti)

rm(hg.data.loo_anti)
```

# transforming, scaling, and formating variables for modeling 
-scale and make sure scalers are saved for prediction 
-scale length by species
-scale lake-level covarites
   (make sure that we scale for each unique lake, not at the fish level)
-filtering out any fish that are +/- 3 SD 
```{r}
#transform variables
hg.data.loo.mw <- hg.data.loo.mw %>% 
  #first, lets provide 0's in our landcover data with a very small number so they can be transformed to logit scale
  mutate(across(starts_with("nlcd"),
                ~ifelse(.x ==0, 0.001, .x))) %>%
  #now apply logit transformations to landcover and log to others
  mutate(logit_wetwood = car::logit(nlcd_wetwood90_pct),
         logit_wetemerge = car::logit(nlcd_wetemerg95_pct),
         logit_formix = car::logit(nlcd_formix43_pct),
         logit_forcon = car::logit(nlcd_forcon42_pct),
         logit_fordec = car::logit(nlcd_fordec41_pct),
         log_area = log(lake_area_ha),
         log_ws_area_ratio = log(watershed_lake_area_ratio),
         log_hgppm = log(hg_ppm))

#scale lake level data (have to get to one lake per row and then apply to fish data)
lake_level <- hg.data.loo.mw %>% 
  distinct(lagos_id, 
           logit_wetwood,
           logit_wetemerge, 
           logit_formix, 
           logit_forcon,
           logit_fordec,
           log_area,
           log_ws_area_ratio)

lake_scalers <- lake_level %>% 
  summarise(
    mean_log_area = mean(log_area),
    sd_log_area   = sd(log_area),

    mean_logit_wetwood = mean(logit_wetwood),
    sd_logit_wetwood  = sd(logit_wetwood),
    
    mean_logit_wetemerge = mean(logit_wetemerge),
    sd_logit_wetemerge  = sd(logit_wetemerge),

    mean_logit_formix = mean(logit_formix),
    sd_logit_formix   = sd(logit_formix),
    
    mean_logit_forcon = mean(logit_forcon),
    sd_logit_forcon   = sd(logit_forcon),
    
    mean_logit_fordec = mean(logit_fordec),
    sd_logit_fordec   = sd(logit_fordec),

    mean_lw_ratio = mean(log_ws_area_ratio),
    sd_lw_ratio   = sd(log_ws_area_ratio),
  )

#length scalers
length_scalers <- hg.data.loo.mw %>% 
  group_by(species) %>% 
  summarise(
    length_mean = mean(length_in),
    length_sd   = sd(length_in)
  )

######################apply scaling#################################
#lake level
hg.data.loo.mw <- hg.data.loo.mw %>%
    mutate(scaled_log_area = (log_area - lake_scalers$mean_log_area) /lake_scalers$sd_log_area,
           
           scaled_logit_wetwood = (logit_wetwood - lake_scalers$mean_logit_wetwood) / lake_scalers$sd_logit_wetwood,
           
           scaled_logit_wetemerge = (logit_wetemerge - lake_scalers$mean_logit_wetemerge) / lake_scalers$sd_logit_wetemerge,
           
           scaled_logit_formix = (logit_formix - lake_scalers$mean_logit_formix) / lake_scalers$sd_logit_formix,
           
           scaled_logit_forcon = (logit_forcon - lake_scalers$mean_logit_forcon) / lake_scalers$sd_logit_forcon,
           
           scaled_logit_fordec = (logit_fordec - lake_scalers$mean_logit_fordec) / lake_scalers$sd_logit_fordec,
           
           scaled_log_ws_area_ratio = (log_ws_area_ratio - lake_scalers$mean_lw_ratio) / lake_scalers$sd_lw_ratio) 

#length scaler
hg.data.loo.mw <- hg.data.loo.mw %>% 
  left_join(length_scalers, by = "species") %>% 
  mutate(scaled_length = (length_in - length_mean)/ length_sd) %>% 
  select(-length_mean,
         -length_sd)
rm(lake_level)

#now prep left out data to match the scaling used in prediction 
left_out_scaled <- left_out_data %>%
  mutate(across(starts_with("nlcd"),
                ~ifelse(.x ==0, 0.001, .x))) %>%
  mutate(logit_wetwood = car::logit(nlcd_wetwood90_pct),
         logit_wetemerge = car::logit(nlcd_wetemerg95_pct),
         logit_formix = car::logit(nlcd_formix43_pct),
         logit_forcon = car::logit(nlcd_forcon42_pct),
         logit_fordec = car::logit(nlcd_fordec41_pct),
         log_area = log(lake_area_ha),
         log_ws_area_ratio = log(watershed_lake_area_ratio),
         log_hgppm = log(hg_ppm)) %>% 
  left_join(length_scalers, by = "species") %>% 
  mutate(
    
    # fish-level scaling 
    scaled_length = (length_in - length_mean) / length_sd,

    # lake-level predictors
           scaled_log_area = (log_area - lake_scalers$mean_log_area) /lake_scalers$sd_log_area,
           
           scaled_logit_wetwood = (logit_wetwood - lake_scalers$mean_logit_wetwood) / lake_scalers$sd_logit_wetwood,
           
           scaled_logit_wetemerge = (logit_wetemerge - lake_scalers$mean_logit_wetemerge) / lake_scalers$sd_logit_wetemerge,
           
           scaled_logit_formix = (logit_formix - lake_scalers$mean_logit_formix) / lake_scalers$sd_logit_formix,
           
           scaled_logit_forcon = (logit_forcon - lake_scalers$mean_logit_forcon) / lake_scalers$sd_logit_forcon,
           
           scaled_logit_fordec = (logit_fordec - lake_scalers$mean_logit_fordec) / lake_scalers$sd_logit_fordec,
           
           scaled_log_ws_area_ratio = (log_ws_area_ratio - lake_scalers$mean_lw_ratio) / lake_scalers$sd_lw_ratio
  ) 
rm(left_out_data)

hg.data.loo.mw <- hg.data.loo.mw %>% 
  mutate(lagos_id = as.character(lagos_id))

left_out_scaled <- left_out_scaled %>% 
  mutate(lagos_id = as.character(lagos_id))


# filtering out fish +/- 3 SD
hg.data.loo.mw <- hg.data.loo.mw %>% 
  filter(scaled_length >= -3 & scaled_length <= 3)

left_out_scaled <- left_out_scaled %>% 
  filter(scaled_length >= -3 & scaled_length <= 3) %>% 
  select(-length_mean, -length_sd)
```
*We loose 19 spec-lake combos in the training data (hg.data.loo.mw) when trimming for scaled length*
*We loose 2 spec-lake combos in the training data (hg.data.loo.mw) when trimming for scaled length*

# map of data
```{r}
#quick combination for mapping
analysis_total <- bind_rows(left_out_scaled %>% 
                              mutate(data = "Left Out"), 
                            hg.data.loo.mw %>%
                              mutate(data = "Training"))

analysis_total <- analysis_total %>% 
  mutate(data = as.factor(data),
         data = factor(data, levels = c("Training", "Left Out")))

data_for_map <- analysis_total %>% 
  group_by(lagos_id) %>% 
  summarise(n_samples = n(),
            n_spec = n_distinct(species))

lat_lon <-mw_hg %>% 
  distinct(lagos_id, lat_decdeg, lon_decdeg)

data_for_map <- data_for_map %>% 
  left_join(lat_lon %>% 
              mutate(lagos_id = as.character(lagos_id)))
rm(lat_lon)

# making map
states <- st_as_sf(map("state", plot = FALSE, fill = TRUE)) %>%
  filter(ID %in% c("minnesota", "wisconsin"))

data_for_map <- data_for_map %>%
  st_as_sf(coords = c("lon_decdeg", "lat_decdeg"), 
           crs = 4326) 

ggplot() +
  geom_sf(data = states, fill = "white", color = "black") +
  geom_sf(data = data_for_map, aes(size = n_samples, color = n_spec)) +
  scale_size(name = "Number of Samples") +
  scale_color_viridis_c(option = "plasma", name = "Number of Species") +
  theme_minimal() +
  theme(legend.position = "bottom",
        legend.title.align =  0.5,
        legend.key.height = unit(0.3, "cm"),
        legend.key.width  = unit(0.3, "cm"),
        legend.text = element_text(size = 7),
        legend.title = element_text(size = 8))
#ggsave("map_pred_samples.jpg", height = 6, width = 6, dpi = 600)

###############all samples (left out and traning)#############################
data_for_map <- analysis_total %>% 
  mutate(species = case_when(species == "black_crappie" ~ "Black Crappie",
                             species == "bluegill" ~ "Bluegill",
                             species == "largemouth_bass" ~ "Largemouth Bass",
                             species == "northern_pike" ~ "Northern Pike",
                             species == "walleye" ~ "Walleye",
                             species == "yellow_perch" ~ "Yellow Perch",
                             species == "white_sucker" ~ "White Sucker")) %>% 
  group_by(lagos_id, species, data) %>% 
  summarise(n_samples = n())

lat_lon <-mw_hg %>% 
  distinct(lagos_id, lat_decdeg, lon_decdeg)

data_for_map <- data_for_map %>% 
  left_join(lat_lon %>% 
              mutate(lagos_id = as.character(lagos_id)))
rm(lat_lon)

data_for_map <- data_for_map %>%
  st_as_sf(coords = c("lon_decdeg", "lat_decdeg"), 
           crs = 4326) 

ggplot() +
  geom_sf(data = states, fill = "white", color = "black") +
  geom_sf(data = subset(data_for_map, data == "Training"), aes(size = n_samples, color = data), alpha = .7) +
  geom_sf(data = subset(data_for_map, data == "Left Out"), aes(size = n_samples, color = data), alpha = .7) +
  scale_color_manual("Data", values = c("Training" = "navy", 
                                "Left Out" = "orange")) +
  scale_size(name = "Number of Samples") +
  facet_wrap(~species) +
  guides(color = guide_legend(direction = "horizontal"),
         size = guide_legend(direction = "horizontal")) +
  theme_minimal() +
  theme(legend.position = c(.69, .15),
        legend.box = "horiztonal",
    legend.key.size = unit(0.6, "lines"),
    legend.title = element_text(size = 10),
    legend.text  = element_text(size = 8),
    legend.spacing.x = unit(0.4, "cm"),
    legend.spacing.y = unit(0.4, "cm"),
    axis.text.x = element_text(angle = 30, hjust = 1)) 
#ggsave("samples_by_spec.jpg", height = 5, width = 5, dpi = 600)
rm(states, data_for_map)
```

# summary of data for modeling
1. how many total lakes were left out
2. partial lakes with no walleye remaining
3. partial lakes with walleye or nop remaining
4. how many lakes in the training data
5. how many spec-lake in the training data
```{r}
#lakes in the training data?
train_lakes <- unique(hg.data.loo.mw$lagos_id)
#see lake count in the environment 

#how many lakes left out?
left_out_scaled <- left_out_scaled %>% 
  mutate(lake_status = case_when(lagos_id %in% train_lakes ~ "partial",
                                 TRUE ~ "new"))

train_species_by_lake <- hg.data.loo.mw %>% 
  group_by(lagos_id) %>% 
  summarise(species_present = list(unique(species)), .groups = "drop")

left_out_scaled <- left_out_scaled %>% 
  left_join(train_species_by_lake, by = "lagos_id") %>% 
  mutate(species_present = replace_na(species_present, list(character())))

left_out_sum <- left_out_scaled %>% 
  group_by(species, lake_status) %>% 
  summarise(n_lakes = n_distinct(lagos_id),
            n = n())
left_out_sum
#write_csv(left_out_sum, "left_out_sum.csv")
rm(left_out_sum)

left_out_scaled %>% 
  group_by(lake_status) %>% 
  summarise(n_lakes = n_distinct(lagos_id),
            n = n())

left_out_scaled %>% 
  summarise(n_lakes = n_distinct(lagos_id),
            n = n())

#how many total lake-spec in training data
hg.data.loo.mw %>% 
  distinct(lagos_id, species) %>% 
  count()

left_out_scaled %>%
  distinct(lagos_id, species, .keep_all = T) %>% 
  count()

left_out_scaled %>% 
  distinct(lagos_id) %>% 
  count()

#quick overall summary
counts <- analysis_total %>% 
  group_by(species, data) %>% 
  summarise(n_lakes = n_distinct(lagos_id),
            n_fish = n()) %>% 
  arrange(species)
#write_csv(counts, "counts.csv")
rm(counts)

analysis_total %>% 
  group_by(data) %>% 
  summarise(n_lakes = n_distinct(lagos_id),
            n_fish = n())

analysis_total %>% 
  summarise(n_lakes = n_distinct(lagos_id),
            n_fish = n())
  

rm(train_lakes, train_species_by_lake)
```

# fit model
```{r}
# fit <- brm(log_hgppm ~
#                  scaled_length*species +
#                  species +
#                  scaled_logit_wetwood +
#              scaled_logit_wetemerge +
#                  scaled_logit_formix +
#              scaled_logit_forcon +
#              scaled_logit_fordec +
#                  scaled_log_ws_area_ratio +
#                  scaled_log_area +
#                  (1|lagos_id),
#                       data = hg.data.loo.mw,
#                       iter = 10000,
#                       warmup = 500,
#                      chains = 3,
#                       family = gaussian())

# saveRDS(fit, file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output", "mw_random_leave_out.rds"))

fit <- readRDS(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output", "mw_random_leave_out.rds"))
```

# check model fit
```{r}
summary(fit)
#plot(fit)
#conditional_effects(fit)
```


# predictions 
```{r}
predictions <- left_out_scaled %>% 
  add_predicted_draws(fit, ndraws = 500, allow_new_levels = T)

#write_csv(predictions, file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output",  "predictions_left_out_mw.csv"))

predictions <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output",  "predictions_left_out_mw.csv"))
rm(fit)
```


# predicitons at the lake-species level - ignores fish level variation 

Lets compare how well we predict the observed mean within a species-lake:
1. calculate the mean values within a species-lake (we will use the same length of fish for predictions so we won't have to worry about that)
2. from the predicted values, calculate the mean of value of each spec-lake. This would mean averaging across species, lake, .draw. (grouping by draw gets us each observation within lake-species and thus a mean mercury value for the grouping) - we will get 500 draws of each mean
3. compare the observed mean to the predicted mean
```{r}
# we group by .draw here so that we get an average of all fish in a lake-spec within a draw
#we essentially get 500 estimates of each lake-spec average value across all lengths
#lets do this on the back transformed scale 
lake_species_preds <- predictions %>% 
  group_by(lagos_id, species, .draw) %>% 
  summarise(mean_pred = mean(exp(.prediction)))  %>% 
  ungroup() %>% 
  group_by(lagos_id, species) %>% 
  median_qi(mean_pred) %>% 
  select(-.width,
         -.point,
         -.interval) %>% 
  mutate(lagos_id = as.character(lagos_id))

lake_species_obs <- left_out_scaled %>% 
  group_by(lagos_id, species) %>% 
  summarise(mean_obs = mean(hg_ppm))

lake_species_compare <- left_join(lake_species_preds, lake_species_obs, by = c("lagos_id", "species"))
rm(lake_species_preds, lake_species_obs)


lake_species_metrics <- lake_species_compare %>%
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )
lake_species_metrics

#what about by various groupings?
loo_lakes <- left_out_scaled %>% 
  distinct(lagos_id, species, lake_status, species_present)

lake_species_compare <- lake_species_compare %>% 
  left_join(loo_lakes)

#confusion matrix - overall
lake_species_compare %>% 
  mutate(obs_harmful = case_when(mean_obs >= 0.22 ~ "y",
                                 TRUE ~ "n"),
         pred_harmful = case_when(mean_pred >= .22 ~ "y",
                                  TRUE ~ "n")) %>% 
  group_by(obs_harmful,
           pred_harmful) %>% 
  count() %>% 
  ungroup() %>% 
  mutate(total_lakes = sum(n),
         prop = n/total_lakes)
  

lake_species_compare %>% 
  mutate(obs_harmful = case_when(mean_obs >= 0.22 ~ "y",
                                 TRUE ~ "n"),
         pred_harmful = case_when(mean_pred >= .22 ~ "y",
                                  TRUE ~ "n")) %>% 
  group_by(obs_harmful,
           pred_harmful) %>% 
  count() %>% 
  mutate(correct = case_when(obs_harmful == "y" & pred_harmful == "y" ~ "y",
                             obs_harmful == "n" & pred_harmful == "n" ~ "y",
                             TRUE ~ "n")) %>% 
  group_by(correct) %>% 
  summarise(total = sum(n)) %>% 
  mutate(total_lakes = sum(total),
         prop = total/total_lakes)

# by lake status
lake_species_compare %>% 
  group_by(lake_status) %>% 
  mutate(obs_harmful = case_when(mean_obs >= 0.22 ~ "y",
                                 TRUE ~ "n"),
         pred_harmful = case_when(mean_pred >= .22 ~ "y",
                                  TRUE ~ "n")) %>% 
  group_by(obs_harmful,
           pred_harmful,
           lake_status) %>% 
  count() %>% 
  group_by(lake_status) %>% 
  mutate(total_lakes = sum(n),
         prop = n/total_lakes)

lake_species_compare %>% 
  group_by(lake_status) %>% 
  mutate(obs_harmful = case_when(mean_obs >= 0.22 ~ "y",
                                 TRUE ~ "n"),
         pred_harmful = case_when(mean_pred >= .22 ~ "y",
                                  TRUE ~ "n")) %>% 
  group_by(obs_harmful,
           pred_harmful,
           lake_status) %>% 
  count() %>% 
  group_by(lake_status) %>% 
mutate(correct = case_when(obs_harmful == "y" & pred_harmful == "y" ~ "y",
                             obs_harmful == "n" & pred_harmful == "n" ~ "y",
                             TRUE ~ "n")) %>% 
  group_by(lake_status, correct) %>% 
  summarise(total = sum(n)) %>% 
  mutate(total_lakes = sum(total),
         prop = total/total_lakes)

metrics <- lake_species_compare %>%
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>%
  group_by(lake_status) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )
metrics


lake_species_compare %>% 
ggplot(aes(x = mean_obs, y = mean_pred, color = lake_status)) +
  geom_point(alpha = 0.7, size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(values = c("orange", "navyblue")) +
  labs(
    x = "Observed Hg",
    y = "Predicted Hg",
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
#ggsave("lake_average_scatter.jpg")

lake_species_compare %>%
  group_by(species) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = 0.22, linetype = "dashed") +
  facet_wrap(~species, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")

lake_species_compare %>%
  group_by(species) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = 0.22, linetype = "dashed") +
  facet_grid(species~lake_status, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentration") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")
#ggsave("lake_average_loo_compare.jpg", height = 5, width = 6)

#lets filter for where the partial miss to see how viz how badly 
lake_species_compare %>%
  filter(mean_obs <= .lower | mean_obs >= .upper) %>% 
  group_by(species) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = 0.22, linetype = "dashed") +
  facet_grid(species~lake_status, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")
#ggsave("lake_average_loo_misses.jpg", height = 5, width = 6)

#how many times do we estimate it is over?
#here we look at cases where the actual mean wasn't in the intervals
# we look at when the actual value was over and under the interval
# we look at cases where the predicted value was over 0.22 but it was obs under
# we look at cases where the obs
miss <- lake_species_compare %>%
  summarise(sum_under = sum(mean_obs <= .lower),
            sum_over = sum(mean_obs >= .upper),
            pred_error_over = sum(mean_pred >= 0.22 & mean_obs <= 0.22),
            pred_error_under = sum(mean_pred <= 0.22 & mean_obs >= 0.22),
            n = n(),
            prop_error_over = pred_error_over/n,
            prop_error_under = pred_error_under/n)
miss

#clean up work space
rm(metrics, miss, predictions_summary, predictions_summary_original, lake_species_compare, lake_species_metrics)
```

# hot spot map
1. create dataframe for lakes that we want to predict to 
    -read in dnr covaraite data and join other dataframes
    -filter out any NA values 
    -scale covary to what was modeled
```{r}
########dataframe for lakes we would like to predict########################
#lake link
lake_link <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "lake_link.csv")) %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

#crosswalk for lagos id to nhdhr
xwalk <- read_csv(file.path("D:",
                          "Shared drives",
                          "Hansen Lab",
                          "LAGOS data extraction",
                          "lake_link.csv"))
glimpse(xwalk)

#land use land cover data
lagos_watershed_lulc <- read_csv(file.path("D:",
                          "Shared drives",
                          "Hansen Lab",
                          "LAGOS data extraction",
                          "lagos_watershed_lulc.csv"))
glimpse(lagos_watershed_lulc)

most_recent_landcover <- lagos_watershed_lulc %>% 
  filter(year == 2016) %>% 
  select(-spatial_division,
         -year,
         -datacoveragepct,
         -precision) %>% 
  rename(lagoslakeid = zoneid)
rm(lagos_watershed_lulc)

#watershed attributes data
lagos_watershed <- read_csv(file.path("D:",
                          "Shared drives",
                          "Hansen Lab",
                          "LAGOS data extraction",
                          "lake_watersheds.csv"))
glimpse(lagos_watershed)

#############join lagos data#####################
covary <- lagos_watershed %>% 
  left_join(most_recent_landcover) %>% 
  mutate(state_keep = case_when(str_detect(ws_states, "MN") ~ "y",
                                str_detect(ws_states, "WI") ~ "y",
                                TRUE ~ "n")) %>% 
  filter(state_keep == "y")
rm(lagos_watershed,
   most_recent_landcover)

#lets just grab the lagos id, nhdhr, name, and coords for cross walking
xwalk <- xwalk %>% 
  distinct(lagoslakeid,
         lake_nhdid,
         lake_namegnis,
         lake_namelagos,
         lake_lat_decdeg,
         lake_lon_decdeg)

#grabbing cross walk information via lagos id (really interested in the nhdhr)
covary <- covary %>% 
  left_join(xwalk, relationship = "one-to-one")
rm(xwalk)
#############################################################################
pred_data <- covary %>% 
  select(lagos_id = lagoslakeid,
         ws_focallakewaterarea_ha,
         ws_lake_arearatio,
         nlcd_forcon42_pct,
         nlcd_formix43_pct,
         nlcd_fordec41_pct,
         nlcd_wetemerg95_pct,
         nlcd_wetwood90_pct,
         ws_lat_decdeg,
         ws_lon_decdeg)
rm(covary)
##############################################################################
#use scalers to put lakes in original data scale
pred_data <- pred_data %>% 
  filter(ws_lake_arearatio < 100) %>% 
  filter(!is.na(ws_lake_arearatio)) %>%
  filter(!is.na(nlcd_forcon42_pct)) %>% 
  rename(lake_area_ha = ws_focallakewaterarea_ha,
         watershed_lake_area_ratio = ws_lake_arearatio) %>% 
  mutate(
    #first, lets provide 0's in our landcover data with a very small number so they can be transfomred to logit scale
  mutate(across(starts_with("nlcd"),
                ~ifelse(.x ==0, 0.001, .x))) %>%
  mutate(across(starts_with("nlcd"),
                ~ifelse(.x ==100, 0.999, .x))) %>%  
  #now apply logit transformations to landcover and log to others
  mutate(logit_wetwood = car::logit(nlcd_wetwood90_pct),
         logit_wetemerge = car::logit(nlcd_wetemerg95_pct),
         logit_formix = car::logit(nlcd_formix43_pct),
         logit_forcon = car::logit(nlcd_forcon42_pct),
         logit_fordec = car::logit(nlcd_fordec41_pct),
         log_area = log(lake_area_ha),
         log_ws_area_ratio = log(watershed_lake_area_ratio))
  ) %>% 
  mutate(scaled_log_area = (log_area - lake_scalers$mean_log_area) /lake_scalers$sd_log_area,
           
           scaled_logit_wetwood = (logit_wetwood - lake_scalers$mean_logit_wetwood) / lake_scalers$sd_logit_wetwood,
           
           scaled_logit_wetemerge = (logit_wetemerge - lake_scalers$mean_logit_wetemerge) / lake_scalers$sd_logit_wetemerge,
           
           scaled_logit_formix = (logit_formix - lake_scalers$mean_logit_formix) / lake_scalers$sd_logit_formix,
           
           scaled_logit_forcon = (logit_forcon - lake_scalers$mean_logit_forcon) / lake_scalers$sd_logit_forcon,
           
           scaled_logit_fordec = (logit_fordec - lake_scalers$mean_logit_fordec) / lake_scalers$sd_logit_fordec,
           
           scaled_log_ws_area_ratio = (log_ws_area_ratio - lake_scalers$mean_lw_ratio) / lake_scalers$sd_lw_ratio)

pred_spec <- tibble(species = c("walleye",
                             "northern_pike",
                             "black_crappie",
                             "largemouth_bass",
                             "bluegill",
                             "white_sucker",
                             "yellow_perch"),
                    length = c(15,
                               24,
                               8.1,
                               12,
                               6.1,
                               12,
                               8.1)) %>% 
  left_join(length_scalers,by = "species") %>% 
  mutate(scaled_length = (length - length_mean)/length_sd)
pred_spec

#cross the datasets
pred_full <- crossing(pred_data, pred_spec)
rm(pred_data, pred_spec)
#############################################################################
fit <- readRDS(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output", "mw_random_leave_out.rds"))

#predicting at the lake-spec level 
#####here we can change the number of draws###########
lake_spec_pred <- pred_full %>% 
  add_epred_draws(fit, ndraws = 500, allow_new_levels = TRUE)
rm(fit, pred_full)

#here we have 500 draw for each row because there is only one length within each lake-spec
lake_spec_sum <- lake_spec_pred %>% 
  #prediction in original scale
  mutate(back_transformed = exp(.epred)) %>% 
  group_by(lagos_id, species) %>% 
  summarise(
    log_hgppm = median(.epred),
    log_lower = quantile(.epred, 0.05),
    log_upper = quantile(.epred, 0.95),
    hgppm = median(back_transformed),
    lower = quantile(back_transformed, 0.05),
    upper = quantile(back_transformed, 0.95),
    prob_over = mean(back_transformed > 0.22),
    .groups = "drop"
  ) 
rm(lake_spec_pred)

lake_spec_sum <- lake_spec_sum %>% 
  left_join(lake_link %>% 
              select(lagos_id = lagoslakeid,
                     lake_lat_decdeg,
                     lake_lon_decdeg))
rm(lake_link)

###################making map#############################

lake_sf <- lake_spec_sum %>%
  st_as_sf(coords = c("lake_lon_decdeg", "lake_lat_decdeg"), crs = 4326) %>% 
  mutate(species = case_when(species == "black_crappie" ~ "Black Crappie",
                             species == "bluegill" ~ "Bluegill",
                             species == "largemouth_bass" ~ "Largemouth Bass",
                             species == "northern_pike" ~ "Northern Pike",
                             species == "walleye" ~ "Walleye",
                             species == "yellow_perch" ~ "Yellow Perch",
                             species == "white_sucker" ~ "White Sucker")) 

states <- st_as_sf(map("state", plot = FALSE, fill = TRUE)) %>%
  filter(ID %in% c("minnesota", "wisconsin"))

#map with prob over 0.22
ggplot() +
  geom_sf(data = states, fill = "white", color = "black") +
  geom_sf(
    data = lake_sf,
    aes(
      color = prob_over,
      alpha = .5
    )
  ) +
  facet_wrap(~ species) +
  scale_color_viridis_c(option = "plasma",
                        name = "Probability Over 0.22 ppm") +
  theme_minimal() +
  theme(legend.position = c(.65, .01),
        axis.text.x = element_text(angle = 30, hjust = 1))
#ggsave("prob_map.jpg", height = 5, width = 5, dpi = 600)


#create four grouping for "risk" based on posterior prob over 0.22
lake_spec_sum <- lake_spec_sum %>% 
  mutate(risk = case_when(prob_over >= 0.9 ~ "high",
                          prob_over >= 0.5 & prob_over < 0.9 ~ "worth a check",
                          prob_over >= 0.25 & prob_over < 0.5 ~ "yellow flag",
                          prob_over < 0.25 ~ "low",
                          TRUE ~ NA)) 


###############################################################################
#summary stuff
lake_spec_sum %>% 
  group_by(species) %>% 
  summarise(high = sum(risk == "high"),
            high_med = sum(risk == "worth a check"),
            low_med = sum(risk == "yellow flag"),
            low = sum(risk == "low"),
            n = n())

############################################################################
#map
lake_sf <- lake_spec_sum %>%
  st_as_sf(coords = c("lake_lon_decdeg", "lake_lat_decdeg"), crs = 4326) %>% 
  mutate(risk = as.factor(risk),
         risk = factor(risk, levels = c("high", "worth a check", "yellow flag", "low"))) %>% 
  mutate(species = case_when(species == "black_crappie" ~ "Black Crappie",
                             species == "bluegill" ~ "Bluegill",
                             species == "largemouth_bass" ~ "Largemouth Bass",
                             species == "northern_pike" ~ "Northern Pike",
                             species == "walleye" ~ "Walleye",
                             species == "yellow_perch" ~ "Yellow Perch",
                             species == "white_sucker" ~ "White Sucker")) 

states <- st_as_sf(map("state", plot = FALSE, fill = TRUE)) %>%
  filter(ID %in% c("minnesota", "wisconsin"))

#map with color by grouping
ggplot() +
  geom_sf(
    data = lake_sf,
    aes(
      color = risk,
      shape = risk
    ),
    alpha = .6
  ) +
  scale_shape_manual("Risk for Consumption", values = c(15, 16, 17, 18),
                     labels = c("High", "Worth a Check", "Yellow Flag", "Low")) +
  scale_color_manual("Risk for Consumption", 
                     values = c("#CC79A7", "#D55E00", "#0072B2", "#009E73"),
                     labels = c("High", "Worth a Check", "Yellow Flag", "Low")) +
  geom_sf(data = states, fill = NA, color = "black", alpha = .5) +
  facet_wrap(~ species) +
  theme_minimal() +
  theme(legend.box = "horizontal",
    legend.position = c(.65, .15),
    legend.key.size = unit(0.6, "lines"),
    legend.title = element_text(size = 10),
    legend.text  = element_text(size = 8),
    legend.spacing.x = unit(0.4, "cm"),
    legend.spacing.y = unit(0.4, "cm"),
    axis.text.x = element_text(angle = 30, hjust = 1)
    )
#ggsave("prediciton_map_levels.jpg", height = 5, width = 5, dpi = 600)

############################################################################
lake_spec_sum %>% 
  group_by(species, risk) %>% 
  summarise(n_lakes = n()) %>% 
  mutate(species = case_when(species == "black_crappie" ~ "Black Crappie",
                             species == "bluegill" ~ "Bluegill",
                             species == "largemouth_bass" ~ "Largemouth Bass",
                             species == "northern_pike" ~ "Northern Pike",
                             species == "walleye" ~ "Walleye",
                             species == "yellow_perch" ~ "Yellow Perch",
                             species == "white_sucker" ~ "White Sucker")) %>%  
  mutate(risk = as.factor(risk),
         risk = factor(risk, levels = c("high", "worth a check", "yellow flag", "low"))) %>% 
  ggplot() +
  geom_col(aes(x = risk, y = n_lakes, fill = risk)) +
  labs(y = NULL,
       x = NULL) +
  scale_fill_manual("Consumption Risk",
                    values = c("#CC79A7", "#D55E00", "#0072B2", "#009E73"),
                    labels = c("High", "Worth a Check", "Yellow Flag", "Low")) +
  facet_wrap(~species) +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1))
#ggsave("predicted_risk_count.jpg", height = 5, width = 5, dpi = 600)

lake_spec_sum %>% 
  group_by(species, risk) %>% 
  summarise(n_lakes = n()) %>% 
  mutate(species = case_when(species == "black_crappie" ~ "Black Crappie",
                             species == "bluegill" ~ "Bluegill",
                             species == "largemouth_bass" ~ "Largemouth Bass",
                             species == "northern_pike" ~ "Northern Pike",
                             species == "walleye" ~ "Walleye",
                             species == "yellow_perch" ~ "Yellow Perch",
                             species == "white_sucker" ~ "White Sucker")) %>% 
  mutate(risk = as.factor(risk),
         risk = factor(risk, levels = c("high", "worth a check", "yellow flag", "low"))) %>% 
  ggplot() +
  geom_col(aes(x = species, y = n_lakes, fill = risk)) +
  scale_fill_manual("Consumption Risk",
                    values = c("#CC79A7", "#D55E00", "#0072B2", "#009E73"),
                    labels = c("High", "Worth a Check", "Yellow Flag", "Low")) +
  labs(x = NULL,
       y = NULL) +
  theme_minimal() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 25, hjust = 1),
        legend.key.size = unit(0.6, "lines"),
    legend.title = element_text(size = 10),
    legend.text  = element_text(size = 8),
    legend.spacing.x = unit(0.4, "cm"),
    legend.spacing.y = unit(0.4, "cm"))
#ggsave("risk_by_species.jpg", height = 5, width = 5, dpi = 600)
```

# lake covarites 
-lets look if there are trends for lakes where the probably of exceeding 0.22ppm is over  80%
-summaries of average covarites for these systems
   -this may be too much, but could then pull out a lake that is most alike the averages for a real example of a lake in that category 
```{r}
lake_link <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "lake_link.csv")) %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

#crosswalk for lagos id to nhdhr
xwalk <- read_csv(file.path("D:",
                          "Shared drives",
                          "Hansen Lab",
                          "LAGOS data extraction",
                          "lake_link.csv"))
glimpse(xwalk)

#land use land cover data
lagos_watershed_lulc <- read_csv(file.path("D:",
                          "Shared drives",
                          "Hansen Lab",
                          "LAGOS data extraction",
                          "lagos_watershed_lulc.csv"))
glimpse(lagos_watershed_lulc)

most_recent_landcover <- lagos_watershed_lulc %>% 
  filter(year == 2016) %>% 
  select(-spatial_division,
         -year,
         -datacoveragepct,
         -precision) %>% 
  rename(lagoslakeid = zoneid)
rm(lagos_watershed_lulc)

#watershed attributes data
lagos_watershed <- read_csv(file.path("D:",
                          "Shared drives",
                          "Hansen Lab",
                          "LAGOS data extraction",
                          "lake_watersheds.csv"))
glimpse(lagos_watershed)

#############join lagos data#####################
covary <- lagos_watershed %>% 
  left_join(most_recent_landcover) %>% 
  mutate(state_keep = case_when(str_detect(ws_states, "MN") ~ "y",
                                str_detect(ws_states, "WI") ~ "y",
                                TRUE ~ "n")) %>% 
  filter(state_keep == "y")
rm(lagos_watershed,
   most_recent_landcover)

#lets just grab the lagos id, nhdhr, name, and coords for cross walking
xwalk <- xwalk %>% 
  distinct(lagoslakeid,
         lake_nhdid,
         lake_namegnis,
         lake_namelagos,
         lake_lat_decdeg,
         lake_lon_decdeg)

#grabbing cross walk information via lagos id (really interested in the nhdhr)
covary <- covary %>% 
  left_join(xwalk, relationship = "one-to-one")
rm(xwalk)

pred_data <- covary %>% 
  select(lagos_id = lagoslakeid,
         ws_focallakewaterarea_ha,
         ws_lake_arearatio,
         nlcd_forcon42_pct,
         nlcd_formix43_pct,
         nlcd_fordec41_pct,
         nlcd_wetemerg95_pct,
         nlcd_wetwood90_pct,
         ws_lat_decdeg,
         ws_lon_decdeg)
rm(covary)

#############################################################################

lakes <- lake_spec_sum %>% 
  select(-lake_lat_decdeg,
         -lake_lon_decdeg) %>% 
  left_join(pred_data)
rm(lake_spec_sum)

high_risk_lakes <- lakes %>% 
  #filter for the probs over being 0.8
  filter(prob_over >= 0.8) %>% 
  distinct(lagos_id, species, .keep_all =  T)


numeric_summary_by_species <- high_risk_lakes %>%
  group_by(species) %>%
  summarise(across(c(ws_focallakewaterarea_ha,
         ws_lake_arearatio,
         nlcd_forcon42_pct,
         nlcd_formix43_pct,
         nlcd_fordec41_pct,
         nlcd_wetemerg95_pct,
         nlcd_wetwood90_pct),
                   list(mean = ~mean(.x),
                        sd = ~sd(.x),
                        min = ~min(.x),
                        max = ~max(.x))))

numeric_long <- numeric_summary_by_species %>%
  pivot_longer(
    cols = -species,
    names_to = c("variable", "stat"),
    names_sep = "_(?=[a-z]+$)"
  ) %>%
  pivot_wider(names_from = stat, values_from = value)

ggplot(numeric_long, aes(x = species, y = mean, fill = species)) +
  geom_col() +
  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = 0.2) +
  facet_wrap(~variable, scales = "free_y") +
  theme_minimal() +
  guides(fill = "none") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    y = "Mean ± SD"
  )
#ggsave("risky_lakes.jpg", height = 5, width = 5, dpi = 600)

ggplot(numeric_long, aes(x = variable, y = mean, fill = variable)) +
  geom_col() +
  geom_errorbar(aes(ymin = mean - sd, ymax = mean + sd), width = 0.2) +
  facet_wrap(~species, scales = "free_y") +
  theme_minimal() +
  guides(fill = "none") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    y = "Mean ± SD"
  )
```


# looking at length cross 0.22ppm 
*This will be pretty memory intensive because we will have 500 draws for each of 80 lengths, of each 7 species, for every unique lake*
-consider dialing in the lengths by species for where they are expected to cross (no super small or super big lengths, varying by species)

- use all lakes in the region 
- cross it with a grid of lengths from each species 
- get the length for when a species crosses 0.22 ppm in 1. lake 2.) region
```{r}
#grab lake data
#lake link
lake_link <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "lake_link.csv")) %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

#crosswalk for lagos id to nhdhr
xwalk <- read_csv(file.path("D:",
                          "Shared drives",
                          "Hansen Lab",
                          "LAGOS data extraction",
                          "lake_link.csv"))
glimpse(xwalk)

#land use land cover data
lagos_watershed_lulc <- read_csv(file.path("D:",
                          "Shared drives",
                          "Hansen Lab",
                          "LAGOS data extraction",
                          "lagos_watershed_lulc.csv"))
glimpse(lagos_watershed_lulc)

most_recent_landcover <- lagos_watershed_lulc %>% 
  filter(year == 2016) %>% 
  select(-spatial_division,
         -year,
         -datacoveragepct,
         -precision) %>% 
  rename(lagoslakeid = zoneid)
rm(lagos_watershed_lulc)

#watershed attributes data
lagos_watershed <- read_csv(file.path("D:",
                          "Shared drives",
                          "Hansen Lab",
                          "LAGOS data extraction",
                          "lake_watersheds.csv"))
glimpse(lagos_watershed)

#############join lagos data#####################
covary <- lagos_watershed %>% 
  left_join(most_recent_landcover) %>% 
  mutate(state_keep = case_when(str_detect(ws_states, "MN") ~ "y",
                                str_detect(ws_states, "WI") ~ "y",
                                TRUE ~ "n")) %>% 
  filter(state_keep == "y")
rm(lagos_watershed,
   most_recent_landcover)

#lets just grab the lagos id, nhdhr, name, and coords for cross walking
xwalk <- xwalk %>% 
  distinct(lagoslakeid,
         lake_nhdid,
         lake_namegnis,
         lake_namelagos,
         lake_lat_decdeg,
         lake_lon_decdeg)

#grabbing cross walk information via lagos id (really interested in the nhdhr)
covary <- covary %>% 
  left_join(xwalk, relationship = "one-to-one")
rm(xwalk)
#############################################################################
pred_data <- covary %>% 
  select(lagos_id = lagoslakeid,
         ws_focallakewaterarea_ha,
         ws_lake_arearatio,
         nlcd_forcon42_pct,
         nlcd_formix43_pct,
         nlcd_fordec41_pct,
         nlcd_wetemerg95_pct,
         nlcd_wetwood90_pct,
         ws_lat_decdeg,
         ws_lon_decdeg)
rm(covary)
##############################################################################
#use scalers to put lakes in original data scale
pred_data <- pred_data %>% 
  filter(ws_lake_arearatio < 100) %>% 
  filter(!is.na(ws_lake_arearatio)) %>%
  filter(!is.na(nlcd_forcon42_pct)) %>% 
  rename(lake_area_ha = ws_focallakewaterarea_ha,
         watershed_lake_area_ratio = ws_lake_arearatio) %>% 
  mutate(
    #first, lets provide 0's in our landcover data with a very small number so they can be transfomred to logit scale
  mutate(across(starts_with("nlcd"),
                ~ifelse(.x ==0, 0.001, .x))) %>%
  mutate(across(starts_with("nlcd"),
                ~ifelse(.x ==100, 0.999, .x))) %>%  
  #now apply logit transformations to landcover and log to others
  mutate(logit_wetwood = car::logit(nlcd_wetwood90_pct),
         logit_wetemerge = car::logit(nlcd_wetemerg95_pct),
         logit_formix = car::logit(nlcd_formix43_pct),
         logit_forcon = car::logit(nlcd_forcon42_pct),
         logit_fordec = car::logit(nlcd_fordec41_pct),
         log_area = log(lake_area_ha),
         log_ws_area_ratio = log(watershed_lake_area_ratio))
  ) %>% 
  mutate(scaled_log_area = (log_area - lake_scalers$mean_log_area) /lake_scalers$sd_log_area,
           
           scaled_logit_wetwood = (logit_wetwood - lake_scalers$mean_logit_wetwood) / lake_scalers$sd_logit_wetwood,
           
           scaled_logit_wetemerge = (logit_wetemerge - lake_scalers$mean_logit_wetemerge) / lake_scalers$sd_logit_wetemerge,
           
           scaled_logit_formix = (logit_formix - lake_scalers$mean_logit_formix) / lake_scalers$sd_logit_formix,
           
           scaled_logit_forcon = (logit_forcon - lake_scalers$mean_logit_forcon) / lake_scalers$sd_logit_forcon,
           
           scaled_logit_fordec = (logit_fordec - lake_scalers$mean_logit_fordec) / lake_scalers$sd_logit_fordec,
           
           scaled_log_ws_area_ratio = (log_ws_area_ratio - lake_scalers$mean_lw_ratio) / lake_scalers$sd_lw_ratio)

##############################################################################
#estimate crossing by solving the equation 

#first grab grab the draws from the model fit
draws <- as_draws_df(fit)
rm(fit)

#solving for where length crosses 0.22
threshold <- 0.22

cross_length_draws <- function(draws, species, lake_row) {
  #grab lake ids
  lake_id <- as.character(lake_row$lagos_id)
  
  #items that help in reference coding and unsampled lakes
  sp_int  <- paste0("b_species", species)
  sp_slope <- paste0("b_scaled_length:species", species)
  re_col  <- paste0("r_lagos_id[", lake_id, ",Intercept]")
  
  #species specific intercept or 0 for reference species
  draws %>%
    mutate(
      # species-specific intercept
      alpha_sp =
        b_Intercept +
        if (sp_int %in% colnames(draws)) .data[[sp_int]] else 0,
      
      #species specific slope or 0 for reference grouping
      # species-specific slope
      beta_sp =
        b_scaled_length +
        if (sp_slope %in% colnames(draws)) .data[[sp_slope]] else 0,
      
      #random intercept if lake exists in data or 0
      # lake random intercept
      alpha_lake =
        if (re_col %in% colnames(draws)) {
          .data[[re_col]]
        } else {
          0
        },

      # covariate contribution
      cov_effect =
        b_scaled_logit_wetwood  * lake_row$scaled_logit_wetwood +
        b_scaled_logit_wetemerge * lake_row$scaled_logit_wetemerge +
        b_scaled_logit_formix   * lake_row$scaled_logit_formix +
        b_scaled_logit_forcon   * lake_row$scaled_logit_forcon +
        b_scaled_logit_fordec   * lake_row$scaled_logit_fordec +
        b_scaled_log_ws_area_ratio * lake_row$scaled_log_ws_area_ratio +
        b_scaled_log_area       * lake_row$scaled_log_area,

      # solve for scaled length
      scaled_l_cross =
        (log(threshold) - alpha_sp - alpha_lake - cov_effect) / beta_sp
    ) %>%
    pull(scaled_l_cross)
}

# output 
summarise_cross <- function(scaled_l_cross) {
  tibble(
    l_median = median(scaled_l_cross),
    l_lo90   = quantile(scaled_l_cross, 0.05),
    l_hi90   = quantile(scaled_l_cross, 0.95)
  )
}

species_list <- c(
    "walleye",
    "northern_pike",
    "black_crappie",
    "largemouth_bass",
    "bluegill",
    "white_sucker",
    "yellow_perch")

#extract results
results <- vector(
  "list",
  length = nrow(pred_data) * length(species_list)
)

k <- 1

for (i in seq_len(nrow(pred_data))) {

  lake_row <- pred_data[i, ]

  for (sp in species_list) {

    l_cross <- cross_length_draws(
      draws   = draws,
      species = sp,
      lake_row = lake_row
    )

    results[[k]] <- summarise_cross(l_cross) %>%
      mutate(
        lagos_id = lake_row$lagos_id,
        species  = sp
      )

    k <- k + 1
  }
}

results <- bind_rows(results) %>% 
  left_join(length_scalers) %>% 
   mutate(length = (l_median*length_sd) + length_mean,
          length_lower = (l_lo90*length_sd) + length_mean,
          length_upper = (l_hi90*length_sd) + length_mean)
#saving results
# write_csv(results, file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output", "length_crossing_by_lake.csv"))

results <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output", "length_crossing_by_lake.csv"))

###########################################################################
# connect back to lake data
final_results <- pred_data %>% 
  left_join(results, by = c("lagos_id")) %>% 
  left_join(lake_link %>% 
              select(lagos_id = lagoslakeid,
                     lake_lat_decdeg,
                     lake_lon_decdeg), by = c("lagos_id"))
rm(results)
###########################################################################
#average median length across lakes - think about how to deal with lake level vary
in_sample_lake_spec <- hg.data.loo.mw %>% 
  mutate(in_sample_check = paste0(lagos_id,species)) %>% 
  distinct(in_sample_check)

final_results %>%
  mutate(in_sample_check = paste0(lagos_id,species)) %>% 
  mutate(in_sample = case_when(in_sample_check %in% in_sample_lake_spec$in_sample_check ~ "y",
                               TRUE ~ "n")) %>% 
  group_by(in_sample, species) %>% 
  summarise(
    min = min(length),
    mean = mean(length),
    max = max(length),
    n())
#in sample means that lake-species was used in training

final_results %>%
  mutate(in_sample_check = paste0(lagos_id,species)) %>% 
  mutate(in_sample = case_when(in_sample_check %in% in_sample_lake_spec$in_sample_check ~ "y",
                               TRUE ~ "n")) %>%
  ggplot() +
  geom_density(aes(length, fill = in_sample), alpha = .5) +
  facet_wrap(~species)
#interesting the smallest length (-7.62) was from bone lake in Minnesota, only 1 sample of white sucker which was SUPER high, shifting the random effect, thus we see pike heavily influenced
#the in sample pike was from Ryan lake in MN St. Louis county with super high measured mercury

##########################################################################
lake_sf <- final_results %>%
  st_as_sf(coords = c("lake_lon_decdeg", "lake_lat_decdeg"), crs = 4326)

states <- st_as_sf(map("state", plot = FALSE, fill = TRUE)) %>%
  filter(ID %in% c("minnesota", "wisconsin"))


ggplot() +
  geom_sf(data = states, fill = "white", color = "black") +
  geom_sf(
    data = lake_sf,
    aes(
      color = length,
      alpha = .5
    )
  ) +
  facet_wrap(~ species) +
  scale_color_viridis_c(option = "plasma",
                        name = "Length (inches)") +
  theme_minimal() +
  theme(legend.position = c(.65, .01),
        axis.text.x = element_text(angle = 30, hjust = 1))
#ggsave("length_threshold_map.jpg", height = 5, width = 5, dpi = 600)
```


###################### old code##################################
```{r}
###########################################################################
#predition summaries based on non-transformed data
lake_species_preds <- predictions %>% 
  group_by(lagos_id, species, .draw) %>% 
  summarise(mean_pred = mean(.prediction))  %>% 
  ungroup() %>% 
  group_by(lagos_id, species) %>% 
  median_qi(mean_pred) %>% 
  select(-.width,
         -.point,
         -.interval) %>% 
  mutate(lagos_id = as.character(lagos_id))

lake_species_obs <- left_out_scaled %>% 
  group_by(lagos_id, species) %>% 
  summarise(mean_obs = mean(log_hgppm))

lake_species_compare <- left_join(lake_species_preds, lake_species_obs, by = c("lagos_id", "species"))
rm(lake_species_preds, lake_species_obs)


lake_species_metrics <- lake_species_compare %>%
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )
lake_species_metrics

#what about by various groupings?

loo_lakes <- left_out_scaled %>% 
  distinct(lagos_id, species, lake_status, species_present)

lake_species_compare <- lake_species_compare %>% 
  left_join(loo_lakes)

lake_species_compare %>%
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>%
  group_by(lake_status) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )


lake_species_compare %>% 
ggplot(aes(x = mean_obs, y = mean_pred)) +
  geom_point(alpha = 0.7, size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(
    x = "Observed log(Hg)",
    y = "Predicted log(Hg)",
  ) +
  theme_minimal()

lake_species_compare %>%
  group_by(species) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = log(0.22), linetype = "dashed") +
  facet_wrap(~species, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")

lake_species_compare %>%
  group_by(species) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = log(0.22), linetype = "dashed") +
  facet_grid(species~lake_status, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")

#lets filter for where the partial miss to see how viz how badly 
lake_species_compare %>%
  filter(mean_obs <= .lower | mean_obs >= .upper) %>% 
  group_by(species) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = log(0.22), linetype = "dashed") +
  facet_grid(species~lake_status, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")
#############################################################################
# prediction summaries 
#summary of the draws for each prediction (on log scale)
predictions_summary <- predictions %>% 
  group_by(lagos_id, hg_ppm, length_in, species, .row) %>%
  summarise(.pred_sd = sd(.prediction),
            .lower = quantile(.prediction, probs = c(0.025)),
            .upper = quantile(.prediction, probs = c(0.975)),
            .prediction = median(.prediction)) %>% 
  mutate(lagos_id = as.character(lagos_id))

#connect these predictions back to the original data
predictions_summary <- left_out_scaled %>% 
  #this is needed due to maybe duplicate spec-length-hgppm-year-lake
  distinct(lagos_id, species, hg_ppm, length_in, species, .keep_all = T) %>% 
  left_join(predictions_summary, by = c("lagos_id", "hg_ppm", "length_in", "species"))

#summary of predictions in the original scale (back transformed)
predictions_summary_original <- predictions %>% 
  mutate(.prediction = exp(.prediction)) %>% 
  group_by(lagos_id, hg_ppm, length_in, species, .row) %>%
  summarise(.pred_sd = sd(.prediction),
            .lower = quantile(.prediction, probs = c(0.025)),
            .upper = quantile(.prediction, probs = c(0.975)),
            .prediction = median(.prediction)) %>% 
  mutate(lagos_id = as.character(lagos_id))

predictions_summary_original <- left_out_scaled %>% 
  #this is needed due to there being some duplicate spec-length-hgppm-year-lake
  distinct(lagos_id, species, hg_ppm, length_in, species, .keep_all = T) %>% 
  left_join(predictions_summary_original, by = c("lagos_id", "hg_ppm", "length_in", "species"))
########################################################################
#old code using grid of lengths to find the crossing point

#generate length data we want
length_grid <- tidyr::crossing(
  species = c(
    "walleye",
    "northern_pike",
    "black_crappie",
    "largemouth_bass",
    "bluegill",
    "white_sucker",
    "yellow_perch"
  ),
  length = seq(10, 30, by = .25)
) %>% 
  left_join(length_scalers,by = "species") %>% 
  mutate(scaled_length = (length - length_mean)/length_sd) %>% 
  select(species, scaled_length)

#cross the datasets
pred_full <- crossing(pred_data, pred_spec)
rm(pred_data, pred_spec)
#############################################################################
fit <- readRDS(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output", "mw_random_leave_out.rds"))

#predicting at the lake-spec level 
#####here we can change the number of draws###########
lake_spec_pred <- pred_full %>% 
  add_epred_draws(fit, ndraws = 500, allow_new_levels = TRUE) %>% 
  mutate(hgppm = exp(.epred))
rm(fit, pred_full)


#here we have 500 draw for each row because there is only one length within each lake-spec
lake_spec_sum <- lake_spec_pred %>% 
  #prediction in original scale
  mutate(back_transformed = exp(.epred)) %>% 
  group_by(lagos_id, species) %>% 
  summarise(
    log_hgppm = median(.epred),
    log_lower = quantile(.epred, 0.05),
    log_upper = quantile(.epred, 0.95),
    hgppm = median(back_transformed),
    lower = quantile(back_transformed, 0.05),
    upper = quantile(back_transformed, 0.95),
    prob_over = mean(back_transformed > 0.22),
    .groups = "drop"
  ) 
rm(lake_spec_pred)

length_threshold <- pred_grid %>% 
  filter(hgppm >= 0.22) %>% 
  left_join(length_scalers) %>% 
  mutate(length = (scaled_length*length_sd) + length_mean) %>% 
  group_by(lagos_id, species, .draw) %>% 
  summarise(length_cross = min(scaled_length),
            length_cross_in = min(length))

threshold_summary <- length_threshold %>%
  group_by(lagos_id, species) %>%
  summarise(
    median_length = median(length_cross_in),
    lower_90 = quantile(length_cross_in, 0.05),
    upper_90 = quantile(length_cross_in, 0.95),
    .groups = "drop"
  ) 

pred_grid <-  pred_grid %>% 
  left_join(length_scalers) %>% 
  mutate(length = (scaled_length*length_sd) + length_mean) 

pred_summary <- pred_grid %>% 
  group_by(lagos_id, species, length) %>% 
  summarise(
    hg_median = median(hgppm),
    hg_lower  = quantile(hgppm, 0.05),
    hg_upper  = quantile(hgppm, 0.95),
    .groups = "drop"
  ) %>% 
   mutate(ci_width = hg_upper - hg_lower) %>% 
  filter(!(species == "yellow_perch" & length > 20)) %>% 
  filter(!(species == "black_crappie" & length > 23)) %>% 
  filter(!(species == "largemouth_bass" & length > 20)) %>% 
  filter(!(species == "white_sucker" & length < 20))

ggplot(pred_summary %>% 
         filter(lagos_id == "996") %>% 
         mutate(lagos_id_words = case_when(lagos_id == 996 ~ "Leech",
                                     TRUE ~ NA)),
       aes(x = length, y = hg_median)) +
  geom_ribbon(
    aes(ymin = hg_lower, ymax = hg_upper),
    alpha = 0.25
  ) +
  geom_line(linewidth = 1) +
  geom_hline(
    yintercept = 0.22,
    linetype = "dashed",
    color = "red"
  ) +
  facet_wrap(lagos_id_words ~ species, scales = "free") +
  theme_minimal()

ggplot(
  pred_summary %>%  filter(lagos_id == "996") %>% 
         mutate(lagos_id_words = case_when(lagos_id == 996 ~ "Leech",
                                     TRUE ~ NA)) %>% 
    filter(species %in% c("northern_pike", "walleye", "black_crappie")),
  aes(x = length, y = hg_median)
) +
  geom_ribbon(
    aes(ymin = hg_lower, ymax = hg_upper),
    alpha = 0.25
  ) +
  geom_line(linewidth = 1) +
  geom_hline(
    yintercept = 0.22,
    linetype = "dashed",
    color = "red"
  ) +
  geom_vline(
    data = threshold_summary %>% filter(lagos_id == "996") %>% 
      filter(species %in% c("northern_pike", "walleye", "black_crappie")),
    aes(xintercept = median_length),
    linewidth = 0.8,
    linetype = "dashed",
    inherit.aes = FALSE
  ) +
  geom_vline(
    data = threshold_summary %>% filter(lagos_id == "996") %>% 
      filter(species %in% c("northern_pike", "walleye", "black_crappie")),
    aes(xintercept = lower_90),
    linewidth = 0.8,
    inherit.aes = FALSE
  ) +
  geom_vline(
    data = threshold_summary %>% filter(lagos_id == "996") %>% 
      filter(species %in% c("northern_pike", "walleye", "black_crappie")),
    aes(xintercept = upper_90),
    linewidth = 0.8,
    inherit.aes = FALSE
  ) +
  facet_wrap(lagos_id_words ~ species) +
  theme_minimal()
###############################################################################

```

