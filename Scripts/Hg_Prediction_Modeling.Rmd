---
title: "Hg_Prediction_Modeling"
author: "Denver Link"
date: "2024-02-27"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Library
```{r}
library(tidyverse)
library(arrow)
library(LAGOSNE)
library(mwlaxeref)
library(mnsentinellakes)
library(corrplot)
library(performance)
library(brms)
library(lme4)
library(Matrix)
library(car)
library(tidybayes)
library(rstanarm)
library(bayesplot)
library(loo)
library(arrow)
library(sf)
library(maps)
library(viridis)
library(tigris)
```

#Data
```{r}
#mercury data
hg <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Mercury Data", "allfish_data_04042024_JLO.csv")) %>% 
  filter(!(is.na(LGTHIN) | is.na(HGPPM) | is.na(DOWID))) %>%   #removes samples with no hg, length, or dow data
  filter(TYPE %in% c("Lake", "LAKE")) %>% 
  mutate(DOWID = fixlakeid(DOWID)) %>% 
  filter(DOWID != "16000100") %>% 
  local_to_nhdhr(from_colname = "DOWID", states = "mn") %>% 
  mutate(nhdhr.id = str_remove(nhdhr.id, "nhdhr_")) %>% 
  select(SAMPLENO,
         WATERWAY,
         TYPE,
         LOCATION,
         DOWID,
         nhdhr.id,
         DATECOL2,
         YEARCOLL,
         SPEC,
         ANATHG,
         NOFISH,
         LGTHIN,
         WTLB,
         AGE,
         SEX,
         HGPPM,
         HGCODE,
         HGLAB,
         HGCMMT) %>% 
  rename(DOW = DOWID,
         nhdid = nhdhr.id) %>% 
  group_by(SPEC) %>% 
  mutate(scaled_length = scale(LGTHIN)) %>% 
  ungroup()

#connection to covariates
dnr_covary <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "MN_lakes_watershed_lulc_fromDNR.csv")) %>% 
  mutate(logit_wetlands = car::logit(Percent_Wetland),
         log_WS_Lake_Ratio = log(WS_Lake_Ratio))

dnr_covary %>% 
  ggplot() +
  geom_histogram(aes(logit_wetlands))

dnr_covary %>% 
  ggplot() +
  geom_histogram(aes(log_WS_Lake_Ratio))

scaled_dnr <- dnr_covary %>%
  mutate_at(vars(Total_Pasture_Hay_Hectares:log_WS_Lake_Ratio), scale) %>% 
  rename_with(~ paste0("scaled_", .), Total_Pasture_Hay_Hectares:log_WS_Lake_Ratio) 

dnr_covary <- dnr_covary %>% 
  left_join(scaled_dnr)
glimpse(dnr_covary)

#adding dnr covariates into the data
hg <- hg %>% 
  left_join(dnr_covary)
rm(dnr_covary, scaled_dnr)

#glm - for clarity and area values
glm <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "glm_lake_metadata.csv")) %>%
  filter(state == "MN") %>% 
  rename(nhdid = site_id) %>% 
  mutate(nhdid = gsub("^nhdhr_", "", nhdid)) %>% 
  mutate(clarity = case_when(is.infinite(clarity) ~ NA,
                             TRUE ~ clarity)) %>% 
  mutate(scaled_clarity = case_when(!is.na(clarity) ~ scale(clarity),
                                    TRUE ~ clarity),
         log_area = log(area),
         scaled_log_area = c(scale(log_area)))

glm %>% 
  ggplot() +
  geom_histogram(aes(log_area))

glm %>% 
  ggplot() +
  geom_histogram(aes(clarity))

hg <- hg %>% 
  left_join(glm) %>% 
  mutate(across(starts_with("scaled_"), as.vector))
rm(glm)
glimpse(hg)

#data for modeling - select for columns of use 
hg.model <- hg %>%
  select(WATERWAY,
         TYPE,
         DOW,
         nhdid,
         DATECOL2,
         YEARCOLL,
         SPEC,
         ANATHG,
         NOFISH,
         LGTHIN,
         scaled_length,
         HGPPM,
         Total_Wetland_Hectares,
         scaled_Total_Wetland_Hectares,
         Percent_Wetland,
         scaled_Percent_Wetland,
         logit_wetlands,
         scaled_logit_wetlands,
         WS_Lake_Ratio,
         log_WS_Lake_Ratio,
         scaled_log_WS_Lake_Ratio,
         scaled_WS_Lake_Ratio,
         centroid_lat,
         centroid_lon,
         max_depth,
         area,
         log_area,
         scaled_log_area,
         elevation,
         clarity,
         scaled_clarity)
```

#Exploring Predictors
```{r}
hg.model %>% 
  ggplot() +
  geom_histogram(aes(log(HGPPM)))

hg.model %>% 
  filter(SPEC == "WAE") %>% 
  ggplot() +
  geom_histogram(aes(scaled_length))

hg.model %>% 
  filter(SPEC == "NOP") %>% 
  ggplot() +
  geom_histogram(aes(scaled_length))

hg.model %>% 
  ggplot() +
  geom_histogram(aes(LGTHIN))

hg.model %>% 
  filter(SPEC == "WAE") %>% 
  ggplot() +
  geom_point(aes(LGTHIN, log(HGPPM))) +
  geom_smooth(aes(LGTHIN, log(HGPPM)))

hg.model %>% 
  filter(SPEC == "WAE") %>% 
  ggplot() +
  geom_point(aes(scaled_length, log(HGPPM))) +
  geom_smooth(aes(scaled_length, log(HGPPM)))

hg.model %>% 
  filter(SPEC == "WAE") %>% 
  ggplot() +
  geom_point(aes(logit_wetlands, log(HGPPM))) +
  geom_smooth(aes(logit_wetlands, log(HGPPM)))

hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  ggplot() +
  geom_histogram(aes(Percent_Wetland))

hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  ggplot() +
  geom_histogram(aes(scaled_Percent_Wetland))

hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  ggplot() +
  geom_histogram(aes(logit_wetlands))

hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  ggplot() +
  geom_histogram(aes(WS_Lake_Ratio))

hg.model %>% 
  distinct(DOW, .keep_all = T) %>%
  filter(log_WS_Lake_Ratio < 10) %>% 
  ggplot() +
  geom_histogram(aes(WS_Lake_Ratio))

hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  summarise(large.ratio = sum(WS_Lake_Ratio >= 250, na.rm = T),
            other.ratios = sum(WS_Lake_Ratio < 250, na.rm = T))
#16 lakes that seem much too big (over 1500)
#82 over 250

#which lakes would be filtered out?
hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  filter(WS_Lake_Ratio >= 250) %>% 
  distinct(WATERWAY, DOW) %>% 
  print( n= nrow(.))


```

#First attempt of modeling
```{r}
#ensure data clean for modeling
hg.model.filtered <- hg.model %>% 
  filter(TYPE == "Lake") %>% 
  filter(YEARCOLL >1996) %>% 
  filter(!(is.na(DOW) |
             is.na(ANATHG) |
             is.na(YEARCOLL) |
             is.na(LGTHIN) |
             is.na(Total_Wetland_Hectares)|
             is.na(Percent_Wetland) |
             is.na(WS_Lake_Ratio) |
             is.na(centroid_lat) |
             is.na(area) |
             is.na(clarity) |
             is.na(scaled_length))) %>% 
  mutate(year = as.character(YEARCOLL),
         DOW = as.character(DOW)) %>% 
  mutate(log.hg = log(HGPPM))
#summary of the data
summary(hg.model.filtered)
glimpse(hg.model.filtered)
rm(hg.model, hg)

###################walleye model#################################
wae <- hg.model.filtered %>% 
  filter(SPEC == "WAE") 

#simple linear mixed effects model to test before going Bayesian
fit.wae <- lmer(log.hg ~ scaled_length + 
                 logit_wetlands +
                 log_area +
                 log_WS_Lake_Ratio + 
                 clarity + 
                 (1|year) + 
                 (1|DOW),
              data = wae)
summary(fit.wae)


#brms model
fit.wae <- brm(log.hg ~ scaled_length +
               scaled_Percent_Wetland + 
               log_area + 
               log_WS_Lake_Ratio +
               clarity + 
               (1|year) + 
               (1|DOW),
             data = wae,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
#save model run/read in saved model run
saveRDS(fit.wae, "WAE_model.rds")
fit.wae <- readRDS("WAE_model.rds")

#basic summary output
summary(fit.wae)
plot(fit.wae)
plot(conditional_effects(fit.wae))

#random effects for year
fit.wae %>%
  spread_draws(r_year[year,]) %>% 
  median_qi(condition_mean = r_year, .width = c(.95, .66)) %>%
  ggplot(aes(y = year, x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()

fit.wae %>%
  spread_draws(r_DOW[DOW,]) %>% 
  median_qi(condition_mean = r_DOW, .width = c(.95, .66)) %>%
  ggplot(aes(y = as.factor(DOW), x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()


chunks <- fit.wae %>%
  spread_draws(r_DOW[DOW,]) %>% 
  median_qi(condition_mean = r_DOW, .width = c(.95, .66)) %>%
  arrange(desc(condition_mean)) %>% 
  group_by(chunk = ceiling(row_number() / 50))

# Plot each chunk separately
plots <- list()
for (i in unique(chunks$chunk)) {
  plot_data <- chunks %>%
    filter(chunk == i) %>%
    ggplot(aes(y = as.factor(DOW), x = condition_mean, xmin = .lower, xmax = .upper)) +
    geom_pointinterval() +
    labs(title = paste("Chunk", i))
  
  plots[[i]] <- plot_data
}

# Print each plot
for (i in seq_along(plots)) {
  print(plots[[i]])
}

#predictive capacity 
yrep <- posterior_predict(fit.wae)

pp_check(fit.wae, ndraws = 100)
ppc_stat_2d(wae$log.hg, yrep = yrep, stat = c("mean", "sd"))
median <- ppc_stat_data(wae$log.hg, yrep = yrep, group = wae$DOW, stat = "median")
error <- ppc_stat_data(wae$log.hg, yrep = yrep, group = wae$DOW, stat = "sd")
#we have a way of getting data from posterior distributions - now just need a better way to display it
##find a way to subtract each yrep value from y to get a difference between them?

levels_per_page <- 20

# Determine the total number of pages
total_pages <- ceiling(nlevels(grouped$data$group) / levels_per_page)

# Create a list to store the ggplot objects for each page
plot_list <- list()

# Loop through each page
for (i in 1:total_pages) {
  # Determine the levels for this page
  start_level <- (i - 1) * levels_per_page + 1
  end_level <- min(i * levels_per_page, nlevels(grouped$data$group))
  levels_this_page <- levels(grouped$data$group)[start_level:end_level]
  
  # Filter data for this page
  filtered_data <- subset(grouped$data, group %in% levels_this_page)
  
  # Create ggplot for this page
  plot <- ggplot(filtered_data, aes(x = value)) +
    # Add your layers, aesthetics, and geoms here
    grouped$layers[[1]] + grouped$layers[[2]] + grouped$layers[[3]] +
    # Facet by group
    facet_wrap(~ group, scales = "free_y") +
    # Add any additional settings as needed
    grouped$theme
  
  # Store the plot in the list
  plot_list[[i]] <- plot
}

# Print or display the plots (e.g., save to a PDF)
pdf("multiple_pages_plot.pdf")
for (i in 1:total_pages) {
  print(plot_list[[i]])
}
dev.off()

#leave one out cross validation
loo.wae <- loo(fit.wae, save_psis = T)

print(loo.wae)
plot(loo.wae)
#marginal posterior predictive checks

ppc_loo_pit_qq(
  y = wae$log.hg,
  yrep = yrep,
  lw = weights(loo.wae$psis_object)
)

#PIT overlay on unif
ppc_loo_pit_overlay(y = wae$log.hg, yrep = yrep, lw = weights(loo.wae$psis_object))

#50 random predictive poionts
keep_obs <- sample(1:10605, 50, replace = FALSE)
ppc_loo_intervals(y = wae$log.hg, 
                  yrep = yrep, 
                  psis_object = loo.wae$psis_object, 
                  subset = keep_obs,
                  order = "median")

rm(fit.wae, loo.wae, plot_data, plots, i, chunks, yrep, wae, keep_obs)
###############northern pike model#######################################
nop <- hg.model.filtered %>% 
  filter(SPEC == "NOP")

fit.nop <- lmer(log.hg ~ scaled_length + 
                 logit_wetlands +
                 log_area +
                 log_WS_Lake_Ratio + 
                 clarity + 
                 (1|year) + 
                 (1|DOW),
              data = nop)
summary(nop)

#brms model for nop
fit.nop <- brm(log.hg ~ scaled_length +
               scaled_Percent_Wetland + 
               log_area + 
               log_WS_Lake_Ratio +
               clarity + 
               (1|year) + 
               (1|DOW),
             data = nop,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
saveRDS(fit.nop, "NOP_model.rds")
fit.nop <- readRDS("NOP_model.rds")

#basic summary output
summary(fit.nop)
plot(fit.nop)
plot(conditional_effects(fit.nop))


#random effects for year
fit.nop %>%
  spread_draws(r_year[year,]) %>% 
  median_qi(condition_mean = r_year, .width = c(.95, .66)) %>%
  ggplot(aes(y = year, x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()

fit.nop %>%
  spread_draws(r_DOW[DOW,]) %>% 
  median_qi(condition_mean = r_DOW, .width = c(.95, .66)) %>%
  ggplot(aes(y = as.factor(DOW), x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()

chunks <- fit.nop %>%
  spread_draws(r_DOW[DOW,]) %>% 
  median_qi(condition_mean = r_DOW, .width = c(.95, .66)) %>%
  arrange(desc(condition_mean)) %>% 
  group_by(chunk = ceiling(row_number() / 50))

# Plot each chunk separately
plots <- list()
for (i in unique(chunks$chunk)) {
  plot_data <- chunks %>%
    filter(chunk == i) %>%
    ggplot(aes(y = as.factor(DOW), x = condition_mean, xmin = .lower, xmax = .upper)) +
    geom_pointinterval() +
    labs(title = paste("Chunk", i))
  
  plots[[i]] <- plot_data
}

# Print each plot
for (i in seq_along(plots)) {
  print(plots[[i]])
}

#predictive capacity 
pp_check(fit.nop, ndraws = 100)
loo.nop <- loo(fit.nop, save_psis = T)

print(loo.nop)
plot(loo.nop)
#marginal posterior predictive checks
yrep <- posterior_predict(fit.nop)
ppc_loo_pit_qq(
  y = nop$log.hg,
  yrep = yrep,
  lw = weights(loo.nop$psis_object)
)
#PIT overlay on unif
ppc_loo_pit_overlay(y = nop$log.hg, yrep = yrep, lw = weights(loo.nop$psis_object))

ppc_data <- ppc_stat_data(nop$log.hg, yrep = yrep, group = nop$DOW, stat = "median")
#actual data within the bounds of the range of the model for how many lakes?
min_max <- ppc_data %>% 
  filter(variable != "y") %>% 
  group_by(group) %>% 
  summarise(min.median = min(exp(value)),
            max.median = max(exp(value)))

min_max <- ppc_data %>% 
  filter(variable == "y") %>% 
  select(group, value) %>% 
  mutate(value = exp(value)) %>% 
  right_join(min_max)
min_max %>% 
  filter(value >= min.median & value <= max.median)
position <- min_max %>% 
  mutate(position = (value - min.median) / (max.median - min.median),
         range = max.median - min.median)
#the position value is a proportion from 0-1 where 0 means the actual value is closer to the min and 1 closer to the max
position %>% 
  ggplot() +
  geom_histogram(aes(position))
position %>% 
  ggplot() +
  geom_histogram(aes(range)) +
  geom_vline(xintercept = .20)
filtered_dows <- position %>% 
  filter(range > 1) %>% 
  rename(DOW = group)
#how do our lakes that have a large range in predicted median Hg stack up to the rest?
#change the variable in the call below for exploration
nop %>% 
  mutate(large.range = case_when(DOW %in% filtered_dows$DOW ~ "Y",
                                 TRUE ~ "N")) %>% 
  distinct(DOW, .keep_all = T) %>% 
  ggplot() +
  geom_histogram(aes(log_WS_Lake_Ratio, fill = large.range))
#I didn't find a pattern here, but maybe there are just low sample sizes?
nop %>% 
  mutate(large.range = case_when(DOW %in% filtered_dows$DOW ~ "Y",
                                 TRUE ~ "N")) %>% 
  group_by(DOW, large.range) %>% 
  count() %>% 
  ggplot()+
  geom_histogram(aes(n, fill = large.range))
#could it be that there is just large variation in the hg of the lake?
nop %>% 
  mutate(large.range = case_when(DOW %in% filtered_dows$DOW ~ "Y",
                                 TRUE ~ "N")) %>%
  filter(large.range == "Y") %>% 
  ggplot() + 
  geom_point(aes(scaled_length, log.hg, color = DOW)) +
  geom_smooth(aes(scaled_length, log.hg, color = DOW), se = F, method = "lm") + 
  geom_smooth(data = nop, aes(scaled_length, log.hg), method = "lm") +
  theme(legend.position = "none")
#it appears the large range lakes tend to have large hg per length than the collective of lakes

ppc_stat_2d(nop$log.hg, yrep = yrep, stat = c("mean", "sd"))

#50 random predictive points
keep_obs <- sample(1:15054, 50, replace = FALSE)
ppc_loo_intervals(y = nop$log.hg, 
                  yrep = yrep, 
                  psis_object = loo.nop$psis_object, 
                  subset = keep_obs,
                  order = "median")
rm(chunks, fit.nop, loo.nop, nop, plot_data, plots, yrep, i, keep_obs)
###############Bluegill##############
blg <- hg.model.filtered %>% 
  filter(SPEC == "BLG")

#brms model for nop
fit.blg <- brm(log.hg ~ scaled_length +
               logit_wetlands + 
               log_area + 
               log_WS_Lake_Ratio +
               clarity + 
               (1|year) + 
               (1|DOW),
             data = blg,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
saveRDS(fit.blg, "BLG_model.rds")

#basic summary output
summary(fit.blg)
plot(fit.blg)
plot(conditional_effects(fit.blg))

#random effects for year
fit.blg %>%
  spread_draws(b_Intercept, r_year[year,]) %>% 
  median_qi(condition_mean = b_Intercept + r_year, .width = c(.95, .66)) %>%
  ggplot(aes(y = year, x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()

chunks <- fit.blg %>%
  spread_draws(b_Intercept, r_DOW[DOW,]) %>% 
  median_qi(condition_mean = b_Intercept + r_DOW, .width = c(.95, .66)) %>%
  arrange(desc(condition_mean)) %>% 
  group_by(chunk = ceiling(row_number() / 50))

# Plot each chunk separately
plots <- list()
for (i in unique(chunks$chunk)) {
  plot_data <- chunks %>%
    filter(chunk == i) %>%
    ggplot(aes(y = as.factor(DOW), x = condition_mean, xmin = .lower, xmax = .upper)) +
    geom_pointinterval() +
    labs(title = paste("Chunk", i))
  
  plots[[i]] <- plot_data
}

# Print each plot
for (i in seq_along(plots)) {
  print(plots[[i]])
}

#predictive capacity 
pp_check(fit.blg, ndraws = 100)
loo.blg <- loo(fit.blg, save_psis = T)

print(loo.blg)
plot(loo.blg)
#marginal posterior predictive checks
yrep <- posterior_predict(fit.blg)
ppc_loo_pit_qq(
  y = blg$log.hg,
  yrep = yrep,
  lw = weights(loo.blg$psis_object)
)
#PIT overlay on unif
ppc_loo_pit_overlay(y = blg$log.hg, yrep = yrep, lw = weights(loo.blg$psis_object))

#50 random predictive poionts
keep_obs <- sample(1:1660, 50, replace = FALSE)
ppc_loo_intervals(y = blg$log.hg, 
                  yrep = yrep, 
                  psis_object = loo.blg$psis_object, 
                  subset = keep_obs,
                  order = "median")
rm(chunks, fit.blg, loo.blg, blg, plot_data, plots, yrep, i, keep_obs)

################largemouth bass####################################
lmb <- hg.model.filtered %>% 
  filter(SPEC == "LMB")

#brms model for nop
fit.lmb <- brm(log.hg ~ scaled_length +
               logit_wetlands + 
               log_area + 
               log_WS_Lake_Ratio +
               clarity + 
               (1|year) + 
               (1|DOW),
             data = lmb,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
saveRDS(fit.lmb, "LMB_model.rds")

#basic summary output
summary(fit.lmb)
plot(fit.lmb)
plot(conditional_effects(fit.lmb))

#random effects for year
fit.lmb %>%
  spread_draws(b_Intercept, r_year[year,]) %>% 
  median_qi(condition_mean = b_Intercept + r_year, .width = c(.95, .66)) %>%
  ggplot(aes(y = year, x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()

chunks <- fit.lmb %>%
  spread_draws(b_Intercept, r_DOW[DOW,]) %>% 
  median_qi(condition_mean = b_Intercept + r_DOW, .width = c(.95, .66)) %>%
  arrange(desc(condition_mean)) %>% 
  group_by(chunk = ceiling(row_number() / 50))

# Plot each chunk separately
plots <- list()
for (i in unique(chunks$chunk)) {
  plot_data <- chunks %>%
    filter(chunk == i) %>%
    ggplot(aes(y = as.factor(DOW), x = condition_mean, xmin = .lower, xmax = .upper)) +
    geom_pointinterval() +
    labs(title = paste("Chunk", i))
  
  plots[[i]] <- plot_data
}

# Print each plot
for (i in seq_along(plots)) {
  print(plots[[i]])
}

#predictive capacity 
pp_check(fit.lmb, ndraws = 100)
loo.lmb <- loo(fit.lmb, save_psis = T)

print(loo.lmb)
plot(loo.lmb)
#marginal posterior predictive checks
yrep <- posterior_predict(fit.lmb)
ppc_loo_pit_qq(
  y = lmb$log.hg,
  yrep = yrep,
  lw = weights(loo.lmb$psis_object)
)
#PIT overlay on unif
ppc_loo_pit_overlay(y = lmb$log.hg, yrep = yrep, lw = weights(loo.lmb$psis_object))

#50 random predictive poionts
keep_obs <- sample(1:1470, 50, replace = FALSE)
ppc_loo_intervals(y = lmb$log.hg, 
                  yrep = yrep, 
                  psis_object = loo.lmb$psis_object, 
                  subset = keep_obs,
                  order = "median")
rm(chunks, fit.lmb, loo.lmb, lmb, plot_data, plots, yrep, i, keep_obs)

####################yellow perch###########################
yep <- hg.model.filtered %>% 
  filter(SPEC == "YEP")

#brms model for nop
fit.yep <- brm(log.hg ~ scaled_length +
               logit_wetlands + 
               log_area + 
               log_WS_Lake_Ratio +
               clarity + 
               (1|year) + 
               (1|DOW),
             data = yep,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
saveRDS(fit.yep, "YEP_model.rds")

#basic summary output
summary(fit.yep)
plot(fit.yep)
plot(conditional_effects(fit.yep))

#random effects for year
fit.yep %>%
  spread_draws(b_Intercept, r_year[year,]) %>% 
  median_qi(condition_mean = b_Intercept + r_year, .width = c(.95, .66)) %>%
  ggplot(aes(y = year, x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()

chunks <- fit.yep %>%
  spread_draws(b_Intercept, r_DOW[DOW,]) %>% 
  median_qi(condition_mean = b_Intercept + r_DOW, .width = c(.95, .66)) %>%
  arrange(desc(condition_mean)) %>% 
  group_by(chunk = ceiling(row_number() / 50))

# Plot each chunk separately
plots <- list()
for (i in unique(chunks$chunk)) {
  plot_data <- chunks %>%
    filter(chunk == i) %>%
    ggplot(aes(y = as.factor(DOW), x = condition_mean, xmin = .lower, xmax = .upper)) +
    geom_pointinterval() +
    labs(title = paste("Chunk", i))
  
  plots[[i]] <- plot_data
}

# Print each plot
for (i in seq_along(plots)) {
  print(plots[[i]])
}

#predictive capacity 
pp_check(fit.yep, ndraws = 100)
loo.yep <- loo(fit.yep, save_psis = T)

print(loo.yep)
plot(loo.yep)
#marginal posterior predictive checks
yrep <- posterior_predict(fit.yep)
ppc_loo_pit_qq(
  y = yep$log.hg,
  yrep = yrep,
  lw = weights(loo.yep$psis_object)
)
#PIT overlay on unif
ppc_loo_pit_overlay(y = yep$log.hg, yrep = yrep, lw = weights(loo.yep$psis_object))

#50 random predictive poionts
keep_obs <- sample(1:1389, 50, replace = FALSE)
ppc_loo_intervals(y = yep$log.hg, 
                  yrep = yrep, 
                  psis_object = loo.yep$psis_object, 
                  subset = keep_obs,
                  order = "median")
rm(chunks, fit.yep, loo.yep, yep, plot_data, plots, yrep, i, keep_obs)
```

#Modeling with Mixed Effects 5/16
```{r}
hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  summarise(lakes.1000 = sum(WS_Lake_Ratio > 1000, na.rm = T),
            lakes.500 = sum(WS_Lake_Ratio > 500, na.rm =T),
            lakes.200 = sum(WS_Lake_Ratio > 200, na.rm =T),
            lakes.100 = sum(WS_Lake_Ratio > 100, na.rm =T))

#filtering for specs that have over 1000 observations and most recent years
hg.data <- hg.model %>% 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  filter(YEARCOLL > 1999) %>% 
  filter(WS_Lake_Ratio < 100) %>% #loose 132 lakes and 4788 samples
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area)))
rm(hg, hg.model)

fit.statewide <- brm(log_hg ~ scaled_length*SPEC + SPEC + scaled_logit_wetlands + scaled_log_WS_Lake_Ratio + scaled_log_area + scaled_clarity + (1|DOW),
                     data = hg.data,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 10)

saveRDS(fit.statewide, "statewide.predict.rds")
fit.statewide <- readRDS("statewide.predict.rds")

plot(fit.statewide)
summary(fit.statewide)
plot(conditional_effects(fit.statewide))

#pulling conditional effects from lakes 
lake_link <- read_csv("Data/lake_link.csv") %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid) %>% 
  right_join(hg.data) %>% 
  distinct(DOW, 
           nhdid, 
           lake_lat_decdeg, 
           lake_lon_decdeg)

dow_effect <- fit.statewide %>% 
  spread_draws(r_DOW[DOW,])  %>% 
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  left_join(lake_link) 

dow_effect %>% 
  filter(is.na(lake_lat_decdeg)) %>% 
  summarise(n = n_distinct(DOW))
#7 lakes do not have nhdids - oddly all in st. louis county

#map 
mn <- map_data("state") %>% 
  filter(region == "minnesota")

median_dow <- dow_effect %>% 
  group_by(DOW, lake_lat_decdeg, lake_lon_decdeg) %>% 
  summarise(lake_effects = median(r_DOW)) 

#how do the lake parameters contribute to lake effects?
random_effect_check <- hg.data %>% 
  select(DOW,
         SPEC,
         scaled_logit_wetlands,
         scaled_log_WS_Lake_Ratio,
         scaled_log_area,
         scaled_clarity) %>% 
  distinct(DOW, SPEC, .keep_all = T) %>% 
  right_join(median_dow)

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = median_dow, aes(lake_lon_decdeg, lake_lat_decdeg, color = lake_effects), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()
rm(dow_effect)

#predictions of lake-species hg are made following the parameters listed
# 1. I pull the lake level parameters for each spec-lake 
# 2. I create a scaled length variable so that predictions are made for the mean length of each species
# 3. I add predicted draws from the model that use the lake level values from the lake and mean length of species
# 4. I filter for the lakes where that species was actually analyzed 
get_variables(fit.statewide)

predictions <- random_effect_check %>% 
  mutate(scaled_length = c(0)) %>% 
  add_predicted_draws(fit.statewide)

glimpse(predictions)

#wae
wae.lakes <- hg.data %>% 
  filter(SPEC == "WAE") %>% 
  distinct(DOW)

dow_effect_wae <- predictions %>% 
  filter(DOW %in% wae.lakes$DOW) %>% 
  filter(SPEC == "WAE") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "WAE")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_wae, aes(lake_lon_decdeg, lake_lat_decdeg, color = wae.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()

#nop
nop.lakes <- hg.data %>% 
  filter(SPEC == "NOP") %>% 
  distinct(DOW)

dow_effect_nop <- predictions %>% 
  filter(DOW %in% nop.lakes$DOW) %>% 
  filter(SPEC == "NOP") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "NOP")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_nop, aes(lake_lon_decdeg, lake_lat_decdeg, color = nop.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()

#yep
yep.lakes <- hg.data %>% 
  filter(SPEC == "YEP") %>% 
  distinct(DOW)

dow_effect_yep <- predictions %>% 
  filter(DOW %in% yep.lakes$DOW) %>% 
  filter(SPEC == "YEP") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "YEP")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_yep, aes(lake_lon_decdeg, lake_lat_decdeg, color = yep.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()


#lmb
lmb.lakes <- hg.data %>% 
  filter(SPEC == "LMB") %>% 
  distinct(DOW)

dow_effect_lmb <- predictions %>% 
  filter(DOW %in% lmb.lakes$DOW) %>% 
  filter(SPEC == "LMB") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "LMB")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_lmb, aes(lake_lon_decdeg, lake_lat_decdeg, color = lmb.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()

#blc
blc.lakes <- hg.data %>% 
  filter(SPEC == "BLC") %>% 
  distinct(DOW)

dow_effect_blc <- predictions %>% 
  filter(DOW %in% blc.lakes$DOW) %>% 
  filter(SPEC == "BLC") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "BLC")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_blc, aes(lake_lon_decdeg, lake_lat_decdeg, color = blc.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()

#wts
wts.lakes <- hg.data %>% 
  filter(SPEC == "WTS") %>% 
  distinct(DOW)

dow_effect_wts <- predictions %>% 
  filter(DOW %in% wts.lakes$DOW) %>% 
  filter(SPEC == "WTS") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "WTS")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_wts, aes(lake_lon_decdeg, lake_lat_decdeg, color = wts.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()

#blg
blg.lakes <- hg.data %>% 
  filter(SPEC == "BLG") %>% 
  distinct(DOW)

dow_effect_blg <- predictions %>% 
  filter(DOW %in% blg.lakes$DOW) %>% 
  filter(SPEC == "BLG") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "BLG")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_blg, aes(lake_lon_decdeg, lake_lat_decdeg, color = blg.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()

#all species together
all_species_preds <- bind_rows(dow_effect_blc, 
                    dow_effect_blg, 
                    dow_effect_lmb, 
                    dow_effect_nop, 
                    dow_effect_wae,
                    dow_effect_wts,
                    dow_effect_yep)

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = all_species_preds, aes(lake_lon_decdeg, lake_lat_decdeg, color = hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.5, .15),
        legend.box = "horizontal") +
  facet_wrap(~SPEC) +
  coord_fixed()
ggsave("all_spec_map.png", width = 11, height = 7, dpi = 600)

#getting hg correlation from predicted values
corr <- all_species_preds %>% 
  filter(.width == "0.95") %>% 
  select(DOW, hg, nhdid, lake_lat_decdeg, lake_lon_decdeg, SPEC) %>% 
  pivot_wider(id_cols = c("DOW", "nhdid", "lake_lat_decdeg", "lake_lon_decdeg"),
              names_from = "SPEC",
              values_from = "hg")
corr %>% 
  ggplot() +
  geom_point(aes(x = WAE, y = NOP)) +
  geom_smooth(aes(x = WAE, y = NOP), method = "lm")

species_cols <- corr[, c("BLC", "BLG", "LMB", "NOP", "WAE", "WTS", "YEP")]
correlation_matrix <- cor(species_cols, use = "pairwise.complete.obs")
corrplot(correlation_matrix, method = "circle")
#all correlations are near 1 due to the nature of predicting from the model based on species coefficients 

#here is a better look at the difference in hg values within a lake
all_species_preds %>% 
  filter(.width == "0.95") %>% 
  ggplot() +
  geom_point(aes(x= SPEC, y = hg, color = DOW)) +
  geom_violin(aes(x= SPEC, y = hg)) +
  theme(legend.position = "none")

#filtering for a lake that has all species
no_nas <- corr %>% 
  drop_na() 
all_species_preds %>%
  filter(DOW %in% no_nas$DOW) %>% 
  ggplot() +
  geom_col(aes(x=SPEC, y = hg)) +
  facet_wrap(~DOW)

#manual residuals 
#looking at log.hg of observed vs log.hg predicted for testing model fit
res <- hg.data %>% 
  filter(!(DOW %in% c("16063300", 
                      "31122500",
                      "38021100",
                      "38053200",
                      "41011000",
                      "69045900",
                      "69058900",
                      "69069000",
                      "69069300",
                      "69069400",
                      "69075500",
                      "69075700"))) %>% 
  select(DOW,
         SPEC,
         scaled_logit_wetlands,
         scaled_log_WS_Lake_Ratio,
         scaled_log_area,
         scaled_clarity,
         scaled_length,
         log_hg) %>% 
  add_residual_draws(fit.statewide)
res <- res %>% 
  ungroup()
#write_dataset(dataset = res, path = "Output/raw_residuals_arrow")
res <- open_dataset("Output/raw_residuals_arrow")

res.summary <- res %>%
  median_qi()



res.summary <- read_csv("summary_residuals.csv") %>% 
  mutate(fitted.value = log_hg - .residual)

res.summary %>% 
ggplot(aes(sample = .residual)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_point(aes(scaled_length, .residual)) +
  geom_smooth(aes(scaled_length, .residual)) +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_histogram(aes(.residual)) +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_density(aes(.residual, fill = SPEC), alpha = .5)

res.summary %>% 
  group_by(SPEC) %>% 
  summarise(mean = mean(.residual),
            median = median(.residual),
            sd = sd(.residual))

res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, .residual)) +
  geom_point(data = res.summary %>% 
               filter(log_hg < -4.60516), aes(fitted.value, .residual,  color = "red")) +
  geom_smooth(aes(fitted.value, .residual)) +
  geom_vline(xintercept = -1.609438, color = "red") +
  facet_wrap(~SPEC) +
  theme(legend.position = "none")
#Most noticeably for BLC and YEP there is a linear trend at low fitted values... can see it from BLG and WTS too
#I think this is a LOD problem... all fish of varying lengths have the same HG value 
#i made all of the values 0.01 and below the red color 

res.summary %>% 
  ggplot() +
  geom_point(aes(scaled_length, exp(fitted.value))) +
  geom_smooth(aes(scaled_length, exp(fitted.value))) +
  facet_wrap(~SPEC)

#RMSE and mad
res.summary %>% 
  mutate(squared_residuals = .residual^2,
         fitted_back = exp(fitted.value),
         obs_back = exp(log_hg),
         back_residual = obs_back - fitted_back,
         abs_back_residual = abs(back_residual)) %>% 
  group_by(SPEC) %>% 
  summarise(rmse = sqrt(mean(squared_residuals)),
            mad = median(abs_back_residual))


#how do the fitted compare to the observed?
res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm")

res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm") +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_point(aes(log_hg, fitted.value)) +
  geom_smooth(aes(log_hg, fitted.value), method = "lm")

fit.res <- lm(log_hg ~ fitted.value, data = res.summary)
summary(fit.res)
plot(fit.res)

fit.res <- lm(fitted.value ~ log_hg, data = res.summary)
summary(fit.res)

#map of residuals
mn <- map_data("state") %>% 
  filter(region == "minnesota")

res.summary.map <- res.summary %>% 
  group_by(DOW, SPEC) %>% 
  summarise(mean.res = mean(.residual)) %>% 
  left_join(lake_link) 

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = res.summary.map, aes(lake_lon_decdeg, lake_lat_decdeg, color = mean.res), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position.inside  = c(.72, .4)) +
  coord_fixed() +
  facet_wrap(~SPEC)

#fitted values vs. length.. how are we shaping up?
res.summary %>% 
  ggplot() +
  geom_smooth(aes(scaled_length, fitted.value, color = SPEC))

res.summary %>% 
  filter(scaled_length >= -2 & scaled_length <= 2) %>% 
  ggplot() +
  geom_smooth(aes(scaled_length, fitted.value, color = SPEC))

res.summary %>% 
  ggplot() +
  geom_smooth(aes(scaled_length, exp(fitted.value), color = SPEC))

res.summary %>% 
  ggplot() +
  geom_point(aes(scaled_length, fitted.value, color = SPEC)) +
  geom_smooth(aes(scaled_length, fitted.value, color = SPEC))
```

#Modeling with removed lake effect
```{r}
lake_link <- read_csv("Data/lake_link.csv") %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

hg.data <- hg.model %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(HGPPM = case_when(HGPPM <= 0.01 ~ HGPPM/2,
                           TRUE ~ HGPPM)) %>% 
  #species with more than 1,000 observations 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  #post method change and more recent 
  filter(YEARCOLL > 1999) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(WS_Lake_Ratio < 100) %>% 
  #transforming some variables for modeling
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area))) %>% 
  #linking data to lat/lon
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  left_join(lake_link) %>% 
  #removes lakes where lat/lon was not grabbed - worth checking in later
  filter(!is.na(lake_lat_decdeg))
rm(hg, hg.model, lake_link)

#fitting model
fit.norand <- brm(log_hg ~ scaled_length*SPEC + SPEC + scaled_logit_wetlands*SPEC + scaled_log_WS_Lake_Ratio*SPEC + scaled_log_area*SPEC + scaled_clarity*SPEC,
                     data = hg.data,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 10)
#saveRDS(fit.norand, "statewide_norandom.rds")
fit.norand <- readRDS("statewide_norandom.rds")

plot(fit.norand)
summary(fit.norand)
plot(conditional_effects(fit.norand))

#residuals
res <- hg.data %>% 
  select(SPEC,
         scaled_logit_wetlands,
         scaled_log_WS_Lake_Ratio,
         scaled_log_area,
         scaled_clarity,
         scaled_length,
         log_hg) %>%
  drop_na() %>% 
  add_residual_draws(fit.norand)

res.summary <- res %>% 
  median_qi(.residual) %>% 
  mutate(fitted.value = log_hg - .residual)
#write_csv(res.summary, "res_summary_norand.csv")
res.summary <- read_csv("res_summary_norand.csv")

#looking at residuals 
#how do the fitted compare to the observed?
res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm")

res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm") +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_point(aes(log_hg, fitted.value)) +
  geom_smooth(aes(log_hg, fitted.value), method = "lm")

#RMSE and mad
res.summary %>% 
  mutate(squared_residuals = .residual^2,
         fitted_back = exp(fitted.value),
         obs_back = exp(log_hg),
         back_residual = obs_back - fitted_back,
         abs_back_residual = abs(back_residual)) %>% 
  group_by(SPEC) %>% 
  summarise(rmse = sqrt(mean(squared_residuals)),
            mad = median(abs_back_residual))


#fitted values vs length
res.summary %>% 
  ggplot() +
  geom_smooth(aes(scaled_length, fitted.value, color = SPEC))

res.summary %>% 
  filter(scaled_length >= -2 & scaled_length <= 2) %>% 
  ggplot() +
  geom_smooth(aes(scaled_length, fitted.value, color = SPEC))

res.summary %>% 
  ggplot() +
  geom_smooth(aes(scaled_length, exp(fitted.value), color = SPEC))

res.summary %>% 
  ggplot() +
  geom_point(aes(scaled_length, fitted.value, color = SPEC)) +
  geom_smooth(aes(scaled_length, fitted.value, color = SPEC))

#outliers
hg.data %>% 
  filter(SPEC == "WTS" & scaled_length < -4)
#all fish under 7 inches and appear to be at the LOD

hg.data %>% 
  filter(HGPPM <= 0.01) %>% 
  group_by(SPEC) %>% 
  count()

hg.data %>% 
  filter(HGPPM <= 0.01) %>% 
  summarise(n_distinct(DOW))

hg.data %>% 
  filter(HGPPM <= 0.01) %>% 
  summarise(n_distinct(YEARCOLL))
#no heavy patterns in samples at LOD - all SPEC represented - 40 distinct lakes and 13 distinct years
```

#Modeling with gaussian processes for auto correlation - with only 100 lakes
```{r}
lake_link <- read_csv("Data/lake_link.csv") %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

#converting coordiantes to utm
lake_link_sf <- lake_link %>% 
  st_as_sf(coords = c("lake_lon_decdeg", "lake_lat_decdeg"), crs = 4326) %>% 
  st_transform(crs = 32615)
utm_coords <- st_coordinates(lake_link_sf)
lake_link <- lake_link %>%
  mutate(easting = utm_coords[, 1]/1000,
         northing = utm_coords[, 2]/1000)
#utm values in km instead of m
rm(lake_link_sf, utm_coords)

hg.data <- hg.model %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(HGPPM = case_when(HGPPM <= 0.01 ~ HGPPM/2,
                           TRUE ~ HGPPM)) %>% 
  #species with more than 1,000 observations 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  #post method change and more recent 
  filter(YEARCOLL > 1999) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(WS_Lake_Ratio < 100) %>% 
  #transforming some variables for modeling
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area))) %>% 
  #linking data to lat/lon
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  left_join(lake_link) %>% 
  #removes lakes where lat/lon was not grabbed - worth checking in later
  filter(!is.na(lake_lat_decdeg))
rm(hg, hg.model)

#sample model run
unique_lakes <- hg.data %>% 
  select(DOW) %>% 
  distinct()

set.seed(123)
sample_lakes <- unique_lakes %>% 
  sample_n(100)

hg.data.sample <- hg.data %>% 
  filter(DOW %in% sample_lakes$DOW)
rm(unique_lakes, sample_lakes)


#fitting model
fit.gp <- brm(log_hg ~ scaled_length*SPEC + SPEC + scaled_logit_wetlands + scaled_log_WS_Lake_Ratio + scaled_log_area + scaled_clarity + gp(easting, northing),
                     data = hg.data.sample,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 10)
#saveRDS(fit.gp, "statewide_gp_sample.rds")
fit.gp <- readRDS("statewide_gp_sample.rds")

#summaries
plot(fit.gp)
summary(fit.gp)
plot(conditional_effects(fit.gp, ndraws = 500))

#residuals
res <- hg.data.sample %>% 
  select(SPEC,
         scaled_logit_wetlands,
         scaled_log_WS_Lake_Ratio,
         scaled_log_area,
         scaled_clarity,
         scaled_length,
         log_hg,
         easting, 
         northing) %>%
  drop_na() %>% 
  add_residual_draws(fit.gp)

res.summary <- res %>% 
  median_qi(.residual) %>% 
  mutate(fitted.value = log_hg - .residual)
#write_csv(res.summary, "res_summary_gp_sample.csv")
res.summary <- read_csv("res_summary_gp_sample.csv")

#looking at residuals 
#how do the fitted compare to the observed?
res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm")

res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm") +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_point(aes(log_hg, fitted.value)) +
  geom_smooth(aes(log_hg, fitted.value), method = "lm")

#RMSE and mad
res.summary %>% 
  mutate(squared_residuals = .residual^2,
         fitted_back = exp(fitted.value),
         obs_back = exp(log_hg),
         back_residual = obs_back - fitted_back,
         abs_back_residual = abs(back_residual)) %>% 
  group_by(SPEC) %>% 
  summarise(rmse = sqrt(mean(squared_residuals)),
            mad = median(abs_back_residual))

#prob of over 0.2 for range of lengths of each species in each lake
length_ranges <- hg.data.sample %>% 
  group_by(SPEC) %>% 
  summarize(min_length = min(scaled_length), max_length = max(scaled_length))

length_seq <- length_ranges %>% 
  rowwise() %>% 
  mutate(scaled_length = list(seq(min_length, max_length, length.out = 25))) %>% 
  unnest(cols = c(scaled_length))

pred_data <- hg.data.sample %>% 
  select(easting, 
         northing, 
         SPEC, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area, 
         scaled_clarity) %>% 
  distinct() %>% 
  inner_join(length_seq, by = "SPEC")

#prob of average size
pred_data <- hg.data.sample %>% 
  select(easting, 
         northing, 
         SPEC, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area, 
         scaled_clarity) %>% 
  distinct() %>% 
  mutate(scaled_length = 0)

#predictions <- add_predicted_draws(fit.gp, newdata = pred_data)
#write_csv(predictions, "preds_gp_sample.csv")
predictions <- read_csv("preds_gp_sample.csv")

threshold <- predictions %>% 
  mutate(is_above_threshold = .prediction > log(0.2)) %>% 
  group_by(SPEC, scaled_length, .draw) %>% 
  summarise(prob = mean(is_above_threshold), .groups = 'drop') %>% 
  group_by(SPEC, scaled_length) %>% 
  summarise(probability_above_threshold = mean(prob),
            lower_ci = quantile(prob, 0.025),
            upper_ci = quantile(prob, 0.975))

threshold %>% 
  ggplot(aes(x = scaled_length, y = probability_above_threshold, color = SPEC)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, group = SPEC, fill = SPEC), alpha = 0.1, color = NA) +
  labs(x = "Scaled Length", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_minimal()
```

#GP with all lakes
```{r}
lake_link <- read_csv("Data/lake_link.csv") %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

#converting coordiantes to utm
lake_link_sf <- lake_link %>% 
  st_as_sf(coords = c("lake_lon_decdeg", "lake_lat_decdeg"), crs = 4326) %>% 
  st_transform(crs = 32615)
utm_coords <- st_coordinates(lake_link_sf)
lake_link <- lake_link %>%
  mutate(easting = utm_coords[, 1]/1000,
         northing = utm_coords[, 2]/1000)
#utm values in km instead of m
rm(lake_link_sf, utm_coords)

hg.data <- hg.model %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(HGPPM = case_when(HGPPM <= 0.01 ~ HGPPM/2,
                           TRUE ~ HGPPM)) %>% 
  #species with more than 1,000 observations 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  #post method change and more recent 
  filter(YEARCOLL > 1999) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(WS_Lake_Ratio < 100) %>% 
  #transforming some variables for modeling
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area)),
         scaled_logit_wetlands = c(scale(logit_wetlands)),
         scaled_log_WS_Lake_Ratio = c(scale(log_WS_Lake_Ratio)),
         scaled_clarity = c(scale(clarity))) %>% 
  #linking data to lat/lon
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  left_join(lake_link) %>% 
  #removes lakes where lat/lon was not grabbed - worth checking in later
  filter(!is.na(lake_lat_decdeg)) %>% 
  filter(!is.na(clarity)) %>% 
  #filtering out super long or short fish
  filter(!(scaled_length > 3 | scaled_length < -3))
rm(hg, hg.model, lake_link)

#fitting model
fit.gp <- brm(log_hg ~ scaled_length*SPEC + SPEC + scaled_logit_wetlands + scaled_log_WS_Lake_Ratio + scaled_log_area + scaled_clarity + gp(easting, northing),
                     data = hg.data,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 12)
#saveRDS(fit.gp, "statewide_gp.rds")
fit.gp <- readRDS("statewide_gp.rds")

#summaries
plot(fit.gp)
summary(fit.gp)
plot(conditional_effects(fit.gp, ndraws = 100))

#residuals - I estimate it would take about 2 hours to run all samples for 50 draws
res.summary <- hg.data %>% 
  select(SPEC,
         scaled_logit_wetlands,
         scaled_log_WS_Lake_Ratio,
         scaled_log_area,
         scaled_clarity,
         scaled_length,
         log_hg,
         easting, 
         northing) %>%
  drop_na() %>% 
  add_residual_draws(fit.gp, ndraws = 50) %>% 
  median_qi() %>% 
  mutate(fitted.value = log_hg - .residual)
write_csv(res.summary, "res_summary_gp_sum.csv")
res.summary <- read_csv("res_summary_gp_sum.csv")


#looking at residuals 
#how do the fitted compare to the observed?
res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm")

res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm") +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_point(aes(log_hg, fitted.value)) +
  geom_smooth(aes(log_hg, fitted.value), method = "lm")

#with jitter - might have to play around with the width and height
res.summary %>% 
  ggplot() +
  geom_jitter(aes(log_hg, fitted.value), width = 0.5, height = 0.5) +
  geom_smooth(aes(log_hg, fitted.value), method = "lm")

#high and low lengths? - not applicable to the newest model run as I filtered these points out
res.summary %>% 
  mutate(long_short_length = case_when(scaled_length > 3 | scaled_length < -3 ~ "Y",
                                 TRUE ~ "N")) %>% 
  ggplot() +
  geom_point(aes(log_hg, fitted.value)) +
  geom_point(. %>% filter(long_short_length == "Y"), mapping = aes(log_hg, fitted.value), color = "orange") +
  geom_smooth(aes(log_hg, fitted.value), method = "lm")

res.summary %>% 
  mutate(long_short_length = case_when(scaled_length > 3 | scaled_length < -3 ~ "Y",
                                 TRUE ~ "N")) %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_point(. %>% filter(long_short_length == "Y"), 
             mapping = aes(log_hg, fitted.value), color = "orange") +
  geom_smooth(aes(fitted.value, log_hg), method = "lm") +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~SPEC, scales = "free")


#RMSE and mad
res.summary %>% 
  mutate(squared_residuals = .residual^2,
         fitted_back = exp(fitted.value),
         obs_back = exp(log_hg),
         back_residual = obs_back - fitted_back,
         abs_back_residual = abs(back_residual)) %>% 
  group_by(SPEC) %>% 
  summarise(rmse = sqrt(mean(squared_residuals)),
            mad = median(abs_back_residual))

#just overall
res.summary %>% 
  mutate(squared_residuals = .residual^2,
         fitted_back = exp(fitted.value),
         obs_back = exp(log_hg),
         back_residual = obs_back - fitted_back,
         abs_back_residual = abs(back_residual)) %>% 
  summarise(rmse = sqrt(mean(squared_residuals)),
            mad = median(abs_back_residual))

#prob of over 0.2 for range of lengths of each species 
length_ranges <- hg.data %>% 
  group_by(SPEC) %>% 
  summarize(min_length = min(scaled_length), max_length = max(scaled_length))

length_seq <- length_ranges %>% 
  rowwise() %>% 
  mutate(scaled_length = list(seq(min_length, max_length, length.out = 100))) %>% 
  unnest(cols = c(scaled_length))

pred_data <- hg.data %>% 
  select(easting, 
         northing, 
         SPEC, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area, 
         scaled_clarity) %>% 
  distinct() %>% 
  drop_na() %>%
  inner_join(length_seq, by = "SPEC")
rm(length_seq, length_ranges)
#3137 unique lake-spec combos - thus, 156850 prediction points with 50 lengths

#memory intensive step - started at 11:15am 7/17
predictions <- pred_data %>% 
  add_predicted_draws(fit.gp, ndraws = 100) 
  #median_qi()
#write_csv(predictions, "preds_gp_sum.csv")
predictions <- read_csv("preds_gp_sum.csv") %>% 
  mutate(easting.factor = as.factor(easting),
         northing.factor = as.factor(northing))

#adding back actual lengths
length_stats <- hg.data %>% 
  group_by(SPEC) %>% 
  summarise(mean_length = mean(LGTHIN),
            sd_length = sd(LGTHIN),
            min_length = min(LGTHIN),
            max_length = max(LGTHIN))

threshold <- predictions %>% 
  mutate(is_above_threshold = .prediction > log(0.2)) %>% 
  group_by(SPEC, scaled_length, .draw) %>% 
  summarise(prob = mean(is_above_threshold), .groups = 'drop') %>%   
  group_by(SPEC, scaled_length) %>% 
  summarise(probability_above_threshold = mean(prob),
            lower_ci = quantile(prob, 0.025),
            upper_ci = quantile(prob, 0.975)) %>% 
  left_join(length_stats, by = "SPEC") %>% 
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length) 
  
#with all draws
draw_sum <- predictions %>% 
  mutate(is_above_threshold = .prediction > log(0.2)) %>% 
  group_by(SPEC, scaled_length, .draw) %>% 
  summarise(prob = mean(is_above_threshold), .groups = 'drop') %>% 
  left_join(length_stats, by = "SPEC") %>% 
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length)

ggplot() +
  geom_line(data = draw_sum, aes(x = LGTHIN, y = prob, group = interaction(.draw, SPEC), color = SPEC), alpha = 0.1) +
  geom_line(data = threshold, aes(x = LGTHIN, y = probability_above_threshold, color = SPEC)) +
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm")

ggplot() +
  geom_line(data = draw_sum, aes(x = LGTHIN, y = prob, group = interaction(.draw, SPEC), color = SPEC), alpha = 0.05) +
  geom_line(data = threshold, aes(x = LGTHIN, y = probability_above_threshold)) +
  scale_y_continuous(limits = c(0,1)) +
  facet_wrap(~SPEC, scales = "free_x") +
  geom_hline(yintercept = .5, color = "red", linetype = "dashed") +
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_bw() +
  theme(legend.position = "none")
ggsave("prob_exceeding.png", width = 5, height = 5)

#lake specific variability 
check <- hg.data %>% 
  mutate(easting.factor = as.factor(easting),
         northing.factor = as.factor(northing),
         parent_dow = substr(DOW, 1, 6)) %>%
  #one lake has multi basins and a few connected lakes get the same UTM, fixing that to get correct join
  mutate(parent_dow = case_when(parent_dow == "110415" ~ "040030",
                         parent_dow == "860230" ~ "860229",
                         TRUE ~ parent_dow)) %>% 
  distinct(easting.factor, northing.factor, parent_dow) %>% 
  right_join(predictions, by = c("easting.factor",
                                 "northing.factor"))

#leech lake specific
leech.data <- hg.data %>% 
  filter(DOW == "11020300") %>% 
  select(WATERWAY, DOW, easting, northing) %>% 
  distinct(WATERWAY, DOW, easting, northing) 
#leech has data for BLC, NOP, WAE, and YEP

leech <- predictions %>% 
  filter(easting >= 392.912 & easting <= 392.913) %>% 
  filter(SPEC == "WAE")
  left_join(length_stats, by = "SPEC")
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length)
glimpse(leech)

leech %>% 
ggplot() +
  geom_point(aes(x = LGTHIN, y = exp(.prediction))) +
  geom_smooth(aes(x = LGTHIN, y = exp(.prediction))) +
  geom_hline(yintercept = 0.2) +
  facet_wrap(~SPEC, scales = "free")

threshold.leech <- leech %>% 
  mutate(is_above_threshold = .prediction > log(0.2)) %>% 
  group_by(SPEC, scaled_length, .draw) %>% 
  summarise(prob = mean(is_above_threshold), .groups = 'drop') %>%   
  group_by(SPEC, scaled_length) %>% 
  summarise(probability_above_threshold = mean(prob),
            lower_ci = quantile(prob, 0.025),
            upper_ci = quantile(prob, 0.975)) %>% 
  left_join(length_stats, by = "SPEC") %>% 
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length) 
  
#with all draws
draw_sum.leech <- leech %>% 
  mutate(is_above_threshold = .prediction > log(0.2)) %>% 
  group_by(SPEC, scaled_length, .draw) %>% 
  summarise(prob = mean(is_above_threshold), .groups = 'drop') %>% 
  left_join(length_stats, by = "SPEC") %>% 
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length)

ggplot() +
  geom_line(data = draw_sum.leech, aes(x = LGTHIN, y = prob, group = interaction(.draw, SPEC)), alpha = 0.1) +
  geom_line(data = threshold.leech, aes(x = LGTHIN, y = probability_above_threshold)) +
  scale_y_continuous(limits = c(0,1)) +
  facet_wrap(~SPEC, scales = "free_x") +
  geom_hline(yintercept = .5, color = "red", linetype = "dashed") +
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_bw()

threshold.leech %>% 
  ggplot(aes(x = LGTHIN, y = probability_above_threshold)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.1, color = NA) +
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_minimal() +
  scale_y_continuous(limits = c(0,1)) +
  geom_hline(yintercept = .5, linetype = "dashed") +
  facet_wrap(~SPEC, scales = "free")
rm(leech, leech.data, threshold.leech, draw_sum.leech)

#standardized length
threshold %>% 
  ggplot(aes(x = scaled_length, y = probability_above_threshold, color = SPEC)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, group = SPEC, fill = SPEC), alpha = 0.1, color = NA) +
  labs(x = "Scaled Length", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_minimal()

#actual length
threshold %>% 
  ggplot(aes(x = LGTHIN, y = probability_above_threshold, color = SPEC)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, group = SPEC, fill = SPEC), alpha = 0.1, color = NA) +
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_minimal()

threshold %>% 
  ggplot(aes(x = LGTHIN, y = probability_above_threshold)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.1, color = NA) +
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_minimal() +
  scale_y_continuous(limits = c(0,1)) +
  geom_hline(yintercept = .5, linetype = "dashed") +
  facet_wrap(~SPEC, scales = "free")

#prob of average size
pred_data_avg <- hg.data %>% 
  select(easting, 
         northing, 
         SPEC, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area, 
         scaled_clarity) %>% 
  distinct() %>% 
  mutate(scaled_length = 0)

#exploring spatial fit
grid <- hg.data %>% 
  filter(SPEC == "WAE") %>% 
  select(easting, 
         northing, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area,
         scaled_clarity) %>% 
  distinct()

grid_data <- grid %>%
  mutate(
    scaled_length = 0,
    SPEC = factor("WAE"))
rm(grid)

predictions <- grid_data %>% 
  add_predicted_draws(fit.gp, ndraws = 50)
predictions_sum <- predictions %>% 
  median_qi() %>% 
  mutate(easting = easting*1000,
         northing = northing*1000)


# Plot the spatial effect
mn_boundary <- tigris::states(cb = TRUE) %>% 
  filter(STUSPS == "MN") %>%
  st_transform(crs = st_crs("+proj=utm +zone=15 +datum=WGS84"))
predictions_sf <- st_as_sf(predictions_sum, coords = c("easting", "northing"), crs = st_crs(mn_boundary))

ggplot() +
  geom_sf(data = mn_boundary) +
  geom_sf(data = predictions_sf, aes(color = exp(.prediction))) +
  scale_color_viridis_c() +  
  theme_minimal()

rm(predictions, 
   predictions_sf, 
   predictions_sum, 
   mn_boundary,
   grid_data)

#how is scaled length working?
length_stats %>% 
  mutate(LGTHIN = (0*sd_length) + mean_length)

hg.data %>% 
  filter(scaled_length >= -0.01 & scaled_length <= 0.01) %>% 
  group_by(SPEC) %>% 
  summarise(mean.length = mean(LGTHIN),
            n = n())

```

#Multivarite approach 
```{r}
wae.nop <- hg.model.filtered %>% 
  filter(SPEC %in% c("WAE", "NOP")) %>% 
  filter(WS_Lake_Ratio < 250)

formula <- bf(log.hg ~ scaled_length + SPEC + scaled_log_WS_Lake_Ratio + scaled_logit_wetlands + scaled_log_area + scaled_clarity + (1|DOW:SPEC) + (1|DOW) + (1|year),
              sigma ~ SPEC)

fit.wae.nop <- brm(formula,
             data = wae.nop,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
saveRDS(fit.wae.nop, "wae_nop_fit.rds")

summary(fit.wae.nop)
plot(conditional_effects(fit.wae.nop))
round(vcov(fit.wae),5)

random_effects <- ranef(fit.wae)
dow.spec <- as.data.frame(random_effects$`DOW:SPEC`)
```

#HMSC
```{r}
library(Hmsc)

#filtered data
hg.model.filtered <- hg.model %>% 
  filter(TYPE == "Lake") %>% 
  filter(YEARCOLL >1996) %>% 
  filter(!(is.na(DOW) |
             is.na(ANATHG) |
             is.na(YEARCOLL) |
             is.na(LGTHIN) |
             is.na(Total_Wetland_Hectares)|
             is.na(Percent_Wetland) |
             is.na(WS_Lake_Ratio) |
             is.na(centroid_lat) |
             is.na(area) |
             is.na(clarity) |
             is.na(scaled_length))) %>% 
  mutate(year = factor(YEARCOLL),
         DOW = factor(DOW)) %>% 
  mutate(log.hg = log(HGPPM)) 
#summary of the data
summary(hg.model.filtered)
rm(hg.model, hg)


#three species
three_spec <- hg.model.filtered %>% 
  filter(SPEC %in% c("WAE", "NOP", "BLC")) %>% 
  select(DOW,
         year,
         SPEC,
         log.hg,
         scaled_length,
         scaled_Percent_Wetland,
         log_WS_Lake_Ratio,
         log_area,
         scaled_clarity) 

test <- three_spec %>% 
  select(SPEC,
         log.hg,
         DOW,
         year) %>% 
  pivot_wider(id_cols = c(DOW, year),
    names_from = SPEC,
              values_from = log.hg)

#creating a length based correction for fish at the lake level
wae <- hg.model.filtered %>% 
  filter(SPEC == "WAE")
length.correction <- brm(log.hg ~ scaled_length + (1|DOW),
                         data = wae,
                         family = gaussian())
summary(length.correction)
conditional_effects(length.correction)

#trying with brms
all_species <- hg.model.filtered %>% 
  filter(SPEC %in% c("NOP", "YEP", "WAE", "BLG", "LMB"))

fit.all <- brm(log.hg ~ scaled_length +
               scaled_Percent_Wetland + 
               scaled_log_area + 
               scaled_log_WS_Lake_Ratio +
               (1|year) + 
               (1|DOW/SPEC),
             data = all_species,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
saveRDS(fit.all, "all_species_model.rds")
summary(fit.all)
conditional_effects(fit.all)

```

#map of all lakes
```{r}
zm <- read_csv("Data/ZM_MN_NHD.csv") %>% 
  rename(DOWID = DOW) %>% 
  mutate(DOWID = fixlakeid(DOWID)) %>% 
  mutate(DOW = DOWID)

hg_zm <- hg.model %>% 
  left_join(zm) %>% 
  mutate(zm_lake = case_when(!is.na(Year_Infested) ~ "y",
                             TRUE ~ "n"),
         pre_post = case_when(YEARCOLL >= Year_Infested ~ "post",
                              YEARCOLL < Year_Infested ~ "pre",
                              TRUE ~ "never"))

map <- hg_zm %>% 
  left_join(lake_link) %>% 
  distinct(DOW, .keep_all = T) %>% 
  filter(!is.na(easting)) %>% 
  mutate(easting = easting*1000,
         northing = northing*1000)

mn_boundary <- tigris::states(cb = TRUE) %>% 
  filter(STUSPS == "MN") %>%
  st_transform(crs = st_crs("+proj=utm +zone=15 +datum=WGS84"))
map_sf <- st_as_sf(map, coords = c("easting", "northing"), crs = st_crs(mn_boundary))


ggplot() +
  geom_sf(data = mn_boundary, fill = "white") +
  geom_sf(data = map_sf, aes(color = zm_lake, shape = zm_lake)) +
  theme_minimal() +
  scale_color_manual("Zebra Mussel Lake", values = c("lightblue", "salmon"), labels = c("No", "Yes")) +
  scale_shape_manual("Zebra Mussel Lake", values = c(16, 17), labels = c("No", "Yes")) +
  theme(legend.position = c(.8, .32),
        legend.background = element_rect(fill = "white", color = "black", size = 0.5),
    legend.box.background = element_rect(color = "black"))
ggsave("hg_map_all_lakes.png", width = 5, height = 5, bg = "white")
rm(hg_zm, hg.model, lake_link, map, map_sf, mn_boundary, zm)
```


#Old code
```{r}
################linking to lagos data############
#load all lagos data
lagos <- lagosne_load()
#extract data relavent to Hg
#epi_nutr contains limno parameters of interest - espically carbon
#hu12 conn gives watershed level paramters for wetlends
#huc12 lulc gives parameters for watershed level landcover
#state gives state level parameters
#filter for just MN lakes 
lg <- left_join(lagos$epi_nutr, lagos$locus)
lg <- left_join(lg, lagos$hu12.conn)
lg <- left_join(lg, lagos$hu12.lulc)
lg <- left_join(lg, lagos$state)
lg.sum <- lg %>% 
  filter(state == "MN") %>% 
  select(state,
         state_name,
         sampledate,
         nhdid,
         hu12_zoneid,
         gnis_name,
         nhd_lat,
         nhd_long,
         lake_area_ha,
         lake_perim_meters,
         doc,
         doc_qual,
         doc_censorcode,
         doc_detectionlimit,
         doc_labmethodname,
         toc,
         toc_qual,
         toc_censorcode,
         toc_detectionlimit,
         toc_labmethodname,
         secchi,
         secchi_qual,
         secchi_censorcode,
         secchi_methodinfo,
         hu12_nlcd2011_pct_90,
         hu12_nlcd2011_ha_90,
         hu12_nlcd2011_pct_95,
         hu12_nlcd2011_ha_95) %>% 
  group_by(nhdid) %>% 
  summarise(meanDOC = mean(doc, na.rm = T),
            meanTOC = mean(toc, na.rm = T),
            meanSECCHI = mean(secchi, na.rm = T),
            mean95 = mean(hu12_nlcd2011_pct_95, na.rm = T),
            mean90 = mean(hu12_nlcd2011_pct_90, na.rm = T),
            meanAREA = mean(lake_area_ha, na.rm = T))

#joining to Hg data
hg.lg <- left_join(hg, lg.sum)

#secchi data
library(dataRetrieval)
library(tidyverse)

secchi.names <- c(
  "Depth, Secchi disk depth",
  "Depth, Secchi disk depth (choice list)",
  "Secchi Reading Condition (choice list)",
  "Water transparency, Secchi disc"
)
args <- list(
  statecode = "MN",
  characteristicName = secchi.names
)
secchi <- readWQPdata(args)
glimpse(secchi)

secchi.clean <- secchi %>%
  filter(!(ResultMeasure.MeasureUnitCode %in% c("None", NA))) %>% 
  mutate(ResultMeasureValue = as.numeric(ResultMeasureValue)) %>% #some strings in the result column
  filter(!is.na(ResultMeasureValue)) %>% 
  mutate(ResultMeasureValue_clean = case_when(ResultMeasure.MeasureUnitCode == "cm" ~ ResultMeasureValue*.01,
                                              ResultMeasure.MeasureUnitCode == "ft" ~ ResultMeasureValue*0.3048,
                                              ResultMeasure.MeasureUnitCode == "in" ~ ResultMeasureValue*0.0254,
                                              TRUE ~ ResultMeasureValue),
         ResultMeasure.MeasureUnitCode_clean = "m") %>% 
  select(OrganizationIdentifier,
         ActivityIdentifier,
         ActivityTypeCode,
         ActivityMediaName,
         ActivityMediaSubdivisionName,
         ActivityStartDate,
         ProjectIdentifier,
         MonitoringLocationIdentifier,
         CharacteristicName,
         ResultMeasureValue,
         ResultMeasureValue_clean,
         ResultMeasure.MeasureUnitCode,
         ResultMeasure.MeasureUnitCode_clean,
         ProviderName)
glimpse(secchi.clean)

secchi.clean %>% 
  summarise(min.year = min(year(ActivityStartDate)),
            max.year = max(year(ActivityStartDate)))

secchi.clean %>% 
  group_by(ResultMeasure.MeasureUnitCode, ResultDepthHeightMeasure.MeasureUnitCode_clean) %>% 
  count()

secchi.clean %>% 
  filter(ResultMeasure.MeasureUnitCode != "m") %>% 
  group_by(ResultMeasureValue, ResultMeasureValue_clean) %>% 
  count() %>% 
  print(n = nrow(.))

secchi.clean %>% 
  group_by(OrganizationIdentifier) %>% 
  count() %>% 
  print(n = nrow(.))

secchi.clean %>% 
  group_by(MonitoringLocationIdentifier) %>% 
  count() %>% 
  print(n = nrow(.))

#need a crosswalk to get to get lake info connected to the site id

hg %>% 
  filter(YEARCOLL == "2023") %>% 
  group_by(WATERWAY) %>% 
  count() %>% 
  print(n = nrow(.))

#old code for calculating specs specific without predictions
dow_effect_species <- fit.statewide %>% 
  spread_draws(b_Intercept, 
               b_SPECWAE, 
               b_SPECBLG,
               b_SPECLMB,
               b_SPECNOP,
               b_SPECWTS,
               b_SPECYEP,
               b_scaled_length, 
               `b_scaled_length:SPECWAE`, 
               `b_scaled_length:SPECNOP`,
               `b_scaled_length:SPECBLG`,
               `b_scaled_length:SPECLMB`,
               `b_scaled_length:SPECWTS`,
               `b_scaled_length:SPECYEP`,
               b_scaled_logit_wetlands, 
               b_scaled_log_WS_Lake_Ratio,
               b_scaled_log_area, 
               b_scaled_clarity, 
               r_DOW[DOW,]) %>% 
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>%
  #since all variables are scaled, when the are 0 (excluded) they are at their means
  #thus, length and lake level parameters are excluded from the calculations below
  mutate(nop.median = b_Intercept + b_SPECNOP + b_SPECNOP +  r_DOW,
         wae.median = b_Intercept + b_SPECWAE + r_DOW,
         lmb.median = b_Intercept + b_SPECLMB + r_DOW,
         blg.median = b_Intercept + b_SPECBLG + r_DOW,
         wts.median = b_Intercept + b_SPECWTS + r_DOW,
         yep.median = b_Intercept + b_SPECWAE + r_DOW,
         blc.median = b_Intercept + r_DOW,
         nop.hg = exp(nop.median),
         wae.hg = exp(wae.median),
         lmb.hg = exp(lmb.median),
         blg.hg = exp(blg.median),
         wts.hg = exp(wts.median),
         yep.hg = exp(yep.median),
         blc.hg = exp(blc.median)) %>% 
  group_by(DOW) %>% 
  median_qi(nop.hg, 
            wae.hg,
            lmb.hg,
            blg.hg,
            wts.hg,
            yep.hg,
            blc.hg,
            .width = c(.9, .66)) %>% 
  left_join(lake_link) %>% 
  mutate(state_threshold.nop = case_when(nop.hg >= 0.2 ~ "y", 
                                         nop.hg < 0.2 ~ "n"),
         state_threshold.wae = case_when(wae.hg >= 0.2 ~ "y",
                                         wae.hg < 0.2 ~ "n"),
         state_threshold.lmb = case_when(lmb.hg >= 0.2 ~ "y",
                                         lmb.hg < 0.2 ~ "n"),
         state_threshold.blg = case_when(blg.hg >= 0.2 ~ "y",
                                         blg.hg < 0.2 ~ "n"),
         state_threshold.blc = case_when(blc.hg >= 0.2 ~ "y",
                                         blc.hg < 0.2 ~ "n"),
         state_threshold.wts = case_when(wts.hg >= 0.2 ~ "y",
                                         wts.hg < 0.2 ~ "n"),
         state_threshold.yep = case_when(yep.hg >= 0.2 ~ "y",
                                         yep.hg < 0.2 ~ "n"))
```



