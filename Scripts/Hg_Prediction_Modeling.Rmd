---
title: "Hg_Prediction_Modeling"
author: "Denver Link"
date: "2024-02-27"
output: html_document
editor_options: 
  chunk_output_type: console
---

The general structure of this markdown
1. load in packages from the library
2. read in mercury data and attach cov data 
3. prediction model using brms and Gaussian processes 
4. old code used to explore data and/or old analysis 

#Library
```{r}
library(tidyverse)
library(arrow)
library(LAGOSNE)
library(mwlaxeref)
library(mnsentinellakes)
library(corrplot)
library(performance)
library(brms)
library(lme4)
library(Matrix)
library(car)
library(tidybayes)
library(rstanarm)
library(bayesplot)
library(loo)
library(arrow)
library(sf)
library(maps)
library(viridis)
library(tigris)
library(emmeans)
```

#Data
```{r}
#mercury data
hg <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Mercury Data", "allfish_data_04042024_JLO.csv")) %>% 
  filter(!(is.na(LGTHIN) | is.na(HGPPM) | is.na(DOWID))) %>%   #removes samples with no hg, length, or dow data
  filter(TYPE %in% c("Lake", "LAKE")) %>% 
  mutate(DOWID = fixlakeid(DOWID)) %>% 
  filter(DOWID != "16000100") %>% 
  local_to_nhdhr(from_colname = "DOWID", states = "mn") %>% 
  mutate(nhdhr.id = str_remove(nhdhr.id, "nhdhr_")) %>% 
  select(SAMPLENO,
         WATERWAY,
         TYPE,
         LOCATION,
         DOWID,
         nhdhr.id,
         DATECOL2,
         YEARCOLL,
         SPEC,
         ANATHG,
         NOFISH,
         LGTHIN,
         WTLB,
         AGE,
         SEX,
         HGPPM,
         HGCODE,
         HGLAB,
         HGCMMT) %>% 
  rename(DOW = DOWID,
         nhdid = nhdhr.id) %>% 
  ungroup()

#connection to covariates
dnr_covary <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "MN_lakes_watershed_lulc_fromDNR.csv")) %>% 
  mutate(logit_wetlands = car::logit(Percent_Wetland),
         logit_urban = car::logit(Percent_Urban),
         logit_ag = car::logit(Percent_Cultivated_Crops),
         log_WS_Lake_Ratio = log(WS_Lake_Ratio))

dnr_covary %>% 
  ggplot() +
  geom_histogram(aes(logit_wetlands))

dnr_covary %>% 
  ggplot() +
  geom_histogram(aes(log_WS_Lake_Ratio))

#scaled_dnr <- dnr_covary %>%
  #mutate_at(vars(Total_Pasture_Hay_Hectares:log_WS_Lake_Ratio), scale) %>% 
  #rename_with(~ paste0("scaled_", .), Total_Pasture_Hay_Hectares:log_WS_Lake_Ratio) 

#dnr_covary <- dnr_covary %>% 
  #left_join(scaled_dnr)
glimpse(dnr_covary)

#adding dnr covariates into the data
hg <- hg %>% 
  left_join(dnr_covary)
rm(dnr_covary, scaled_dnr)

#glm - for clarity and area values
glm <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "glm_lake_metadata.csv")) %>%
  filter(state == "MN") %>% 
  rename(nhdid = site_id) %>% 
  mutate(nhdid = gsub("^nhdhr_", "", nhdid)) %>% 
  mutate(clarity = case_when(is.infinite(clarity) ~ NA,
                             TRUE ~ clarity)) %>% 
  mutate(log_area = log(area))

glm %>% 
  ggplot() +
  geom_histogram(aes(log_area))

glm %>% 
  ggplot() +
  geom_histogram(aes(clarity))

hg <- hg %>% 
  left_join(glm) %>% 
  mutate(across(starts_with("scaled_"), as.vector))
rm(glm)
glimpse(hg)

#infested waters list
zm <- read_csv("Data/ZM_MN_NHD.csv") %>% 
  filter(!is.na(DOW)) %>% 
  mutate(DOW = as.character(gsub("mndow_", "", MNDOW_ID)),
         nhdid = NHD_ID) %>% 
  select(DOW,
         Year_Infested) %>% 
  mutate(zm_lag_year_infested = Year_Infested +2)

hg <- hg %>% 
  left_join(zm, by = c("DOW"))
rm(zm)

#lake link
lake_link <- read_csv("Data/lake_link.csv") %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

lake_link_sf <- lake_link %>% 
  st_as_sf(coords = c("lake_lon_decdeg", "lake_lat_decdeg"), crs = 4326) %>% 
  st_transform(crs = 32615)
utm_coords <- st_coordinates(lake_link_sf)
lake_link <- lake_link %>%
  mutate(easting = utm_coords[, 1]/1000,
         northing = utm_coords[, 2]/1000)
#utm values in km instead of m
rm(lake_link_sf, utm_coords)

hg <- hg %>% 
  left_join(lake_link)
rm(lake_link)


#data for modeling - select for columns of use 
hg.model <- hg %>%
  select(WATERWAY,
         TYPE,
         DOW,
         nhdid,
         DATECOL2,
         YEARCOLL,
         SPEC,
         ANATHG,
         NOFISH,
         LGTHIN,
         HGPPM,
         Total_Wetland_Hectares,
         Percent_Wetland,
         Percent_Urban,
         Percent_Cultivated_Crops,
         logit_wetlands,
         logit_urban,
         logit_ag,
         WS_Lake_Ratio,
         log_WS_Lake_Ratio,
         centroid_lat,
         centroid_lon,
         max_depth,
         area,
         log_area,
         elevation,
         clarity,
         Year_Infested,
         zm_lag_year_infested,
         lake_lat_decdeg,
         lake_lon_decdeg,
         easting,
         northing) %>% 
  mutate(zm_lake = case_when(is.na(Year_Infested) ~ "n",
                             TRUE ~ "y"),
         zm_sample = case_when(zm_lag_year_infested <= YEARCOLL ~ "y",
                               TRUE ~ "n"),
         time_since_invasion = YEARCOLL - zm_lag_year_infested)
```

#Models exploring spacial term
```{r}
####################filtering hg data for predictive modeling##################
hg.data <- hg.model %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(HGPPM = case_when(HGPPM <= 0.01 ~ HGPPM/2,
                           TRUE ~ HGPPM)) %>% 
  #species with more than 1,000 observations 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  #post method change and more recent 
  filter(YEARCOLL > 1999) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(WS_Lake_Ratio < 100) %>% 
  #linking data to lat/lon
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  #removes lakes where lat/lon was not grabbed - worth checking in later
  filter(!is.na(lake_lat_decdeg)) %>% 
  filter(!is.na(clarity)) %>% 
  #transforming some variables for modeling
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area)),
         scaled_logit_wetlands = c(scale(logit_wetlands)),
         scaled_logit_urban = c(scale(logit_urban)),
         scaled_logit_ag = c(scale(logit_ag)),
         scaled_log_WS_Lake_Ratio = c(scale(log_WS_Lake_Ratio)),
         scaled_clarity = c(scale(clarity))) %>% 
  #filtering out super long or short fish
  filter(!(scaled_length > 3 | scaled_length < -3)) 
rm(hg, hg.model)

#############lscale prior inverse gamma with 1% prob below 20 or above 600#####
fit.gp <- brm(log_hg ~ 
                scaled_length*SPEC + 
                SPEC + 
                scaled_logit_wetlands + 
                scaled_logit_urban +
                scaled_logit_ag + 
                scaled_log_WS_Lake_Ratio + 
                scaled_log_area + 
                zm_sample +
                gp(easting, northing, gr = T,  scale = F),
              prior = prior(inv_gamma(3, 664), class = "lscale", coef = "gpeastingnorthing"),
                     data = hg.data,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 12)

#saveRDS(fit.gp, "statewide_gp_lscale_prior.rds")
fit.gp <- readRDS("statewide_gp_lscale_prior.rds")

summary(fit.gp)
plot(fit.gp)

#inspect the prior
prior_summary(fit.gp)
get_variables(fit.gp)

post_lscale <- fit.gp %>% 
  spread_draws(lscale_gpeastingnorthing)

# Define the parameters for the prior
inv_gamma_density <- function(x, shape, scale) {
  ifelse(x > 0, (scale^shape) * exp(-scale / x) / (gamma(shape) * x^(shape + 1)), 0)
}

shape <- 3
scale <- 664

# Create a sequence of values for lscale
x_values <- seq(0.01, 600, length.out = 1000) # Avoid 0 since lscale > 0

# Compute the density
y_values <- inv_gamma_density(x_values, shape, scale)

# Create a data frame for plotting
prior_data <- data.frame(lscale = x_values, density = y_values)

# Plot the prior
ggplot(prior_data, aes(x = lscale, y = density)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title = "Default Prior for lscale",
    x = "lscale (length scale)",
    y = "Density"
  ) +
  geom_vline(xintercept = c(20, 600), color = "red")+
  theme_minimal()

#compare the plot
post_lscale %>% 
  ggplot() +
  geom_density(aes(lscale_gpeastingnorthing, fill = "orange"), alpha = .5)

prior_data %>% 
  ggplot() +
  geom_line(data = prior_data, aes(x = lscale, y = density, fill = "blue"), alpha = .5)

#########lscale with uniform, no less than 20 or larger than 100###############
# Set the range for the uniform distribution
min_val <- 20
max_val <- 100

# Create a tibble for the x values and corresponding density
uniform_data <- tibble(
  x = seq(min_val, max_val, length.out = 1000),
  density = dunif(x, min = min_val, max = max_val)
)

# Plot using ggplot
ggplot(uniform_data, aes(x = x, y = density)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title = "Uniform Distribution",
    x = "x",
    y = "Density"
  ) +
  theme_minimal()
rm(min_val, max_val, uniform_data)

fit.gp.uni <- brm(log_hg ~ 
                scaled_length*SPEC + 
                SPEC + 
                scaled_logit_wetlands + 
                scaled_logit_urban +
                scaled_logit_ag + 
                scaled_log_WS_Lake_Ratio + 
                scaled_log_area + 
                zm_sample +
                gp(easting, northing, gr = T,  scale = F),
              prior = set_prior("uniform(20, 100)", lb = 20, ub = 100, class = "lscale"),
                     data = hg.data,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 12)
#saveRDS(fit.gp.uni, "statewide_hg_gp_uni.rds")
fit.gp <- readRDS("statewide_hg_gp_uni.rds")

prior_summary(fit.gp)
summary(fit.gp)
plot(fit.gp)



##########fixing lscale???#################
trial <- brm(log_hg ~ 
                scaled_length*SPEC + 
                SPEC + 
                scaled_logit_wetlands + 
                scaled_logit_urban +
                scaled_logit_ag + 
                scaled_log_WS_Lake_Ratio + 
                scaled_log_area + 
                zm_sample +
                gp(easting, northing, gr = T,  scale = F),
              prior = prior(constant(10), class = "lscale", coef = "gpeastingnorthing"),
                     data = hg.data,
                     iter = 4000,
                     warmup = 200,
                     chains = 3,
                     family = gaussian(),
                     cores = 12)

prior_summary(trial)
summary(trial)
plot(trial)
```


#GP with all lakes
The general structure of this chunk
1. filter MN state hg data reverent to prediction modeling
  -notes on the filtering are included below
2. fitting model with brms
3. some generic model summary and checks
4. exploring residuals 
5. creating products based on the model
   -prob that a fish at a given length is over the threshold 
   -length vs hg plots
   -map of hg concentrations of an average size fish throughout the state of MN
```{r}
####################filtering hg data for predictive modeling##################
hg.data <- hg.model %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(HGPPM = case_when(HGPPM <= 0.01 ~ HGPPM/2,
                           TRUE ~ HGPPM)) %>% 
  #species with more than 1,000 observations 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  #post method change and more recent 
  filter(YEARCOLL > 1999) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(WS_Lake_Ratio < 100) %>% 
  #linking data to lat/lon
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  #removes lakes where lat/lon was not grabbed - worth checking in later
  filter(!is.na(lake_lat_decdeg)) %>% 
  filter(!is.na(clarity)) %>% 
  #transforming some variables for modeling
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area)),
         scaled_logit_wetlands = c(scale(logit_wetlands)),
         scaled_logit_urban = c(scale(logit_urban)),
         scaled_logit_ag = c(scale(logit_ag)),
         scaled_log_WS_Lake_Ratio = c(scale(log_WS_Lake_Ratio)),
         scaled_clarity = c(scale(clarity))) %>% 
  #filtering out super long or short fish
  filter(!(scaled_length > 3 | scaled_length < -3)) 
rm(hg, hg.model)

####################fitting model###########################
fit.gp <- brm(log_hg ~ 
                scaled_length*SPEC + 
                SPEC + 
                scaled_logit_wetlands + 
                scaled_logit_urban +
                scaled_logit_ag + 
                scaled_log_WS_Lake_Ratio + 
                scaled_log_area + 
                zm_sample +
                gp(easting, northing, gr = T, scale = F),
                     data = hg.data,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 12)
#saveRDS(fit.gp, "statewide_gp.rds")
fit.gp <- readRDS("statewide_gp.rds")

##################basic summaries###############################
get_prior(fit.gp)
plot(fit.gp)
summary(fit.gp)
plot(conditional_effects(fit.gp, ndraws = 100))

summary <- summary(fit.gp)
fixed <- as.data.frame(summary$fixed) %>% 
  rownames_to_column()
gp <- as.data.frame(summary$gp) %>% 
  rownames_to_column()
error <- as.data.frame(summary$spec_pars) %>% 
  rownames_to_column()
#write_csv(fixed, "fixed_gp.csv")
rm(fixed, gp, error)



######################residuals#############################################
res.summary <- hg.data %>% 
  select(SPEC,
         scaled_logit_wetlands,
         scaled_log_WS_Lake_Ratio,
         scaled_log_area,
         scaled_logit_urban,
         scaled_logit_ag,
         zm_sample,
         scaled_length,
         log_hg,
         easting, 
         northing) %>%
  drop_na() %>% 
  add_residual_draws(fit.gp, ndraws = 50) %>% 
  median_qi() %>% 
  mutate(fitted.value = log_hg - .residual)
#write_csv(res.summary, "res_summary_gp_sum.csv")
res.summary <- read_csv("res_summary_gp_sum.csv")


#looking at residuals 
#how do the fitted compare to the observed?
res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm")

res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm") +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_point(aes(log_hg, fitted.value)) +
  geom_smooth(aes(log_hg, fitted.value), method = "lm")

#with jitter - might have to play around with the width and height
res.summary %>% 
  ggplot() +
  geom_jitter(aes(log_hg, fitted.value), width = 0.5, height = 0.5) +
  geom_smooth(aes(log_hg, fitted.value), method = "lm")

#high and low lengths? - not applicable to the newest model run as I filtered these points out
res.summary %>% 
  mutate(long_short_length = case_when(scaled_length > 3 | scaled_length < -3 ~ "Y",
                                 TRUE ~ "N")) %>% 
  ggplot() +
  geom_point(aes(log_hg, fitted.value)) +
  geom_point(. %>% filter(long_short_length == "Y"), mapping = aes(log_hg, fitted.value), color = "orange") +
  geom_smooth(aes(log_hg, fitted.value), method = "lm")

res.summary %>% 
  mutate(long_short_length = case_when(scaled_length > 3 | scaled_length < -3 ~ "Y",
                                 TRUE ~ "N")) %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_point(. %>% filter(long_short_length == "Y"), 
             mapping = aes(log_hg, fitted.value), color = "orange") +
  geom_smooth(aes(fitted.value, log_hg), method = "lm") +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~SPEC, scales = "free")


#RMSE and mad
res.summary %>% 
  mutate(squared_residuals = .residual^2,
         fitted_back = exp(fitted.value),
         obs_back = exp(log_hg),
         back_residual = obs_back - fitted_back,
         abs_back_residual = abs(back_residual)) %>% 
  group_by(SPEC) %>% 
  summarise(rmse = sqrt(mean(squared_residuals)),
            mad = median(abs_back_residual))

#just overall
res.summary %>% 
  mutate(squared_residuals = .residual^2,
         fitted_back = exp(fitted.value),
         obs_back = exp(log_hg),
         back_residual = obs_back - fitted_back,
         abs_back_residual = abs(back_residual)) %>% 
  summarise(rmse = sqrt(mean(squared_residuals)),
            mad = median(abs_back_residual))

##########################probability of exceedance#####################
#create length ranges from original data truncated to fit min and max
length_ranges <- hg.data %>% 
  group_by(SPEC) %>% 
  summarize(min_length = min(scaled_length), max_length = max(scaled_length))
length_seq <- length_ranges %>% 
  rowwise() %>% 
  mutate(scaled_length = list(seq(min_length, max_length, length.out = 200))) %>% 
  unnest(cols = c(scaled_length))

#create data to predict on 
#this is based on original data where a SPEC-Lake combo was observed at least once
pred_data <- hg.data %>% 
  select(easting, 
         northing, 
         SPEC, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area,
         scaled_logit_urban,
         scaled_logit_ag,
         zm_sample) %>% 
  distinct() %>% 
  drop_na() %>%
  inner_join(length_seq, by = "SPEC")
rm(length_seq, length_ranges)

hg.data %>% 
  group_by(DOW, SPEC, zm_sample) %>% 
  count()
#3344 unique lake-spec-invasion combos

#add predictive draws
predictions <- pred_data %>% 
  add_predicted_draws(fit.gp, ndraws = 100) 
  #median_qi()
#the obs in this df = unique lake-spec-invasion combos X lengths X # of predictive draws

#write_csv(predictions, "preds_gp_sum.csv")

#reading in saved predictive draws
predictions <- read_csv("preds_gp_sum.csv") %>% 
  mutate(easting.factor = as.factor(easting),
         northing.factor = as.factor(northing)) %>% 
  mutate(species = case_when(SPEC == "BLC" ~ "Black Crappie",
                             SPEC == "BLG" ~ "Bluegill",
                             SPEC == "LMB" ~ "Largemouth Bass",
                             SPEC == "NOP" ~ "Northern Pike",
                             SPEC == "WAE" ~ "Walleye",
                             SPEC == "WTS" ~ "White Sucker",
                             SPEC == "YEP" ~ "Yellow Perch"))
 
#add in lengths to convert scaled lengths to inches from original data set
length_stats <- hg.data %>% 
  mutate(species = case_when(SPEC == "BLC" ~ "Black Crappie",
                             SPEC == "BLG" ~ "Bluegill",
                             SPEC == "LMB" ~ "Largemouth Bass",
                             SPEC == "NOP" ~ "Northern Pike",
                             SPEC == "WAE" ~ "Walleye",
                             SPEC == "WTS" ~ "White Sucker",
                             SPEC == "YEP" ~ "Yellow Perch")) %>% 
  group_by(species) %>% 
  summarise(mean_length = mean(LGTHIN),
            sd_length = sd(LGTHIN),
            min_length = min(LGTHIN),
            max_length = max(LGTHIN))

#set us prob of crossing threshold 
threshold <- predictions %>% 
  #create column that indicates if the predicted value is over 0.2
  mutate(is_above_threshold = .prediction > log(0.2)) %>% 
  #This step creates a probability across all lakes for each spec-length-draw
  group_by(species, scaled_length, .draw) %>% 
  summarise(prob = mean(is_above_threshold), .groups = 'drop')  %>% 
  #this step sums each spec-length across draws so that we get a variability in the prob like for the state
  group_by(species, scaled_length) %>% 
  summarise(probability_above_threshold = mean(prob),
            lower_ci = quantile(prob, 0.025),
            upper_ci = quantile(prob, 0.975)) %>% 
  #get original lengths back
  left_join(length_stats, by = "species") %>% 
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length) 
# the number of species and the number of lengths should = the obs of this df
  
#this chuck does not condense .draws so that we can plot each draw to viz the variability at each length
draw_sum <- predictions %>% 
  mutate(is_above_threshold = .prediction > log(0.2)) %>% 
  group_by(species, scaled_length, .draw) %>% 
  summarise(prob = mean(is_above_threshold), .groups = 'drop') %>% 
  left_join(length_stats, by = "species") %>% 
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length)
#number of species X the number of lengths X the number of predictive draws makes up this df

#length in inches by prob
ggplot() +
  geom_line(data = draw_sum, aes(x = LGTHIN, y = prob, group = interaction(.draw, species), color = species), alpha = 0.1) +
  geom_line(data = threshold, aes(x = LGTHIN, y = probability_above_threshold, color = species)) +
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm")

#length in inches by prob by smoothed
ggplot() +
  geom_line(data = draw_sum, 
            aes(x = LGTHIN, y = prob, group = interaction(.draw, species), color = species), 
            alpha = 0.1, 
            stat = "smooth", method = "loess", span = 0.3, se = FALSE) +  # Smoothing individual draws
  geom_line(data = threshold, 
            aes(x = LGTHIN, y = probability_above_threshold, color = species), 
            stat = "smooth", method = "loess", span = 0.3, se = FALSE) +  # Smoothing average line
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm")

ggplot() +
  geom_line(data = draw_sum, 
            aes(x = LGTHIN, y = prob, group = interaction(.draw, species), color = species), 
            alpha = 0.05, 
            stat = "smooth", method = "loess", span = 0.3, se = FALSE) +  # Smoothing individual draws
  geom_line(data = threshold, 
            aes(x = LGTHIN, y = probability_above_threshold), 
            stat = "smooth", method = "loess", span = 0.3, se = FALSE) +  # Smoothing average line
  scale_y_continuous(limits = c(-0.01,1.01)) +
  facet_wrap(~species, scales = "free_x") +
  geom_hline(yintercept = 0.5, color = "red", linetype = "dashed") +
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_bw() +
  theme(legend.position = "none",
        strip.background = element_blank(),
        strip.text = element_text(size = 12))
ggsave("prob_exceeding.png", width = 5, height = 5)

#lake specific variability prob of exceeding
lakes <- hg.data %>% 
  #generating xwalk back to original data from predicted data
  mutate(easting.factor = as.factor(easting),
         northing.factor = as.factor(northing),
         parent_dow = substr(DOW, 1, 6)) %>%
  #one lake has multi basins and a few connected lakes get the same UTM, fixing that to get correct join
  mutate(parent_dow = case_when(parent_dow == "110415" ~ "040030",
                         parent_dow == "860230" ~ "860229",
                         TRUE ~ parent_dow)) %>% 
  distinct(easting.factor, northing.factor, parent_dow) %>% 
  right_join(predictions, by = c("easting.factor",
                                 "northing.factor"))
lakes.sum <- lakes %>% 
  #creating a column that shows if the prediction is above .2
  mutate(above_threshold = .prediction > log(.2)) %>% 
  #generating a prob for each lake, spec, length it exceeds the threshold across draws
  group_by(parent_dow, species, scaled_length) %>% 
  summarise(prob = mean(above_threshold), .groups = 'drop') %>% 
  left_join(length_stats, by = "species") %>% 
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length)

#doing the same thing but getting the species level average prob
species.sum <- lakes %>% 
  mutate(above_threshold = .prediction > log(.2)) %>% 
  group_by(species, scaled_length) %>% 
  summarise(prob = mean(above_threshold), .groups = 'drop') %>% 
  left_join(length_stats, by = "species") %>% 
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length)

#initial plot
ggplot() +
  geom_line(data = lakes.sum, aes(LGTHIN, prob, group = parent_dow, color = species), 
              alpha = 0.1,
            stat = "smooth", method = "loess", span = 0.3, se = FALSE) +
  geom_line(data = species.sum, aes(LGTHIN, prob),
            stat = "smooth", method = "loess", span = 0.3, se = FALSE) +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  facet_wrap(~species, scales = "free") +
  xlab("Length of Fish (inches)") +
  ylab("Probability of Exceeding 0.2 ppm")+
  theme_bw() +
  theme(legend.position = "none",
        strip.background = element_blank(),
        strip.text = element_text(size = 12))
ggsave("lake_vary_hg.png", height = 5, width = 7, bg = "white", dpi = 600)

#playing around with density using hex
ggplot() +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  geom_hex(data = lakes.sum, aes(LGTHIN, prob), bins = 10, alpha = 0.7) +
  scale_fill_viridis_c(option = "C") +  # Use viridis palette
  facet_wrap(~species, scales = "free") +
  theme_minimal() +
  labs(x = "Length of Fish (inches)", y = "Probability of Exceeding 0.2 ppm")

###########stopped here 9/26######################


#leech lake specific
leech.data <- hg.data %>% 
  filter(DOW == "11020300") %>% 
  select(WATERWAY, DOW, easting, northing) %>% 
  distinct(WATERWAY, DOW, easting, northing) 
#leech has data for BLC, NOP, WAE, and YEP

leech <- predictions %>% 
  filter(easting >= 392.912 & easting <= 392.913) %>% 
  filter(SPEC == "WAE")
  left_join(length_stats, by = "SPEC")
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length)
glimpse(leech)

leech %>% 
ggplot() +
  geom_point(aes(x = LGTHIN, y = exp(.prediction))) +
  geom_smooth(aes(x = LGTHIN, y = exp(.prediction))) +
  geom_hline(yintercept = 0.2) +
  facet_wrap(~SPEC, scales = "free")

threshold.leech <- leech %>% 
  mutate(is_above_threshold = .prediction > log(0.2)) %>% 
  group_by(SPEC, scaled_length, .draw) %>% 
  summarise(prob = mean(is_above_threshold), .groups = 'drop') %>%   
  group_by(SPEC, scaled_length) %>% 
  summarise(probability_above_threshold = mean(prob),
            lower_ci = quantile(prob, 0.025),
            upper_ci = quantile(prob, 0.975)) %>% 
  left_join(length_stats, by = "SPEC") %>% 
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length) 
  
#with all draws
draw_sum.leech <- leech %>% 
  mutate(is_above_threshold = .prediction > log(0.2)) %>% 
  group_by(SPEC, scaled_length, .draw) %>% 
  summarise(prob = mean(is_above_threshold), .groups = 'drop') %>% 
  left_join(length_stats, by = "SPEC") %>% 
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length)

ggplot() +
  geom_line(data = draw_sum.leech, aes(x = LGTHIN, y = prob, group = interaction(.draw, SPEC)), alpha = 0.1) +
  geom_line(data = threshold.leech, aes(x = LGTHIN, y = probability_above_threshold)) +
  scale_y_continuous(limits = c(0,1)) +
  facet_wrap(~SPEC, scales = "free_x") +
  geom_hline(yintercept = .5, color = "red", linetype = "dashed") +
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_bw()

threshold.leech %>% 
  ggplot(aes(x = LGTHIN, y = probability_above_threshold)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.1, color = NA) +
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_minimal() +
  scale_y_continuous(limits = c(0,1)) +
  geom_hline(yintercept = .5, linetype = "dashed") +
  facet_wrap(~SPEC, scales = "free")
rm(leech, leech.data, threshold.leech, draw_sum.leech)

#standardized length
threshold %>% 
  ggplot(aes(x = scaled_length, y = probability_above_threshold, color = SPEC)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, group = SPEC, fill = SPEC), alpha = 0.1, color = NA) +
  labs(x = "Scaled Length", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_minimal()

#actual length
threshold %>% 
  ggplot(aes(x = LGTHIN, y = probability_above_threshold, color = SPEC)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, group = SPEC, fill = SPEC), alpha = 0.1, color = NA) +
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_minimal()

threshold %>% 
  ggplot(aes(x = LGTHIN, y = probability_above_threshold)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.1, color = NA) +
  labs(x = "Length (Inches)", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_minimal() +
  scale_y_continuous(limits = c(0,1)) +
  geom_hline(yintercept = .5, linetype = "dashed") +
  facet_wrap(~SPEC, scales = "free")

#prob of average size
hg.data %>% 
  filter(SPEC == "WAE") %>% 
  filter(LGTHIN == 15) %>% 
  select(LGTHIN, scaled_length)

pred_data_avg <- hg.data %>% 
  select(easting, 
         northing, 
         SPEC, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area,
         scaled_logit_urban,
         scaled_logit_ag,
         zm_sample) %>% 
  distinct() %>% 
  mutate(scaled_length = 0)

#exploring spatial fit
grid <- hg.data %>% 
  filter(SPEC == "WAE") %>% 
  select(easting, 
         northing, 
         SPEC, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area,
         scaled_logit_urban,
         scaled_logit_ag,
         zm_sample) %>% 
  distinct()

grid_data <- grid %>%
  mutate(
    scaled_length = 0,
    SPEC = factor("WAE"))
rm(grid)

predictions <- grid_data %>% 
  add_predicted_draws(fit.gp, ndraws = 50)
predictions_sum <- predictions %>% 
  median_qi() %>% 
  mutate(easting = easting*1000,
         northing = northing*1000)


# Plot the spatial effect
mn_boundary <- tigris::states(cb = TRUE) %>% 
  filter(STUSPS == "MN") %>%
  st_transform(crs = st_crs("+proj=utm +zone=15 +datum=WGS84"))
predictions_sf <- st_as_sf(predictions_sum, coords = c("easting", "northing"), crs = st_crs(mn_boundary))

ggplot() +
  geom_sf(data = mn_boundary) +
  geom_sf(data = predictions_sf, aes(color = exp(.prediction))) +
  scale_color_viridis_c() +  
  theme_minimal()

rm(predictions, 
   predictions_sf, 
   predictions_sum, 
   mn_boundary,
   grid_data)

#how is scaled length working?
length_stats %>% 
  mutate(LGTHIN = (0*sd_length) + mean_length)

hg.data %>% 
  filter(scaled_length >= -0.01 & scaled_length <= 0.01) %>% 
  group_by(SPEC) %>% 
  summarise(mean.length = mean(LGTHIN),
            n = n())

#making predition data
length_ranges <- hg.data %>% 
  group_by(SPEC) %>% 
  summarize(min_length = min(scaled_length), max_length = max(scaled_length))

length_seq <- length_ranges %>% 
  rowwise() %>% 
  mutate(scaled_length = list(seq(min_length, max_length, length.out = 100))) %>% 
  unnest(cols = c(scaled_length))

pred_data <- hg.data %>% 
  select(easting, 
         northing, 
         SPEC, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area,
         scaled_logit_urban,
         scaled_logit_ag,
         zm_sample) %>% 
  distinct() %>% 
  drop_na() %>%
  inner_join(length_seq, by = "SPEC")
rm(length_seq, length_ranges)

#memory intensive step
predictions <- fit.gp %>% 
  epred_draws(pred_data, ndraws = 100)

length_stats <- hg.data %>% 
  group_by(SPEC) %>% 
  summarise(mean_length = mean(LGTHIN),
            sd_length = sd(LGTHIN),
            min_length = min(LGTHIN),
            max_length = max(LGTHIN))

predictions %>% 
  left_join(length_stats, by = "SPEC") %>% 
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length) %>% 
  ggplot(aes(x = LGTHIN, y = exp(.epred))) +
  stat_lineribbon() +
  geom_hline(yintercept = 0.2, linetype = "dashed") +
  scale_fill_brewer(palette = "Reds") +
  theme_bw() +
  labs(x = "Length (inches)",
       y = "Mercury Concentration (ppm)") +
  theme(legend.position = "none")
ggsave("wae_length_hg.png", height = 7, width = 5, dpi = 300, bg = "white")

#walleye specific line
predictions %>% 
  filter(SPEC == "WAE") %>% 
  left_join(length_stats, by = "SPEC") %>%
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length) %>%
  group_by(LGTHIN, SPEC) %>% 
  summarise(median=median(exp(.epred)), lci=quantile(exp(.epred), .025), uci=quantile(exp(.epred), 0.975)) %>% 
  group_by(SPEC) %>%
  mutate(smoothed_uci = predict(loess(uci ~ LGTHIN, span = 0.5)),
         smoothed_lci = predict(loess(lci ~ LGTHIN, span = 0.5)),
         smoothed_median = predict(loess(median ~ LGTHIN, span = 0.5))) %>%
  ungroup() %>% 
  ggplot() +
  facet_wrap(~SPEC, scales='free') +
  theme_bw() +
  geom_ribbon(aes(x = LGTHIN, y = median, ymax = smoothed_uci, ymin = smoothed_lci), lwd = 0, alpha = .3) + # alpha is transparency
  geom_line(lwd = 1, aes(x = LGTHIN, y = median)) + # center line of ribbon
  geom_hline(yintercept = 0.2, linetype = "dashed") +
  ylab("Mercury Concentration (ppm)") +
  xlab("Fish Length (inches)") +
  theme(strip.background = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.background = element_blank(),
        panel.grid = element_blank(),
        strip.text = element_text(size = 13))
ggsave("wae_length_hg_not_pretty.png", height = 7, width = 5, dpi = 300, bg = "white")

#zebra mussel difference line
predictions %>% 
  left_join(length_stats, by = "SPEC") %>%
  mutate(LGTHIN = (scaled_length*sd_length) + mean_length) %>%
  group_by(LGTHIN, SPEC, zm_sample) %>% 
  summarise(median=median(exp(.epred)), lci=quantile(exp(.epred), .025), uci=quantile(exp(.epred), 0.975)) %>% 
  group_by(SPEC, zm_sample) %>%
  mutate(smoothed_uci = predict(loess(uci ~ LGTHIN, span = 0.5)),
         smoothed_lci = predict(loess(lci ~ LGTHIN, span = 0.5)),
         smoothed_median = predict(loess(median ~ LGTHIN, span = 0.5))) %>%
  ungroup() %>% 
  mutate(SPEC = case_when(SPEC == "BLC" ~ "Black Crappie",
                          SPEC == "BLG" ~ "Bluegill",
                          SPEC == "LMB" ~ "Largemouth Bass",
                          SPEC == "NOP" ~ "Northern Pike",
                          SPEC == "WAE" ~ "Walleye",
                          SPEC == "WTS" ~ "White Sucker",
                          SPEC == "YEP" ~ "Yellow Perch",
                          TRUE ~ "NA")) %>% 
  ggplot() +
  facet_wrap(~SPEC, scales='free') +
  theme_bw() +
  geom_ribbon(aes(x = LGTHIN, y = median, color = zm_sample, ymax = smoothed_uci, ymin = smoothed_lci, fill = zm_sample), lwd = 0, alpha = .3) + # alpha is transparency
  geom_line(lwd = 1, aes(x = LGTHIN, y = median, color = zm_sample, lty = zm_sample)) + # center line of ribbon
  scale_linetype_manual(values = c(1, 6), labels = c("Uninvaded", "Zebra mussel")) + # to tell it which line type you want
  scale_colour_manual(values = c("#00CCCC", "#FF6666"), labels = c("Uninvaded", "Zebra mussel")) +
  scale_fill_manual(values = c("#00CCCC", "#FF6666"), labels = c("Uninvaded", "Zebra mussel")) +
  ylab("Mercury Concentration (ppm)") +
  xlab("Fish Length (inches)") +
  labs(fill = "Invasion status", lty = "Invasion status", color = "Invasion status", labels = c("Uninvaded", "Zebra Mussel")) +
  theme(strip.background = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.background = element_blank(),
        panel.grid = element_blank(),
        strip.text = element_text(size = 11))
ggsave("species_length_zm.png", height = 7, width = 5, dpi = 300, bg = "white")


#conditional effects for zm
pred_data2 <- hg.data %>%
  select(easting, 
         northing, 
         SPEC, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area,
         scaled_logit_urban,
         scaled_logit_ag,
         zm_sample) %>% 
  distinct() %>% 
  drop_na() %>%
  mutate(scaled_length = 0)

invasion_average_fish <- fit.gp %>% 
  epred_draws(pred_data2, ndraws = 100) 

invasion_average_fish %>% 
  group_by(SPEC, zm_sample) %>% 
  summarise(median=median(exp(.epred)), lci=quantile(exp(.epred), .025), uci=quantile(exp(.epred), 0.975)) %>% 
  ungroup() %>%  
  ggplot() +
  geom_pointinterval(aes(y = zm_sample, x = median, xmin = lci, xmax = uci, colour=zm_sample)) +
  xlab("Mercury Concentration (ppm)")+
  ylab("")+
  guides(color = "none") +
  scale_color_manual("", values=c("#00BFC4","#F8766D")) +
  facet_wrap(~SPEC, scales="free") +
  coord_flip() +
  theme_bw() +
  theme(strip.background = element_blank(),
        legend.position = "bottom",
        legend.title = element_blank(),
        legend.background = element_blank(),
        panel.grid = element_blank(),
        strip.text = element_text(size = 11)) 
ggsave("species_hg_epred_zm_conditional.png", height = 7, width = 5, dpi = 600, bg = "white")

plot(conditional_effects(fit.gp))
```

#Testing lscale term without covariates 
```{r}
####################filtering hg data for predictive modeling##################
hg.data <- hg.model %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(HGPPM = case_when(HGPPM <= 0.01 ~ HGPPM/2,
                           TRUE ~ HGPPM)) %>% 
  #species with more than 1,000 observations 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  #post method change and more recent 
  filter(YEARCOLL > 1999) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(WS_Lake_Ratio < 100) %>% 
  #linking data to lat/lon
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  #removes lakes where lat/lon was not grabbed - worth checking in later
  filter(!is.na(lake_lat_decdeg)) %>% 
  filter(!is.na(clarity)) %>% 
  #transforming some variables for modeling
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area)),
         scaled_logit_wetlands = c(scale(logit_wetlands)),
         scaled_logit_urban = c(scale(logit_urban)),
         scaled_logit_ag = c(scale(logit_ag)),
         scaled_log_WS_Lake_Ratio = c(scale(log_WS_Lake_Ratio)),
         scaled_clarity = c(scale(clarity))) %>% 
  #filtering out super long or short fish
  filter(!(scaled_length > 3 | scaled_length < -3)) 
rm(hg, hg.model)

####################fitting model###########################
fit.gp <- brm(log_hg ~ 
                scaled_length*SPEC + 
                SPEC + 
                gp(easting, northing, gr = T, scale = F),
                     data = hg.data,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 12)
#saveRDS(fit.gp, "statewide_gp_lscale_test.rds")
fit.gp <- readRDS("statewide_gp_lscale_test.rds")

#############summary###################################
summary(fit.gp)
plot(fit.gp)
```

#lscale plotting
```{r}
exp_quad_kernel <- function(distance, lscale) {
  exp(-distance^2 / (2 * lscale^2))
}

# Generate data
lscale <- 10
distances <- seq(0, 50, length.out = 500)
correlations <- exp_quad_kernel(distances, lscale)
data <- data.frame(distance = distances, correlation = correlations)

# Create the plot
ggplot(data, aes(x = distance, y = correlation)) +
  geom_line(color = "blue", size = 1) +
  geom_hline(yintercept = 0.5, color = "red", linetype = "dashed") +
  geom_hline(yintercept = 0.001, color = "green", linetype = "dashed") +
  labs(
    title = "Gaussian Process Correlation vs. Distance",
    x = "Distance (km)",
    y = "Correlation"
  ) +
  theme_minimal(base_size = 14) +
  annotate("text", x = 15, y = 0.52, label = "Correlation = 0.5", color = "red", size = 4, hjust = 0) +
  annotate("text", x = 40, y = 0.01, label = "Correlation = 0.001", color = "green", size = 4, hjust = 0)
```


#Leave the recent year out
```{r}
hg.data <- hg.model %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(HGPPM = case_when(HGPPM <= 0.01 ~ HGPPM/2,
                           TRUE ~ HGPPM)) %>% 
  #species with more than 1,000 observations 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  #post method change and more recent 
  filter(YEARCOLL > 1999) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(WS_Lake_Ratio < 100) %>% 
  #linking data to lat/lon
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  #removes lakes where lat/lon was not grabbed - worth checking in later
  filter(!is.na(lake_lat_decdeg)) %>% 
  filter(!is.na(clarity)) %>% 
  #transforming some variables for modeling
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area)),
         scaled_logit_wetlands = c(scale(logit_wetlands)),
         scaled_logit_urban = c(scale(logit_urban)),
         scaled_logit_ag = c(scale(logit_ag)),
         scaled_log_WS_Lake_Ratio = c(scale(log_WS_Lake_Ratio)),
         scaled_clarity = c(scale(clarity))) %>% 
  #filtering out super long or short fish
  filter(!(scaled_length > 3 | scaled_length < -3)) 
rm(hg, hg.model)

#filtering out 2023
hg.data.loo <- hg.data %>% 
  filter(!(YEARCOLL %in% c("2023"))) 

loo <- hg.data %>% 
  filter(YEARCOLL == "2023")

summary_counts <- data.frame(
  data = c("original", "loo"),
  count = c(nrow(hg.data), nrow(hg.data.loo))
)

summary_year <- hg.data %>% 
  group_by(YEARCOLL) %>% 
  summarise(n_original = n()) %>% 
  left_join(hg.data.loo %>% 
              group_by(YEARCOLL) %>% 
              summarise(n_loo = n()))

summary_numeric <- bind_rows(
  hg.data %>% 
    summarise(across(where(is.numeric), list(mean = mean, sd = sd, min = min, max = max), .names = "{.col}_{.fn}")) %>%
    mutate(data = "original"),
  
  hg.data.loo %>% 
    summarise(across(where(is.numeric), list(mean = mean, sd = sd, min = min, max = max), .names = "{.col}_{.fn}")) %>%
    mutate(dataset = "loo")
)

summary_categorical <- bind_rows(
  hg.data %>%
    summarise(across(where(is.character), ~ n_distinct(.), .names = "Unique_{.col}")) %>%
    mutate(data = "original"),
  
  hg.data.loo %>%
    summarise(across(where(is.character), ~ n_distinct(.), .names = "Unique_{.col}")) %>%
    mutate(data = "loo")
)

rm(summary_categorical, summary_counts, summary_numeric, summary_year)

lost_lakes <- hg.data %>% 
  anti_join(hg.data.loo, by = "DOW") %>% 
  distinct(DOW)

lost_species <- hg.data %>% 
  anti_join(hg.data.loo, by = "SPEC") %>% 
  distinct(SPEC)
rm(lost_lakes, lost_species)


####################fitting model###########################
rm(hg.data)

fit.gp <- brm(log_hg ~ 
                scaled_length*SPEC + 
                SPEC + 
                scaled_logit_wetlands + 
                scaled_logit_urban +
                scaled_logit_ag + 
                scaled_log_WS_Lake_Ratio + 
                scaled_log_area + 
                zm_sample +
                gp(easting, northing, gr = T, scale = F),
                     data = hg.data.loo,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 12)
#saveRDS(fit.gp, "statewide_gp_loo_RecentYear.rds")
fit.gp_loo <- readRDS("statewide_gp_loo_RecentYear.rds")

#basic model checks
plot(fit.gp_loo)
summary(fit.gp_loo)
#plot(conditional_effects(fit.gp, ndraws = 100))

########how well do we predict 2023 when it was left out?###################

pred_data <- loo %>% 
  distinct(easting, 
         northing, 
         SPEC, 
         scaled_length,
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area,
         scaled_logit_urban,
         scaled_logit_ag,
         zm_sample) 
#there are some duplicate spec-lake-length, I use the distinct call so that I don't have duplicated data and make more than one predication for the same info

#add predictive draws
predictions <- pred_data %>% 
  add_predicted_draws(fit.gp_loo, ndraws = 100) 

predictions_summary <- predictions %>% 
  median_qi()  %>% 
  distinct(easting, 
         northing, 
         SPEC, 
         scaled_length,
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area,
         scaled_logit_urban,
         scaled_logit_ag,
         zm_sample,
         .keep_all = T)

#joins predicted results to the actual results 
loo_with_prediction <- loo %>% 
  distinct(easting, 
         northing, 
         SPEC, 
         scaled_length,
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area,
         scaled_logit_urban,
         scaled_logit_ag,
         zm_sample,
         log_hg) %>% 
  left_join(predictions_summary)
#we have two points that are duplicates of each other at different leaves (the same length-spec-lake AND the same length-spec-lake-hgppm). I create only one prediction for unique length-spec-lake values, but the same predication gets applied to all hgppm values 


#plots that show how well we predicted our result 
loo_with_prediction %>% 
  ggplot() +
  geom_point(aes(log_hg, .prediction)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  theme_minimal()

error_metrics <- loo_with_prediction %>%
  mutate(
    error = abs(log_hg - .prediction),
    squared_error = (log_hg - .prediction)^2,
    within_interval = ifelse(log_hg >= .lower & log_hg <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>%
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )
error_metrics

error_metrics_backtransformed <- loo_with_prediction %>%
  mutate(
    observed_original = exp(log_hg),
    prediction_original = exp(.prediction),
    lower_original = exp(.lower),
    upper_original = exp(.upper),
    error_original = abs(observed_original - prediction_original),
    squared_error_original = (observed_original - prediction_original)^2,
    within_interval_original = ifelse(observed_original >= lower_original & observed_original <= upper_original, 1, 0),
    interval_width_original = upper_original - lower_original
  ) %>%
  summarise(
    MAE_original = mean(error_original, na.rm = TRUE),
    RMSE_original = sqrt(mean(squared_error_original, na.rm = TRUE)),
    coverage_original = mean(within_interval_original, na.rm = TRUE),
    avg_interval_width_original = mean(interval_width_original, na.rm = TRUE)
  )
error_metrics_backtransformed

#plot
loo_with_prediction %>%
  mutate(
    observed_orig = exp(log_hg),
    predicted_orig = exp(.prediction),
    lower_orig = exp(.lower),
    upper_orig = exp(.upper)
  ) %>% 
  group_by(SPEC) %>% 
  mutate(.row = row_number()) %>% 
ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = lower_orig, ymax = upper_orig), width = 0.2, color = "lightblue") +
  geom_point(aes(y = predicted_orig), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = observed_orig), color = "black", size = 2, shape = 17) +
  facet_wrap(~SPEC, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentration (original scale)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  scale_color_manual(values = c("Prediction" = "blue", "Observed" = "red")) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top") +
  scale_y_log10() 
```
when this model finishes running:
1. read back in hg.data
2. filter to get only the data that is left out
3. save the rds model file
4. make predictions using the loo data
5. compare predictions to the actual observed data

#Leave out some specs of some lakes 
```{r}
hg.data <- hg.model %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(HGPPM = case_when(HGPPM <= 0.01 ~ HGPPM/2,
                           TRUE ~ HGPPM)) %>% 
  #species with more than 1,000 observations 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  #post method change and more recent 
  filter(YEARCOLL > 1999) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(WS_Lake_Ratio < 100) %>% 
  #linking data to lat/lon
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  #removes lakes where lat/lon was not grabbed - worth checking in later
  filter(!is.na(lake_lat_decdeg)) %>% 
  filter(!is.na(clarity)) %>% 
  #transforming some variables for modeling
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area)),
         scaled_logit_wetlands = c(scale(logit_wetlands)),
         scaled_logit_urban = c(scale(logit_urban)),
         scaled_logit_ag = c(scale(logit_ag)),
         scaled_log_WS_Lake_Ratio = c(scale(log_WS_Lake_Ratio)),
         scaled_clarity = c(scale(clarity))) %>% 
  #filtering out super long or short fish
  filter(!(scaled_length > 3 | scaled_length < -3)) 
rm(hg, hg.model)

#filtering out spec-lake combinations
loo_spec_lakes <- hg.data %>% 
  distinct(SPEC, DOW) %>% 
  sample_frac(0.15)

hg.data.loo <- hg.data %>% 
  anti_join(loo_spec_lakes, by = c("SPEC", "DOW"))

#summary of samples that were left out
hg.data %>% 
  semi_join(loo_spec_lakes, by = c("SPEC", "DOW")) %>% 
  summarise(avg_hg = mean(HGPPM),
            wae = sum(SPEC == "WAE"),
            nop = sum(SPEC == "NOP"),
            spec_else = sum(SPEC != "WAE" & SPEC != "NOP"),
            zm_samples = sum(zm_sample == "y"),
            n = n())

hg.data %>% 
  semi_join(loo_spec_lakes, by = c("SPEC", "DOW")) %>% 
  distinct(DOW, .keep_all = T) %>% 
  summarise(avg_size = mean(area),
            avg_wetland = mean(Percent_Wetland),
            avg_urban = mean(Percent_Urban),
            zm_lake = sum(zm_sample == "y"),
            n_lakes = n_distinct(DOW))

#are there any lakes totally removed?

#modeling
fit.gp <- brm(log_hg ~ 
                scaled_length*SPEC + 
                SPEC + 
                scaled_logit_wetlands + 
                scaled_logit_urban +
                scaled_logit_ag + 
                scaled_log_WS_Lake_Ratio + 
                scaled_log_area + 
                zm_sample +
                gp(easting, northing, gr = T, scale = F),
                     data = hg.data.loo,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 12)
#saveRDS(fit.gp, "statewide_hg_loo_spec_lake.rds")
fit.gp <- readRDS("statewide_hg_loo_spec_lake.rds")

#data used in model
loo_lake_spec <- fit.gp$data
#write_csv(loo_lake_spec, "statewide_hg_loo_spec_lake_data.csv")

#summary
summary(fit.gp)

#how well can we predict left out samples?
pred_data <- hg.data %>% 
  semi_join(loo_spec_lakes, by = c("SPEC", "DOW")) %>% 
  distinct(easting, 
         northing, 
         SPEC, 
         scaled_length,
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area,
         scaled_logit_urban,
         scaled_logit_ag,
         zm_sample) 

predictions <- pred_data %>% 
  add_predicted_draws(fit.gp, ndraws = 100) 

predictions_summary <- predictions %>% 
  median_qi()

#joins predicted results to the actual results 
loo_with_prediction <- hg.data %>%
  semi_join(loo_spec_lakes, by = c("SPEC", "DOW")) %>% 
  distinct(easting, 
         northing, 
         SPEC, 
         scaled_length,
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area,
         scaled_logit_urban,
         scaled_logit_ag,
         zm_sample,
         log_hg) %>% 
  left_join(predictions_summary)

#plots that show how well we predicted our result 
loo_with_prediction %>% 
  ggplot() +
  geom_point(aes(log_hg, .prediction)) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  theme_minimal()

error_metrics <- loo_with_prediction %>%
  mutate(
    error = abs(log_hg - .prediction),
    squared_error = (log_hg - .prediction)^2,
    within_interval = ifelse(log_hg >= .lower & log_hg <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>%
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )
error_metrics
rm(error_metrics)

#percentage of median predictions being off by a threshold (.1)
loo_with_prediction %>%   
  mutate(
    observed_orig = exp(log_hg),
    predicted_orig = exp(.prediction),
    diff = predicted_orig - observed_orig,
    abs_diff = abs(diff)
  ) %>%
    group_by(SPEC) %>% 
  summarise(n_within_1 = sum(abs_diff < 0.1),
            n_within_05 = sum(abs_diff < 0.05),
            n_within_01 = sum(abs_diff <0.01),
            n = n()) %>% 
  mutate(per_within_1 = n_within_1 / n *100,
            per_within_05 = n_within_05 / n * 100,
            per_within_01 = n_within_01 / n * 100)

error_metrics_backtransformed <- loo_with_prediction %>%
  mutate(
    observed_original = exp(log_hg),
    prediction_original = exp(.prediction),
    lower_original = exp(.lower),
    upper_original = exp(.upper),
    error_original = abs(observed_original - prediction_original),
    squared_error_original = (observed_original - prediction_original)^2,
    within_interval_original = ifelse(observed_original >= lower_original & observed_original <= upper_original, 1, 0),
    interval_width_original = upper_original - lower_original
  ) %>%
  group_by(SPEC) %>% 
  summarise(
    MAE_original = mean(error_original, na.rm = TRUE),
    RMSE_original = sqrt(mean(squared_error_original, na.rm = TRUE)),
    coverage_original = mean(within_interval_original, na.rm = TRUE),
    avg_interval_width_original = mean(interval_width_original, na.rm = TRUE)
  )
error_metrics_backtransformed
rm(error_metrics_backtransformed)

#plot
loo_with_prediction %>%
  mutate(
    observed_orig = exp(log_hg),
    predicted_orig = exp(.prediction),
    lower_orig = exp(.lower),
    upper_orig = exp(.upper)
  ) %>% 
  group_by(SPEC) %>% 
  arrange(observed_orig) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = lower_orig, ymax = upper_orig), width = 0.2, color = "lightblue") +
  geom_point(aes(y = predicted_orig), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = observed_orig), color = "black", size = 2, shape = 17) +
  facet_wrap(~SPEC, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentration (original scale)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  scale_color_manual(values = c("Prediction" = "blue", "Observed" = "red")) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top") +
  scale_y_log10()
ggsave("ind_pred_spec_lake.png", dpi = 600, width = 11, height = 7, bg = "white")

loo_with_prediction %>%
  mutate(
    observed_orig = exp(log_hg),
    predicted_orig = exp(.prediction),
    lower_orig = exp(.lower),
    upper_orig = exp(.upper)
  ) %>% 
  group_by(SPEC) %>% 
  arrange(observed_orig) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  group_by(SPEC) %>% 
  sample_frac(.1) %>% 
  ungroup() %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = lower_orig, ymax = upper_orig), width = 0.2, color = "lightblue") +
  geom_point(aes(y = predicted_orig), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = observed_orig), color = "black", size = 2, shape = 17) +
  facet_wrap(~SPEC, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentration (original scale)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  scale_color_manual(values = c("Prediction" = "blue", "Observed" = "red")) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top") +
  scale_y_log10()
ggsave("ind_pred_spec_lake_sample.png", dpi = 600, width = 11, height = 7, bg = "white")

loo_with_prediction %>%
  mutate(
    observed_orig = exp(log_hg),
    predicted_orig = exp(.prediction),
    lower_orig = exp(.lower),
    upper_orig = exp(.upper)
  ) %>% 
  group_by(SPEC) %>% 
  arrange(predicted_orig) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = lower_orig, ymax = upper_orig), width = 0.2, color = "lightblue") +
  geom_point(aes(y = predicted_orig), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = observed_orig), color = "black", size = 2, shape = 17, alpha = .5) +
  facet_wrap(~SPEC, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentration (original scale)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  scale_color_manual(values = c("Prediction" = "blue", "Observed" = "red")) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top") +
  scale_y_log10()
ggsave("ind_pred_spec_lake_pred_order.png", dpi = 600, width = 11, height = 7, bg = "white")

#how off are we on average?
loo_with_prediction %>%
  mutate(
    observed_orig = exp(log_hg),
    predicted_orig = exp(.prediction),
    diff = predicted_orig - observed_orig
  ) %>% 
  ggplot() +
  geom_histogram(aes(diff,)) +
  facet_wrap(~SPEC, scales = "free")

#generate predictions for an average size fish (or important for consumption) and compare how often our predicted values would make an advisory vs what the data do now
pred_data <- hg.data %>% 
  semi_join(loo_spec_lakes, by = c("SPEC", "DOW")) %>% 
  distinct(easting, 
         northing, 
         SPEC, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area,
         scaled_logit_urban,
         scaled_logit_ag,
         zm_sample) %>% 
  mutate(scaled_length = 0)

predictions <- pred_data %>% 
  add_predicted_draws(fit.gp, ndraws = 100) 

predictions_summary <- predictions %>% 
  median_qi()

sample_from_data <- hg.data %>%
  semi_join(loo_spec_lakes, by = c("SPEC", "DOW")) %>% 
  group_by(easting, northing, SPEC, zm_sample) %>% 
  summarise(mean_lake_spec_sample = mean(log_hg) >= log(0.22))

sample_from_preditions <- predictions_summary %>% 
  group_by(SPEC, easting, northing, zm_sample) %>% 
  summarise(mean_lake_spec_pred = .prediction >= log(0.22))

combined <- sample_from_data %>% 
  left_join(sample_from_preditions)
rm(sample_from_data, sample_from_preditions)

lake_level_check <- combined %>% 
  group_by(easting, northing, SPEC, zm_sample) %>% 
  summarise(same_designation = case_when(mean_lake_spec_pred == mean_lake_spec_sample ~ T,
                                         TRUE ~ F),
            predicted_not_observed = case_when((mean_lake_spec_pred == TRUE & mean_lake_spec_sample == FALSE) ~ T,
                                               T ~ F),
            observed_not_predicted = case_when((mean_lake_spec_pred == F & mean_lake_spec_sample == T) ~ T,
            T ~ F)
            )  %>% 
  group_by(SPEC) %>% 
  summarise(total_same = sum(same_designation == T),
            total_predicted_not_observed = sum(predicted_not_observed == T),
            total_observed_not_predicted = sum(observed_not_predicted == T),
            n = n()) %>% 
  mutate(per_same = round(total_same/n *100, 3),
         per_predicted_not_observed = round(total_predicted_not_observed/ n *100, 3),
         per_observed_not_predicted = round(total_observed_not_predicted /n *100, 3))
rm(lake_level_check)
```

###########All code below is exploratory##########################
#GLIFWC
```{r}
glifwc <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Mercury Data", "glifwc_hg.csv")) %>% 
  rename(STATE = state,
         WATERWAY = waterbody,
         DATECOL2 = sample_date,
         YEARCOLL = year,
         SPEC = spp_code,
         LGTHIN = length_in,
         HGPPM = hg_ppm,
         WTKG = wt_lbs,
         AGE = age) %>% 
  mutate(original_file = "glifwc") %>% 
  select(STATE,
         WATERWAY,
         county,
         DATECOL2,
         YEARCOLL,
         SPEC,
         LGTHIN,
         WTKG,
         AGE,
         HGPPM,
         original_file) %>% 
  mutate(WATERWAY = str_remove(WATERWAY, " L$"))

mn_data <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Mercury Data", "allfish_data_04042024_JLO.csv")) %>% 
  filter(!(is.na(LGTHIN) | is.na(HGPPM) | is.na(DOWID))) %>%   #removes samples with no hg, length, or dow data
  filter(TYPE %in% c("Lake", "LAKE")) %>% 
  mutate(DOWID = fixlakeid(DOWID)) %>% 
  filter(DOWID != "16000100") %>% 
  mutate(STATE = "MN") %>% 
  mutate(original_file = "mn_state") %>% 
  select(STATE,
         WATERWAY,
         DATECOL2,
         YEARCOLL,
         SPEC,
         LGTHIN,
         WTKG,
         AGE,
         HGPPM,
         original_file,
         OWNER) %>% 
  mutate(DATECOL2 = as.Date(DATECOL2, format = "%m/%d/%Y"),
         YEARCOLL = year(DATECOL2)) 


#row bind the data
comb_hg <- bind_rows(glifwc, mn_data)
rm(glifwc, mn_data)

#do glifwc samples exist in the MN data?
mn_glifwc_samples <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Mercury Data", "allfish_data_04042024_JLO.csv")) %>% 
  filter(!(is.na(LGTHIN) | is.na(HGPPM) | is.na(DOWID))) %>%   #removes samples with no hg, length, or dow data
  filter(TYPE %in% c("Lake", "LAKE")) %>% 
  filter(OWNER == "GLIFWC")
#there are some

mn_glifwc_samples %>% 
  group_by(WATERWAY, SPEC, YEARCOLL) %>% 
  count() %>% 
  print(n = nrow(.))
#samples from glifwc in the mn data are mostly walleye from 

#are there duplicates?
dups <- comb_hg %>% 
  filter(SPEC == "WAE") %>% 
  filter(STATE == "MN") %>% 
  select(WATERWAY, YEARCOLL, LGTHIN, HGPPM) %>% 
  group_by(WATERWAY, YEARCOLL, LGTHIN, HGPPM) %>% 
  filter(n() > 1) %>% 
  ungroup()

dups <- comb_hg %>% 
  semi_join(dups) %>% 
  arrange(WATERWAY, YEARCOLL, LGTHIN, HGPPM)
#looks like most of the duplicates are from mn data containing the glifwc data

dups %>% 
  mutate(HGPPM = as.character(round(HGPPM, 3))) %>%
  group_by(WATERWAY, YEARCOLL, LGTHIN, HGPPM) %>%  
  summarise(n = n_distinct(original_file))  %>% 
  filter(n >1) 
#looks like 304 of the glifwic minnesota samples are duplicates 
#there are also some duplicates within the mn data base - are those really two samples (check sample id)

rm(mn_glifwc_samples, dups)

comb_hg %>% 
  group_by(STATE) %>% 
  count()

comb_hg %>% 
  group_by(original_file, STATE, SPEC) %>% 
  count() %>% 
  arrange(SPEC) %>% 
  print(n = nrow(.))
#glifwc only sent us walleye data

comb_hg %>% 
  filter(original_file == "glifwc") %>% 
  group_by(STATE) %>% 
  summarise(n_lakes = n_distinct(WATERWAY))

comb_hg %>% 
  filter(original_file == "glifwc") %>%
  group_by(STATE, WATERWAY, YEARCOLL) %>% 
  count() %>% 
  group_by(STATE) %>% 
  summarise(average_n = mean(n))

comb_hg %>%
  group_by(WATERWAY, YEARCOLL) %>%
  filter(n_distinct(original_file) > 1) %>%
  ungroup() %>% 
  ggplot() +
  geom_point(aes(LGTHIN, log(HGPPM), color = original_file)) +
  facet_wrap(WATERWAY~YEARCOLL, scales = "free")
#so many combos its hard to make much of anything

comb_hg %>%
  filter(SPEC == "WAE") %>% 
  group_by(WATERWAY, YEARCOLL) %>%
  filter(n_distinct(original_file) > 1) %>%
  ungroup() %>% 
  filter(WATERWAY == "MILLE LACS") %>% 
  filter(YEARCOLL == "1996") %>% 
  ggplot() +
  geom_jitter(aes(LGTHIN, log(HGPPM), color = original_file)) +
  facet_wrap(~YEARCOLL, scales = "free")
#these are duplicates, have to jitter to even see the glifwc samples 
```


#Exploring Predictors
```{r}
hg.model %>% 
  ggplot() +
  geom_histogram(aes(log(HGPPM)))

hg.model %>% 
  filter(SPEC == "WAE") %>% 
  ggplot() +
  geom_histogram(aes(scaled_length))

hg.model %>% 
  filter(SPEC == "NOP") %>% 
  ggplot() +
  geom_histogram(aes(scaled_length))

hg.model %>% 
  ggplot() +
  geom_histogram(aes(LGTHIN))

hg.model %>% 
  filter(SPEC == "WAE") %>% 
  ggplot() +
  geom_point(aes(LGTHIN, log(HGPPM))) +
  geom_smooth(aes(LGTHIN, log(HGPPM)))

hg.model %>% 
  filter(SPEC == "WAE") %>% 
  ggplot() +
  geom_point(aes(scaled_length, log(HGPPM))) +
  geom_smooth(aes(scaled_length, log(HGPPM)))

hg.model %>% 
  filter(SPEC == "WAE") %>% 
  ggplot() +
  geom_point(aes(logit_wetlands, log(HGPPM))) +
  geom_smooth(aes(logit_wetlands, log(HGPPM)))

hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  ggplot() +
  geom_histogram(aes(Percent_Wetland))

hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  ggplot() +
  geom_histogram(aes(scaled_Percent_Wetland))

hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  ggplot() +
  geom_histogram(aes(logit_wetlands))

hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  ggplot() +
  geom_histogram(aes(WS_Lake_Ratio))

hg.model %>% 
  distinct(DOW, .keep_all = T) %>%
  filter(log_WS_Lake_Ratio < 10) %>% 
  ggplot() +
  geom_histogram(aes(WS_Lake_Ratio))

hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  summarise(large.ratio = sum(WS_Lake_Ratio >= 250, na.rm = T),
            other.ratios = sum(WS_Lake_Ratio < 250, na.rm = T))
#16 lakes that seem much too big (over 1500)
#82 over 250

#which lakes would be filtered out?
hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  filter(WS_Lake_Ratio >= 250) %>% 
  distinct(WATERWAY, DOW) %>% 
  print( n= nrow(.))


```

#First attempt of modeling
```{r}
#ensure data clean for modeling
hg.model.filtered <- hg.model %>% 
  filter(TYPE == "Lake") %>% 
  filter(YEARCOLL >1996) %>% 
  filter(!(is.na(DOW) |
             is.na(ANATHG) |
             is.na(YEARCOLL) |
             is.na(LGTHIN) |
             is.na(Total_Wetland_Hectares)|
             is.na(Percent_Wetland) |
             is.na(WS_Lake_Ratio) |
             is.na(centroid_lat) |
             is.na(area) |
             is.na(clarity) |
             is.na(scaled_length))) %>% 
  mutate(year = as.character(YEARCOLL),
         DOW = as.character(DOW)) %>% 
  mutate(log.hg = log(HGPPM))
#summary of the data
summary(hg.model.filtered)
glimpse(hg.model.filtered)
rm(hg.model, hg)

###################walleye model#################################
wae <- hg.model.filtered %>% 
  filter(SPEC == "WAE") 

#simple linear mixed effects model to test before going Bayesian
fit.wae <- lmer(log.hg ~ scaled_length + 
                 logit_wetlands +
                 log_area +
                 log_WS_Lake_Ratio + 
                 clarity + 
                 (1|year) + 
                 (1|DOW),
              data = wae)
summary(fit.wae)


#brms model
fit.wae <- brm(log.hg ~ scaled_length +
               scaled_Percent_Wetland + 
               log_area + 
               log_WS_Lake_Ratio +
               clarity + 
               (1|year) + 
               (1|DOW),
             data = wae,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
#save model run/read in saved model run
saveRDS(fit.wae, "WAE_model.rds")
fit.wae <- readRDS("WAE_model.rds")

#basic summary output
summary(fit.wae)
plot(fit.wae)
plot(conditional_effects(fit.wae))

#random effects for year
fit.wae %>%
  spread_draws(r_year[year,]) %>% 
  median_qi(condition_mean = r_year, .width = c(.95, .66)) %>%
  ggplot(aes(y = year, x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()

fit.wae %>%
  spread_draws(r_DOW[DOW,]) %>% 
  median_qi(condition_mean = r_DOW, .width = c(.95, .66)) %>%
  ggplot(aes(y = as.factor(DOW), x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()


chunks <- fit.wae %>%
  spread_draws(r_DOW[DOW,]) %>% 
  median_qi(condition_mean = r_DOW, .width = c(.95, .66)) %>%
  arrange(desc(condition_mean)) %>% 
  group_by(chunk = ceiling(row_number() / 50))

# Plot each chunk separately
plots <- list()
for (i in unique(chunks$chunk)) {
  plot_data <- chunks %>%
    filter(chunk == i) %>%
    ggplot(aes(y = as.factor(DOW), x = condition_mean, xmin = .lower, xmax = .upper)) +
    geom_pointinterval() +
    labs(title = paste("Chunk", i))
  
  plots[[i]] <- plot_data
}

# Print each plot
for (i in seq_along(plots)) {
  print(plots[[i]])
}

#predictive capacity 
yrep <- posterior_predict(fit.wae)

pp_check(fit.wae, ndraws = 100)
ppc_stat_2d(wae$log.hg, yrep = yrep, stat = c("mean", "sd"))
median <- ppc_stat_data(wae$log.hg, yrep = yrep, group = wae$DOW, stat = "median")
error <- ppc_stat_data(wae$log.hg, yrep = yrep, group = wae$DOW, stat = "sd")
#we have a way of getting data from posterior distributions - now just need a better way to display it
##find a way to subtract each yrep value from y to get a difference between them?

levels_per_page <- 20

# Determine the total number of pages
total_pages <- ceiling(nlevels(grouped$data$group) / levels_per_page)

# Create a list to store the ggplot objects for each page
plot_list <- list()

# Loop through each page
for (i in 1:total_pages) {
  # Determine the levels for this page
  start_level <- (i - 1) * levels_per_page + 1
  end_level <- min(i * levels_per_page, nlevels(grouped$data$group))
  levels_this_page <- levels(grouped$data$group)[start_level:end_level]
  
  # Filter data for this page
  filtered_data <- subset(grouped$data, group %in% levels_this_page)
  
  # Create ggplot for this page
  plot <- ggplot(filtered_data, aes(x = value)) +
    # Add your layers, aesthetics, and geoms here
    grouped$layers[[1]] + grouped$layers[[2]] + grouped$layers[[3]] +
    # Facet by group
    facet_wrap(~ group, scales = "free_y") +
    # Add any additional settings as needed
    grouped$theme
  
  # Store the plot in the list
  plot_list[[i]] <- plot
}

# Print or display the plots (e.g., save to a PDF)
pdf("multiple_pages_plot.pdf")
for (i in 1:total_pages) {
  print(plot_list[[i]])
}
dev.off()

#leave one out cross validation
loo.wae <- loo(fit.wae, save_psis = T)

print(loo.wae)
plot(loo.wae)
#marginal posterior predictive checks

ppc_loo_pit_qq(
  y = wae$log.hg,
  yrep = yrep,
  lw = weights(loo.wae$psis_object)
)

#PIT overlay on unif
ppc_loo_pit_overlay(y = wae$log.hg, yrep = yrep, lw = weights(loo.wae$psis_object))

#50 random predictive poionts
keep_obs <- sample(1:10605, 50, replace = FALSE)
ppc_loo_intervals(y = wae$log.hg, 
                  yrep = yrep, 
                  psis_object = loo.wae$psis_object, 
                  subset = keep_obs,
                  order = "median")

rm(fit.wae, loo.wae, plot_data, plots, i, chunks, yrep, wae, keep_obs)
###############northern pike model#######################################
nop <- hg.model.filtered %>% 
  filter(SPEC == "NOP")

fit.nop <- lmer(log.hg ~ scaled_length + 
                 logit_wetlands +
                 log_area +
                 log_WS_Lake_Ratio + 
                 clarity + 
                 (1|year) + 
                 (1|DOW),
              data = nop)
summary(nop)

#brms model for nop
fit.nop <- brm(log.hg ~ scaled_length +
               scaled_Percent_Wetland + 
               log_area + 
               log_WS_Lake_Ratio +
               clarity + 
               (1|year) + 
               (1|DOW),
             data = nop,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
saveRDS(fit.nop, "NOP_model.rds")
fit.nop <- readRDS("NOP_model.rds")

#basic summary output
summary(fit.nop)
plot(fit.nop)
plot(conditional_effects(fit.nop))


#random effects for year
fit.nop %>%
  spread_draws(r_year[year,]) %>% 
  median_qi(condition_mean = r_year, .width = c(.95, .66)) %>%
  ggplot(aes(y = year, x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()

fit.nop %>%
  spread_draws(r_DOW[DOW,]) %>% 
  median_qi(condition_mean = r_DOW, .width = c(.95, .66)) %>%
  ggplot(aes(y = as.factor(DOW), x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()

chunks <- fit.nop %>%
  spread_draws(r_DOW[DOW,]) %>% 
  median_qi(condition_mean = r_DOW, .width = c(.95, .66)) %>%
  arrange(desc(condition_mean)) %>% 
  group_by(chunk = ceiling(row_number() / 50))

# Plot each chunk separately
plots <- list()
for (i in unique(chunks$chunk)) {
  plot_data <- chunks %>%
    filter(chunk == i) %>%
    ggplot(aes(y = as.factor(DOW), x = condition_mean, xmin = .lower, xmax = .upper)) +
    geom_pointinterval() +
    labs(title = paste("Chunk", i))
  
  plots[[i]] <- plot_data
}

# Print each plot
for (i in seq_along(plots)) {
  print(plots[[i]])
}

#predictive capacity 
pp_check(fit.nop, ndraws = 100)
loo.nop <- loo(fit.nop, save_psis = T)

print(loo.nop)
plot(loo.nop)
#marginal posterior predictive checks
yrep <- posterior_predict(fit.nop)
ppc_loo_pit_qq(
  y = nop$log.hg,
  yrep = yrep,
  lw = weights(loo.nop$psis_object)
)
#PIT overlay on unif
ppc_loo_pit_overlay(y = nop$log.hg, yrep = yrep, lw = weights(loo.nop$psis_object))

ppc_data <- ppc_stat_data(nop$log.hg, yrep = yrep, group = nop$DOW, stat = "median")
#actual data within the bounds of the range of the model for how many lakes?
min_max <- ppc_data %>% 
  filter(variable != "y") %>% 
  group_by(group) %>% 
  summarise(min.median = min(exp(value)),
            max.median = max(exp(value)))

min_max <- ppc_data %>% 
  filter(variable == "y") %>% 
  select(group, value) %>% 
  mutate(value = exp(value)) %>% 
  right_join(min_max)
min_max %>% 
  filter(value >= min.median & value <= max.median)
position <- min_max %>% 
  mutate(position = (value - min.median) / (max.median - min.median),
         range = max.median - min.median)
#the position value is a proportion from 0-1 where 0 means the actual value is closer to the min and 1 closer to the max
position %>% 
  ggplot() +
  geom_histogram(aes(position))
position %>% 
  ggplot() +
  geom_histogram(aes(range)) +
  geom_vline(xintercept = .20)
filtered_dows <- position %>% 
  filter(range > 1) %>% 
  rename(DOW = group)
#how do our lakes that have a large range in predicted median Hg stack up to the rest?
#change the variable in the call below for exploration
nop %>% 
  mutate(large.range = case_when(DOW %in% filtered_dows$DOW ~ "Y",
                                 TRUE ~ "N")) %>% 
  distinct(DOW, .keep_all = T) %>% 
  ggplot() +
  geom_histogram(aes(log_WS_Lake_Ratio, fill = large.range))
#I didn't find a pattern here, but maybe there are just low sample sizes?
nop %>% 
  mutate(large.range = case_when(DOW %in% filtered_dows$DOW ~ "Y",
                                 TRUE ~ "N")) %>% 
  group_by(DOW, large.range) %>% 
  count() %>% 
  ggplot()+
  geom_histogram(aes(n, fill = large.range))
#could it be that there is just large variation in the hg of the lake?
nop %>% 
  mutate(large.range = case_when(DOW %in% filtered_dows$DOW ~ "Y",
                                 TRUE ~ "N")) %>%
  filter(large.range == "Y") %>% 
  ggplot() + 
  geom_point(aes(scaled_length, log.hg, color = DOW)) +
  geom_smooth(aes(scaled_length, log.hg, color = DOW), se = F, method = "lm") + 
  geom_smooth(data = nop, aes(scaled_length, log.hg), method = "lm") +
  theme(legend.position = "none")
#it appears the large range lakes tend to have large hg per length than the collective of lakes

ppc_stat_2d(nop$log.hg, yrep = yrep, stat = c("mean", "sd"))

#50 random predictive points
keep_obs <- sample(1:15054, 50, replace = FALSE)
ppc_loo_intervals(y = nop$log.hg, 
                  yrep = yrep, 
                  psis_object = loo.nop$psis_object, 
                  subset = keep_obs,
                  order = "median")
rm(chunks, fit.nop, loo.nop, nop, plot_data, plots, yrep, i, keep_obs)
###############Bluegill##############
blg <- hg.model.filtered %>% 
  filter(SPEC == "BLG")

#brms model for nop
fit.blg <- brm(log.hg ~ scaled_length +
               logit_wetlands + 
               log_area + 
               log_WS_Lake_Ratio +
               clarity + 
               (1|year) + 
               (1|DOW),
             data = blg,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
saveRDS(fit.blg, "BLG_model.rds")

#basic summary output
summary(fit.blg)
plot(fit.blg)
plot(conditional_effects(fit.blg))

#random effects for year
fit.blg %>%
  spread_draws(b_Intercept, r_year[year,]) %>% 
  median_qi(condition_mean = b_Intercept + r_year, .width = c(.95, .66)) %>%
  ggplot(aes(y = year, x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()

chunks <- fit.blg %>%
  spread_draws(b_Intercept, r_DOW[DOW,]) %>% 
  median_qi(condition_mean = b_Intercept + r_DOW, .width = c(.95, .66)) %>%
  arrange(desc(condition_mean)) %>% 
  group_by(chunk = ceiling(row_number() / 50))

# Plot each chunk separately
plots <- list()
for (i in unique(chunks$chunk)) {
  plot_data <- chunks %>%
    filter(chunk == i) %>%
    ggplot(aes(y = as.factor(DOW), x = condition_mean, xmin = .lower, xmax = .upper)) +
    geom_pointinterval() +
    labs(title = paste("Chunk", i))
  
  plots[[i]] <- plot_data
}

# Print each plot
for (i in seq_along(plots)) {
  print(plots[[i]])
}

#predictive capacity 
pp_check(fit.blg, ndraws = 100)
loo.blg <- loo(fit.blg, save_psis = T)

print(loo.blg)
plot(loo.blg)
#marginal posterior predictive checks
yrep <- posterior_predict(fit.blg)
ppc_loo_pit_qq(
  y = blg$log.hg,
  yrep = yrep,
  lw = weights(loo.blg$psis_object)
)
#PIT overlay on unif
ppc_loo_pit_overlay(y = blg$log.hg, yrep = yrep, lw = weights(loo.blg$psis_object))

#50 random predictive poionts
keep_obs <- sample(1:1660, 50, replace = FALSE)
ppc_loo_intervals(y = blg$log.hg, 
                  yrep = yrep, 
                  psis_object = loo.blg$psis_object, 
                  subset = keep_obs,
                  order = "median")
rm(chunks, fit.blg, loo.blg, blg, plot_data, plots, yrep, i, keep_obs)

################largemouth bass####################################
lmb <- hg.model.filtered %>% 
  filter(SPEC == "LMB")

#brms model for nop
fit.lmb <- brm(log.hg ~ scaled_length +
               logit_wetlands + 
               log_area + 
               log_WS_Lake_Ratio +
               clarity + 
               (1|year) + 
               (1|DOW),
             data = lmb,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
saveRDS(fit.lmb, "LMB_model.rds")

#basic summary output
summary(fit.lmb)
plot(fit.lmb)
plot(conditional_effects(fit.lmb))

#random effects for year
fit.lmb %>%
  spread_draws(b_Intercept, r_year[year,]) %>% 
  median_qi(condition_mean = b_Intercept + r_year, .width = c(.95, .66)) %>%
  ggplot(aes(y = year, x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()

chunks <- fit.lmb %>%
  spread_draws(b_Intercept, r_DOW[DOW,]) %>% 
  median_qi(condition_mean = b_Intercept + r_DOW, .width = c(.95, .66)) %>%
  arrange(desc(condition_mean)) %>% 
  group_by(chunk = ceiling(row_number() / 50))

# Plot each chunk separately
plots <- list()
for (i in unique(chunks$chunk)) {
  plot_data <- chunks %>%
    filter(chunk == i) %>%
    ggplot(aes(y = as.factor(DOW), x = condition_mean, xmin = .lower, xmax = .upper)) +
    geom_pointinterval() +
    labs(title = paste("Chunk", i))
  
  plots[[i]] <- plot_data
}

# Print each plot
for (i in seq_along(plots)) {
  print(plots[[i]])
}

#predictive capacity 
pp_check(fit.lmb, ndraws = 100)
loo.lmb <- loo(fit.lmb, save_psis = T)

print(loo.lmb)
plot(loo.lmb)
#marginal posterior predictive checks
yrep <- posterior_predict(fit.lmb)
ppc_loo_pit_qq(
  y = lmb$log.hg,
  yrep = yrep,
  lw = weights(loo.lmb$psis_object)
)
#PIT overlay on unif
ppc_loo_pit_overlay(y = lmb$log.hg, yrep = yrep, lw = weights(loo.lmb$psis_object))

#50 random predictive poionts
keep_obs <- sample(1:1470, 50, replace = FALSE)
ppc_loo_intervals(y = lmb$log.hg, 
                  yrep = yrep, 
                  psis_object = loo.lmb$psis_object, 
                  subset = keep_obs,
                  order = "median")
rm(chunks, fit.lmb, loo.lmb, lmb, plot_data, plots, yrep, i, keep_obs)

####################yellow perch###########################
yep <- hg.model.filtered %>% 
  filter(SPEC == "YEP")

#brms model for nop
fit.yep <- brm(log.hg ~ scaled_length +
               logit_wetlands + 
               log_area + 
               log_WS_Lake_Ratio +
               clarity + 
               (1|year) + 
               (1|DOW),
             data = yep,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
saveRDS(fit.yep, "YEP_model.rds")

#basic summary output
summary(fit.yep)
plot(fit.yep)
plot(conditional_effects(fit.yep))

#random effects for year
fit.yep %>%
  spread_draws(b_Intercept, r_year[year,]) %>% 
  median_qi(condition_mean = b_Intercept + r_year, .width = c(.95, .66)) %>%
  ggplot(aes(y = year, x = condition_mean,  xmin = .lower, xmax = .upper)) +
  geom_pointinterval()

chunks <- fit.yep %>%
  spread_draws(b_Intercept, r_DOW[DOW,]) %>% 
  median_qi(condition_mean = b_Intercept + r_DOW, .width = c(.95, .66)) %>%
  arrange(desc(condition_mean)) %>% 
  group_by(chunk = ceiling(row_number() / 50))

# Plot each chunk separately
plots <- list()
for (i in unique(chunks$chunk)) {
  plot_data <- chunks %>%
    filter(chunk == i) %>%
    ggplot(aes(y = as.factor(DOW), x = condition_mean, xmin = .lower, xmax = .upper)) +
    geom_pointinterval() +
    labs(title = paste("Chunk", i))
  
  plots[[i]] <- plot_data
}

# Print each plot
for (i in seq_along(plots)) {
  print(plots[[i]])
}

#predictive capacity 
pp_check(fit.yep, ndraws = 100)
loo.yep <- loo(fit.yep, save_psis = T)

print(loo.yep)
plot(loo.yep)
#marginal posterior predictive checks
yrep <- posterior_predict(fit.yep)
ppc_loo_pit_qq(
  y = yep$log.hg,
  yrep = yrep,
  lw = weights(loo.yep$psis_object)
)
#PIT overlay on unif
ppc_loo_pit_overlay(y = yep$log.hg, yrep = yrep, lw = weights(loo.yep$psis_object))

#50 random predictive poionts
keep_obs <- sample(1:1389, 50, replace = FALSE)
ppc_loo_intervals(y = yep$log.hg, 
                  yrep = yrep, 
                  psis_object = loo.yep$psis_object, 
                  subset = keep_obs,
                  order = "median")
rm(chunks, fit.yep, loo.yep, yep, plot_data, plots, yrep, i, keep_obs)
```

#Modeling with Mixed Effects 5/16
```{r}
hg.model %>% 
  distinct(DOW, .keep_all = T) %>% 
  summarise(lakes.1000 = sum(WS_Lake_Ratio > 1000, na.rm = T),
            lakes.500 = sum(WS_Lake_Ratio > 500, na.rm =T),
            lakes.200 = sum(WS_Lake_Ratio > 200, na.rm =T),
            lakes.100 = sum(WS_Lake_Ratio > 100, na.rm =T))

#filtering for specs that have over 1000 observations and most recent years
hg.data <- hg.model %>% 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  filter(YEARCOLL > 1999) %>% 
  filter(WS_Lake_Ratio < 100) %>% #loose 132 lakes and 4788 samples
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area)))
rm(hg, hg.model)

fit.statewide <- brm(log_hg ~ scaled_length*SPEC + SPEC + scaled_logit_wetlands + scaled_log_WS_Lake_Ratio + scaled_log_area + scaled_clarity + (1|DOW),
                     data = hg.data,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 10)

saveRDS(fit.statewide, "statewide.predict.rds")
fit.statewide <- readRDS("statewide.predict.rds")

plot(fit.statewide)
summary(fit.statewide)
plot(conditional_effects(fit.statewide))

#pulling conditional effects from lakes 
lake_link <- read_csv("Data/lake_link.csv") %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid) %>% 
  right_join(hg.data) %>% 
  distinct(DOW, 
           nhdid, 
           lake_lat_decdeg, 
           lake_lon_decdeg)

dow_effect <- fit.statewide %>% 
  spread_draws(r_DOW[DOW,])  %>% 
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  left_join(lake_link) 

dow_effect %>% 
  filter(is.na(lake_lat_decdeg)) %>% 
  summarise(n = n_distinct(DOW))
#7 lakes do not have nhdids - oddly all in st. louis county

#map 
mn <- map_data("state") %>% 
  filter(region == "minnesota")

median_dow <- dow_effect %>% 
  group_by(DOW, lake_lat_decdeg, lake_lon_decdeg) %>% 
  summarise(lake_effects = median(r_DOW)) 

#how do the lake parameters contribute to lake effects?
random_effect_check <- hg.data %>% 
  select(DOW,
         SPEC,
         scaled_logit_wetlands,
         scaled_log_WS_Lake_Ratio,
         scaled_log_area,
         scaled_clarity) %>% 
  distinct(DOW, SPEC, .keep_all = T) %>% 
  right_join(median_dow)

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = median_dow, aes(lake_lon_decdeg, lake_lat_decdeg, color = lake_effects), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()
rm(dow_effect)

#predictions of lake-species hg are made following the parameters listed
# 1. I pull the lake level parameters for each spec-lake 
# 2. I create a scaled length variable so that predictions are made for the mean length of each species
# 3. I add predicted draws from the model that use the lake level values from the lake and mean length of species
# 4. I filter for the lakes where that species was actually analyzed 
get_variables(fit.statewide)

predictions <- random_effect_check %>% 
  mutate(scaled_length = c(0)) %>% 
  add_predicted_draws(fit.statewide)

glimpse(predictions)

#wae
wae.lakes <- hg.data %>% 
  filter(SPEC == "WAE") %>% 
  distinct(DOW)

dow_effect_wae <- predictions %>% 
  filter(DOW %in% wae.lakes$DOW) %>% 
  filter(SPEC == "WAE") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "WAE")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_wae, aes(lake_lon_decdeg, lake_lat_decdeg, color = wae.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()

#nop
nop.lakes <- hg.data %>% 
  filter(SPEC == "NOP") %>% 
  distinct(DOW)

dow_effect_nop <- predictions %>% 
  filter(DOW %in% nop.lakes$DOW) %>% 
  filter(SPEC == "NOP") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "NOP")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_nop, aes(lake_lon_decdeg, lake_lat_decdeg, color = nop.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()

#yep
yep.lakes <- hg.data %>% 
  filter(SPEC == "YEP") %>% 
  distinct(DOW)

dow_effect_yep <- predictions %>% 
  filter(DOW %in% yep.lakes$DOW) %>% 
  filter(SPEC == "YEP") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "YEP")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_yep, aes(lake_lon_decdeg, lake_lat_decdeg, color = yep.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()


#lmb
lmb.lakes <- hg.data %>% 
  filter(SPEC == "LMB") %>% 
  distinct(DOW)

dow_effect_lmb <- predictions %>% 
  filter(DOW %in% lmb.lakes$DOW) %>% 
  filter(SPEC == "LMB") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "LMB")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_lmb, aes(lake_lon_decdeg, lake_lat_decdeg, color = lmb.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()

#blc
blc.lakes <- hg.data %>% 
  filter(SPEC == "BLC") %>% 
  distinct(DOW)

dow_effect_blc <- predictions %>% 
  filter(DOW %in% blc.lakes$DOW) %>% 
  filter(SPEC == "BLC") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "BLC")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_blc, aes(lake_lon_decdeg, lake_lat_decdeg, color = blc.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()

#wts
wts.lakes <- hg.data %>% 
  filter(SPEC == "WTS") %>% 
  distinct(DOW)

dow_effect_wts <- predictions %>% 
  filter(DOW %in% wts.lakes$DOW) %>% 
  filter(SPEC == "WTS") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "WTS")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_wts, aes(lake_lon_decdeg, lake_lat_decdeg, color = wts.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()

#blg
blg.lakes <- hg.data %>% 
  filter(SPEC == "BLG") %>% 
  distinct(DOW)

dow_effect_blg <- predictions %>% 
  filter(DOW %in% blg.lakes$DOW) %>% 
  filter(SPEC == "BLG") %>% 
  mutate(hg = exp(.prediction)) %>% 
  group_by(DOW) %>% 
  median_qi(hg, .width = c(.66,.95)) %>% 
  mutate(state_line = case_when(hg >= 0.2 ~ "y",
                                hg < 0.2 ~ "n")) %>% 
  left_join(lake_link) %>% 
  mutate(SPEC = "BLG")

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = dow_effect_blg, aes(lake_lon_decdeg, lake_lat_decdeg, color = blg.hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.72, .4)) +
  coord_fixed()

#all species together
all_species_preds <- bind_rows(dow_effect_blc, 
                    dow_effect_blg, 
                    dow_effect_lmb, 
                    dow_effect_nop, 
                    dow_effect_wae,
                    dow_effect_wts,
                    dow_effect_yep)

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = all_species_preds, aes(lake_lon_decdeg, lake_lat_decdeg, color = hg, shape = state_line), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position = c(.5, .15),
        legend.box = "horizontal") +
  facet_wrap(~SPEC) +
  coord_fixed()
ggsave("all_spec_map.png", width = 11, height = 7, dpi = 600)

#getting hg correlation from predicted values
corr <- all_species_preds %>% 
  filter(.width == "0.95") %>% 
  select(DOW, hg, nhdid, lake_lat_decdeg, lake_lon_decdeg, SPEC) %>% 
  pivot_wider(id_cols = c("DOW", "nhdid", "lake_lat_decdeg", "lake_lon_decdeg"),
              names_from = "SPEC",
              values_from = "hg")
corr %>% 
  ggplot() +
  geom_point(aes(x = WAE, y = NOP)) +
  geom_smooth(aes(x = WAE, y = NOP), method = "lm")

species_cols <- corr[, c("BLC", "BLG", "LMB", "NOP", "WAE", "WTS", "YEP")]
correlation_matrix <- cor(species_cols, use = "pairwise.complete.obs")
corrplot(correlation_matrix, method = "circle")
#all correlations are near 1 due to the nature of predicting from the model based on species coefficients 

#here is a better look at the difference in hg values within a lake
all_species_preds %>% 
  filter(.width == "0.95") %>% 
  ggplot() +
  geom_point(aes(x= SPEC, y = hg, color = DOW)) +
  geom_violin(aes(x= SPEC, y = hg)) +
  theme(legend.position = "none")

#filtering for a lake that has all species
no_nas <- corr %>% 
  drop_na() 
all_species_preds %>%
  filter(DOW %in% no_nas$DOW) %>% 
  ggplot() +
  geom_col(aes(x=SPEC, y = hg)) +
  facet_wrap(~DOW)

#manual residuals 
#looking at log.hg of observed vs log.hg predicted for testing model fit
res <- hg.data %>% 
  filter(!(DOW %in% c("16063300", 
                      "31122500",
                      "38021100",
                      "38053200",
                      "41011000",
                      "69045900",
                      "69058900",
                      "69069000",
                      "69069300",
                      "69069400",
                      "69075500",
                      "69075700"))) %>% 
  select(DOW,
         SPEC,
         scaled_logit_wetlands,
         scaled_log_WS_Lake_Ratio,
         scaled_log_area,
         scaled_clarity,
         scaled_length,
         log_hg) %>% 
  add_residual_draws(fit.statewide)
res <- res %>% 
  ungroup()
#write_dataset(dataset = res, path = "Output/raw_residuals_arrow")
res <- open_dataset("Output/raw_residuals_arrow")

res.summary <- res %>%
  median_qi()



res.summary <- read_csv("summary_residuals.csv") %>% 
  mutate(fitted.value = log_hg - .residual)

res.summary %>% 
ggplot(aes(sample = .residual)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_point(aes(scaled_length, .residual)) +
  geom_smooth(aes(scaled_length, .residual)) +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_histogram(aes(.residual)) +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_density(aes(.residual, fill = SPEC), alpha = .5)

res.summary %>% 
  group_by(SPEC) %>% 
  summarise(mean = mean(.residual),
            median = median(.residual),
            sd = sd(.residual))

res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, .residual)) +
  geom_point(data = res.summary %>% 
               filter(log_hg < -4.60516), aes(fitted.value, .residual,  color = "red")) +
  geom_smooth(aes(fitted.value, .residual)) +
  geom_vline(xintercept = -1.609438, color = "red") +
  facet_wrap(~SPEC) +
  theme(legend.position = "none")
#Most noticeably for BLC and YEP there is a linear trend at low fitted values... can see it from BLG and WTS too
#I think this is a LOD problem... all fish of varying lengths have the same HG value 
#i made all of the values 0.01 and below the red color 

res.summary %>% 
  ggplot() +
  geom_point(aes(scaled_length, exp(fitted.value))) +
  geom_smooth(aes(scaled_length, exp(fitted.value))) +
  facet_wrap(~SPEC)

#RMSE and mad
res.summary %>% 
  mutate(squared_residuals = .residual^2,
         fitted_back = exp(fitted.value),
         obs_back = exp(log_hg),
         back_residual = obs_back - fitted_back,
         abs_back_residual = abs(back_residual)) %>% 
  group_by(SPEC) %>% 
  summarise(rmse = sqrt(mean(squared_residuals)),
            mad = median(abs_back_residual))


#how do the fitted compare to the observed?
res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm")

res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm") +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_point(aes(log_hg, fitted.value)) +
  geom_smooth(aes(log_hg, fitted.value), method = "lm")

fit.res <- lm(log_hg ~ fitted.value, data = res.summary)
summary(fit.res)
plot(fit.res)

fit.res <- lm(fitted.value ~ log_hg, data = res.summary)
summary(fit.res)

#map of residuals
mn <- map_data("state") %>% 
  filter(region == "minnesota")

res.summary.map <- res.summary %>% 
  group_by(DOW, SPEC) %>% 
  summarise(mean.res = mean(.residual)) %>% 
  left_join(lake_link) 

ggplot() +
  geom_polygon(data = mn, aes(x = long, y = lat, group = group), fill = NA, color = "black") +
  geom_point(data = res.summary.map, aes(lake_lon_decdeg, lake_lat_decdeg, color = mean.res), size = 3) +
  scale_color_gradient(low = "blue", high = "orange") +
  theme(panel.background = element_rect(fill = "white"),
        legend.position.inside  = c(.72, .4)) +
  coord_fixed() +
  facet_wrap(~SPEC)

#fitted values vs. length.. how are we shaping up?
res.summary %>% 
  ggplot() +
  geom_smooth(aes(scaled_length, fitted.value, color = SPEC))

res.summary %>% 
  filter(scaled_length >= -2 & scaled_length <= 2) %>% 
  ggplot() +
  geom_smooth(aes(scaled_length, fitted.value, color = SPEC))

res.summary %>% 
  ggplot() +
  geom_smooth(aes(scaled_length, exp(fitted.value), color = SPEC))

res.summary %>% 
  ggplot() +
  geom_point(aes(scaled_length, fitted.value, color = SPEC)) +
  geom_smooth(aes(scaled_length, fitted.value, color = SPEC))
```

#Modeling with removed lake effect
```{r}
lake_link <- read_csv("Data/lake_link.csv") %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

hg.data <- hg.model %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(HGPPM = case_when(HGPPM <= 0.01 ~ HGPPM/2,
                           TRUE ~ HGPPM)) %>% 
  #species with more than 1,000 observations 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  #post method change and more recent 
  filter(YEARCOLL > 1999) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(WS_Lake_Ratio < 100) %>% 
  #transforming some variables for modeling
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area))) %>% 
  #linking data to lat/lon
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  left_join(lake_link) %>% 
  #removes lakes where lat/lon was not grabbed - worth checking in later
  filter(!is.na(lake_lat_decdeg))
rm(hg, hg.model, lake_link)

#fitting model
fit.norand <- brm(log_hg ~ scaled_length*SPEC + SPEC + scaled_logit_wetlands*SPEC + scaled_log_WS_Lake_Ratio*SPEC + scaled_log_area*SPEC + scaled_clarity*SPEC,
                     data = hg.data,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 10)
#saveRDS(fit.norand, "statewide_norandom.rds")
fit.norand <- readRDS("statewide_norandom.rds")

plot(fit.norand)
summary(fit.norand)
plot(conditional_effects(fit.norand))

#residuals
res <- hg.data %>% 
  select(SPEC,
         scaled_logit_wetlands,
         scaled_log_WS_Lake_Ratio,
         scaled_log_area,
         scaled_clarity,
         scaled_length,
         log_hg) %>%
  drop_na() %>% 
  add_residual_draws(fit.norand)

res.summary <- res %>% 
  median_qi(.residual) %>% 
  mutate(fitted.value = log_hg - .residual)
#write_csv(res.summary, "res_summary_norand.csv")
res.summary <- read_csv("res_summary_norand.csv")

#looking at residuals 
#how do the fitted compare to the observed?
res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm")

res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm") +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_point(aes(log_hg, fitted.value)) +
  geom_smooth(aes(log_hg, fitted.value), method = "lm")

#RMSE and mad
res.summary %>% 
  mutate(squared_residuals = .residual^2,
         fitted_back = exp(fitted.value),
         obs_back = exp(log_hg),
         back_residual = obs_back - fitted_back,
         abs_back_residual = abs(back_residual)) %>% 
  group_by(SPEC) %>% 
  summarise(rmse = sqrt(mean(squared_residuals)),
            mad = median(abs_back_residual))


#fitted values vs length
res.summary %>% 
  ggplot() +
  geom_smooth(aes(scaled_length, fitted.value, color = SPEC))

res.summary %>% 
  filter(scaled_length >= -2 & scaled_length <= 2) %>% 
  ggplot() +
  geom_smooth(aes(scaled_length, fitted.value, color = SPEC))

res.summary %>% 
  ggplot() +
  geom_smooth(aes(scaled_length, exp(fitted.value), color = SPEC))

res.summary %>% 
  ggplot() +
  geom_point(aes(scaled_length, fitted.value, color = SPEC)) +
  geom_smooth(aes(scaled_length, fitted.value, color = SPEC))

#outliers
hg.data %>% 
  filter(SPEC == "WTS" & scaled_length < -4)
#all fish under 7 inches and appear to be at the LOD

hg.data %>% 
  filter(HGPPM <= 0.01) %>% 
  group_by(SPEC) %>% 
  count()

hg.data %>% 
  filter(HGPPM <= 0.01) %>% 
  summarise(n_distinct(DOW))

hg.data %>% 
  filter(HGPPM <= 0.01) %>% 
  summarise(n_distinct(YEARCOLL))
#no heavy patterns in samples at LOD - all SPEC represented - 40 distinct lakes and 13 distinct years
```

#Modeling with gaussian processes for auto correlation - with only 100 lakes
```{r}
lake_link <- read_csv("Data/lake_link.csv") %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

#converting coordiantes to utm
lake_link_sf <- lake_link %>% 
  st_as_sf(coords = c("lake_lon_decdeg", "lake_lat_decdeg"), crs = 4326) %>% 
  st_transform(crs = 32615)
utm_coords <- st_coordinates(lake_link_sf)
lake_link <- lake_link %>%
  mutate(easting = utm_coords[, 1]/1000,
         northing = utm_coords[, 2]/1000)
#utm values in km instead of m
rm(lake_link_sf, utm_coords)

hg.data <- hg.model %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(HGPPM = case_when(HGPPM <= 0.01 ~ HGPPM/2,
                           TRUE ~ HGPPM)) %>% 
  #species with more than 1,000 observations 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  #post method change and more recent 
  filter(YEARCOLL > 1999) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(WS_Lake_Ratio < 100) %>% 
  #transforming some variables for modeling
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area))) %>% 
  #linking data to lat/lon
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  left_join(lake_link) %>% 
  #removes lakes where lat/lon was not grabbed - worth checking in later
  filter(!is.na(lake_lat_decdeg))
rm(hg, hg.model)

#sample model run
unique_lakes <- hg.data %>% 
  select(DOW) %>% 
  distinct()

set.seed(123)
sample_lakes <- unique_lakes %>% 
  sample_n(100)

hg.data.sample <- hg.data %>% 
  filter(DOW %in% sample_lakes$DOW)
rm(unique_lakes, sample_lakes)


#fitting model
fit.gp <- brm(log_hg ~ scaled_length*SPEC + SPEC + scaled_logit_wetlands + scaled_log_WS_Lake_Ratio + scaled_log_area + scaled_clarity + gp(easting, northing),
                     data = hg.data.sample,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 10)
#saveRDS(fit.gp, "statewide_gp_sample.rds")
fit.gp <- readRDS("statewide_gp_sample.rds")

#summaries
plot(fit.gp)
summary(fit.gp)
plot(conditional_effects(fit.gp, ndraws = 500))

#residuals
res <- hg.data.sample %>% 
  select(SPEC,
         scaled_logit_wetlands,
         scaled_log_WS_Lake_Ratio,
         scaled_log_area,
         scaled_clarity,
         scaled_length,
         log_hg,
         easting, 
         northing) %>%
  drop_na() %>% 
  add_residual_draws(fit.gp)

res.summary <- res %>% 
  median_qi(.residual) %>% 
  mutate(fitted.value = log_hg - .residual)
#write_csv(res.summary, "res_summary_gp_sample.csv")
res.summary <- read_csv("res_summary_gp_sample.csv")

#looking at residuals 
#how do the fitted compare to the observed?
res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm")

res.summary %>% 
  ggplot() +
  geom_point(aes(fitted.value, log_hg)) +
  geom_smooth(aes(fitted.value, log_hg), method = "lm") +
  geom_abline(intercept = 0, slope = 1) +
  facet_wrap(~SPEC, scales = "free")

res.summary %>% 
  ggplot() +
  geom_point(aes(log_hg, fitted.value)) +
  geom_smooth(aes(log_hg, fitted.value), method = "lm")

#RMSE and mad
res.summary %>% 
  mutate(squared_residuals = .residual^2,
         fitted_back = exp(fitted.value),
         obs_back = exp(log_hg),
         back_residual = obs_back - fitted_back,
         abs_back_residual = abs(back_residual)) %>% 
  group_by(SPEC) %>% 
  summarise(rmse = sqrt(mean(squared_residuals)),
            mad = median(abs_back_residual))

#prob of over 0.2 for range of lengths of each species in each lake
length_ranges <- hg.data.sample %>% 
  group_by(SPEC) %>% 
  summarize(min_length = min(scaled_length), max_length = max(scaled_length))

length_seq <- length_ranges %>% 
  rowwise() %>% 
  mutate(scaled_length = list(seq(min_length, max_length, length.out = 25))) %>% 
  unnest(cols = c(scaled_length))

pred_data <- hg.data.sample %>% 
  select(easting, 
         northing, 
         SPEC, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area, 
         scaled_clarity) %>% 
  distinct() %>% 
  inner_join(length_seq, by = "SPEC")

#prob of average size
pred_data <- hg.data.sample %>% 
  select(easting, 
         northing, 
         SPEC, 
         scaled_logit_wetlands, 
         scaled_log_WS_Lake_Ratio, 
         scaled_log_area, 
         scaled_clarity) %>% 
  distinct() %>% 
  mutate(scaled_length = 0)

#predictions <- add_predicted_draws(fit.gp, newdata = pred_data)
#write_csv(predictions, "preds_gp_sample.csv")
predictions <- read_csv("preds_gp_sample.csv")

threshold <- predictions %>% 
  mutate(is_above_threshold = .prediction > log(0.2)) %>% 
  group_by(SPEC, scaled_length, .draw) %>% 
  summarise(prob = mean(is_above_threshold), .groups = 'drop') %>% 
  group_by(SPEC, scaled_length) %>% 
  summarise(probability_above_threshold = mean(prob),
            lower_ci = quantile(prob, 0.025),
            upper_ci = quantile(prob, 0.975))

threshold %>% 
  ggplot(aes(x = scaled_length, y = probability_above_threshold, color = SPEC)) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, group = SPEC, fill = SPEC), alpha = 0.1, color = NA) +
  labs(x = "Scaled Length", 
       y = "Probability of Exceeding 0.2 ppm") +
  theme_minimal()
```

#Multivarite approach 
```{r}
wae.nop <- hg.model.filtered %>% 
  filter(SPEC %in% c("WAE", "NOP")) %>% 
  filter(WS_Lake_Ratio < 250)

formula <- bf(log.hg ~ scaled_length + SPEC + scaled_log_WS_Lake_Ratio + scaled_logit_wetlands + scaled_log_area + scaled_clarity + (1|DOW:SPEC) + (1|DOW) + (1|year),
              sigma ~ SPEC)

fit.wae.nop <- brm(formula,
             data = wae.nop,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
saveRDS(fit.wae.nop, "wae_nop_fit.rds")

summary(fit.wae.nop)
plot(conditional_effects(fit.wae.nop))
round(vcov(fit.wae),5)

random_effects <- ranef(fit.wae)
dow.spec <- as.data.frame(random_effects$`DOW:SPEC`)
```

#HMSC
```{r}
library(Hmsc)

#filtered data
hg.model.filtered <- hg.model %>% 
  filter(TYPE == "Lake") %>% 
  filter(YEARCOLL >1996) %>% 
  filter(!(is.na(DOW) |
             is.na(ANATHG) |
             is.na(YEARCOLL) |
             is.na(LGTHIN) |
             is.na(Total_Wetland_Hectares)|
             is.na(Percent_Wetland) |
             is.na(WS_Lake_Ratio) |
             is.na(centroid_lat) |
             is.na(area) |
             is.na(clarity) |
             is.na(scaled_length))) %>% 
  mutate(year = factor(YEARCOLL),
         DOW = factor(DOW)) %>% 
  mutate(log.hg = log(HGPPM)) 
#summary of the data
summary(hg.model.filtered)
rm(hg.model, hg)


#three species
three_spec <- hg.model.filtered %>% 
  filter(SPEC %in% c("WAE", "NOP", "BLC")) %>% 
  select(DOW,
         year,
         SPEC,
         log.hg,
         scaled_length,
         scaled_Percent_Wetland,
         log_WS_Lake_Ratio,
         log_area,
         scaled_clarity) 

test <- three_spec %>% 
  select(SPEC,
         log.hg,
         DOW,
         year) %>% 
  pivot_wider(id_cols = c(DOW, year),
    names_from = SPEC,
              values_from = log.hg)

#creating a length based correction for fish at the lake level
wae <- hg.model.filtered %>% 
  filter(SPEC == "WAE")
length.correction <- brm(log.hg ~ scaled_length + (1|DOW),
                         data = wae,
                         family = gaussian())
summary(length.correction)
conditional_effects(length.correction)

#trying with brms
all_species <- hg.model.filtered %>% 
  filter(SPEC %in% c("NOP", "YEP", "WAE", "BLG", "LMB"))

fit.all <- brm(log.hg ~ scaled_length +
               scaled_Percent_Wetland + 
               scaled_log_area + 
               scaled_log_WS_Lake_Ratio +
               (1|year) + 
               (1|DOW/SPEC),
             data = all_species,
             family = gaussian(),
             iter = 10000,
             warmup = 1000,
            chains = 3,
            cores = 10)
saveRDS(fit.all, "all_species_model.rds")
summary(fit.all)
conditional_effects(fit.all)

```

#map of all lakes
```{r}
zm <- read_csv("Data/ZM_MN_NHD.csv") %>% 
  rename(DOWID = DOW) %>% 
  mutate(DOWID = fixlakeid(DOWID)) %>% 
  mutate(DOW = DOWID)

hg_zm <- hg.model %>% 
  left_join(zm) %>% 
  mutate(zm_lake = case_when(!is.na(Year_Infested) ~ "y",
                             TRUE ~ "n"),
         pre_post = case_when(YEARCOLL >= Year_Infested ~ "post",
                              YEARCOLL < Year_Infested ~ "pre",
                              TRUE ~ "never"))

map <- hg_zm %>% 
  left_join(lake_link) %>% 
  distinct(DOW, .keep_all = T) %>% 
  filter(!is.na(easting)) %>% 
  mutate(easting = easting*1000,
         northing = northing*1000)

mn_boundary <- tigris::states(cb = TRUE) %>% 
  filter(STUSPS == "MN") %>%
  st_transform(crs = st_crs("+proj=utm +zone=15 +datum=WGS84"))
map_sf <- st_as_sf(map, coords = c("easting", "northing"), crs = st_crs(mn_boundary))


ggplot() +
  geom_sf(data = mn_boundary, fill = "white") +
  geom_sf(data = map_sf, aes(color = zm_lake, shape = zm_lake)) +
  theme_minimal() +
  scale_color_manual("Zebra Mussel Lake", values = c("lightblue", "salmon"), labels = c("No", "Yes")) +
  scale_shape_manual("Zebra Mussel Lake", values = c(16, 17), labels = c("No", "Yes")) +
  theme(legend.position = c(.8, .32),
        legend.background = element_rect(fill = "white", color = "black", size = 0.5),
    legend.box.background = element_rect(color = "black"))
ggsave("hg_map_all_lakes.png", width = 5, height = 5, bg = "white")
rm(hg_zm, hg.model, lake_link, map, map_sf, mn_boundary, zm)
```


#Old code
```{r}
#finding distances between lakes
library(stats)
lakes <- hg.data %>% 
  distinct(DOW, easting, northing)

dist_matrix <- dist(lakes[, c("easting", "northing")])
dist_tibble <- as.matrix(dist_matrix) %>% 
  as_tibble(rownames = "DOW") %>% 
  pivot_longer(-DOW, names_to = "DOW_2", values_to = "distance") %>% 
  filter(DOW  != DOW_2) %>% 
  mutate(unique_pair = pmap_chr(list(DOW, DOW_2), ~ paste(sort(c(..1, ..2)), collapse = "-"))) %>% 
  distinct(unique_pair, .keep_all = T)

dist_tibble %>% 
  ggplot() +
  geom_histogram(aes(distance), fill = "gray", color = "black", binwidth = 10) +
  theme_minimal()

dist_tibble %>% 
  ggplot() +
  geom_density(aes(distance), fill = "gray", color = "black", binwidth = 10) +
  theme_minimal()

dist_tibble %>% 
  summarise(min_dist = min(distance),
            mean_dist = mean(distance),
            median_dist = median(distance),
            max_dist = max(distance),
            per_under_2 = (sum(distance <= 2)/n()) * 100)

################linking to lagos data############
#load all lagos data
lagos <- lagosne_load()
#extract data relavent to Hg
#epi_nutr contains limno parameters of interest - espically carbon
#hu12 conn gives watershed level paramters for wetlends
#huc12 lulc gives parameters for watershed level landcover
#state gives state level parameters
#filter for just MN lakes 
lg <- left_join(lagos$epi_nutr, lagos$locus)
lg <- left_join(lg, lagos$hu12.conn)
lg <- left_join(lg, lagos$hu12.lulc)
lg <- left_join(lg, lagos$state)
lg.sum <- lg %>% 
  filter(state == "MN") %>% 
  select(state,
         state_name,
         sampledate,
         nhdid,
         hu12_zoneid,
         gnis_name,
         nhd_lat,
         nhd_long,
         lake_area_ha,
         lake_perim_meters,
         doc,
         doc_qual,
         doc_censorcode,
         doc_detectionlimit,
         doc_labmethodname,
         toc,
         toc_qual,
         toc_censorcode,
         toc_detectionlimit,
         toc_labmethodname,
         secchi,
         secchi_qual,
         secchi_censorcode,
         secchi_methodinfo,
         hu12_nlcd2011_pct_90,
         hu12_nlcd2011_ha_90,
         hu12_nlcd2011_pct_95,
         hu12_nlcd2011_ha_95) %>% 
  group_by(nhdid) %>% 
  summarise(meanDOC = mean(doc, na.rm = T),
            meanTOC = mean(toc, na.rm = T),
            meanSECCHI = mean(secchi, na.rm = T),
            mean95 = mean(hu12_nlcd2011_pct_95, na.rm = T),
            mean90 = mean(hu12_nlcd2011_pct_90, na.rm = T),
            meanAREA = mean(lake_area_ha, na.rm = T))

#joining to Hg data
hg.lg <- left_join(hg, lg.sum)

#secchi data
library(dataRetrieval)
library(tidyverse)

secchi.names <- c(
  "Depth, Secchi disk depth",
  "Depth, Secchi disk depth (choice list)",
  "Secchi Reading Condition (choice list)",
  "Water transparency, Secchi disc"
)
args <- list(
  statecode = "MN",
  characteristicName = secchi.names
)
secchi <- readWQPdata(args)
glimpse(secchi)

secchi.clean <- secchi %>%
  filter(!(ResultMeasure.MeasureUnitCode %in% c("None", NA))) %>% 
  mutate(ResultMeasureValue = as.numeric(ResultMeasureValue)) %>% #some strings in the result column
  filter(!is.na(ResultMeasureValue)) %>% 
  mutate(ResultMeasureValue_clean = case_when(ResultMeasure.MeasureUnitCode == "cm" ~ ResultMeasureValue*.01,
                                              ResultMeasure.MeasureUnitCode == "ft" ~ ResultMeasureValue*0.3048,
                                              ResultMeasure.MeasureUnitCode == "in" ~ ResultMeasureValue*0.0254,
                                              TRUE ~ ResultMeasureValue),
         ResultMeasure.MeasureUnitCode_clean = "m") %>% 
  select(OrganizationIdentifier,
         ActivityIdentifier,
         ActivityTypeCode,
         ActivityMediaName,
         ActivityMediaSubdivisionName,
         ActivityStartDate,
         ProjectIdentifier,
         MonitoringLocationIdentifier,
         CharacteristicName,
         ResultMeasureValue,
         ResultMeasureValue_clean,
         ResultMeasure.MeasureUnitCode,
         ResultMeasure.MeasureUnitCode_clean,
         ProviderName)
glimpse(secchi.clean)

secchi.clean %>% 
  summarise(min.year = min(year(ActivityStartDate)),
            max.year = max(year(ActivityStartDate)))

secchi.clean %>% 
  group_by(ResultMeasure.MeasureUnitCode, ResultDepthHeightMeasure.MeasureUnitCode_clean) %>% 
  count()

secchi.clean %>% 
  filter(ResultMeasure.MeasureUnitCode != "m") %>% 
  group_by(ResultMeasureValue, ResultMeasureValue_clean) %>% 
  count() %>% 
  print(n = nrow(.))

secchi.clean %>% 
  group_by(OrganizationIdentifier) %>% 
  count() %>% 
  print(n = nrow(.))

secchi.clean %>% 
  group_by(MonitoringLocationIdentifier) %>% 
  count() %>% 
  print(n = nrow(.))

#need a crosswalk to get to get lake info connected to the site id

hg %>% 
  filter(YEARCOLL == "2023") %>% 
  group_by(WATERWAY) %>% 
  count() %>% 
  print(n = nrow(.))

#old code for calculating specs specific without predictions
dow_effect_species <- fit.statewide %>% 
  spread_draws(b_Intercept, 
               b_SPECWAE, 
               b_SPECBLG,
               b_SPECLMB,
               b_SPECNOP,
               b_SPECWTS,
               b_SPECYEP,
               b_scaled_length, 
               `b_scaled_length:SPECWAE`, 
               `b_scaled_length:SPECNOP`,
               `b_scaled_length:SPECBLG`,
               `b_scaled_length:SPECLMB`,
               `b_scaled_length:SPECWTS`,
               `b_scaled_length:SPECYEP`,
               b_scaled_logit_wetlands, 
               b_scaled_log_WS_Lake_Ratio,
               b_scaled_log_area, 
               b_scaled_clarity, 
               r_DOW[DOW,]) %>% 
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>%
  #since all variables are scaled, when the are 0 (excluded) they are at their means
  #thus, length and lake level parameters are excluded from the calculations below
  mutate(nop.median = b_Intercept + b_SPECNOP + b_SPECNOP +  r_DOW,
         wae.median = b_Intercept + b_SPECWAE + r_DOW,
         lmb.median = b_Intercept + b_SPECLMB + r_DOW,
         blg.median = b_Intercept + b_SPECBLG + r_DOW,
         wts.median = b_Intercept + b_SPECWTS + r_DOW,
         yep.median = b_Intercept + b_SPECWAE + r_DOW,
         blc.median = b_Intercept + r_DOW,
         nop.hg = exp(nop.median),
         wae.hg = exp(wae.median),
         lmb.hg = exp(lmb.median),
         blg.hg = exp(blg.median),
         wts.hg = exp(wts.median),
         yep.hg = exp(yep.median),
         blc.hg = exp(blc.median)) %>% 
  group_by(DOW) %>% 
  median_qi(nop.hg, 
            wae.hg,
            lmb.hg,
            blg.hg,
            wts.hg,
            yep.hg,
            blc.hg,
            .width = c(.9, .66)) %>% 
  left_join(lake_link) %>% 
  mutate(state_threshold.nop = case_when(nop.hg >= 0.2 ~ "y", 
                                         nop.hg < 0.2 ~ "n"),
         state_threshold.wae = case_when(wae.hg >= 0.2 ~ "y",
                                         wae.hg < 0.2 ~ "n"),
         state_threshold.lmb = case_when(lmb.hg >= 0.2 ~ "y",
                                         lmb.hg < 0.2 ~ "n"),
         state_threshold.blg = case_when(blg.hg >= 0.2 ~ "y",
                                         blg.hg < 0.2 ~ "n"),
         state_threshold.blc = case_when(blc.hg >= 0.2 ~ "y",
                                         blc.hg < 0.2 ~ "n"),
         state_threshold.wts = case_when(wts.hg >= 0.2 ~ "y",
                                         wts.hg < 0.2 ~ "n"),
         state_threshold.yep = case_when(yep.hg >= 0.2 ~ "y",
                                         yep.hg < 0.2 ~ "n"))
```

#Exploring prior for lscale
```{r}
library(stats)  # For numerical solvers like `uniroot`

# Inverse Gamma CDF i takes the gamma distribution and gets the inverse
inv_gamma_cdf <- function(x, a, b) {
  pgamma(1 / x, shape = a, rate = 1 / b, lower.tail = TRUE)
}

#x is the value used to comput the cumulative distruiton funciton 
#a is the shae of the inverse gamma
#b is hte scale parameter of the inverse gamma

# Function to compute deltas (residuals for quantile constraints)
# this is making sure that the lower l is close to .01 and the upper is close to 0.99 such that the probability is above 0.01
tail_delta <- function(params, theta) {
  a <- exp(params[1])  # Shape parameter
  b <- exp(params[2])  # Scale parameter
  
  l <- theta[1]  # Lower bound
  u <- theta[2]  # Upper bound
  
  delta1 <- inv_gamma_cdf(l, a, b) - 0.01  # 1% below lower bound
  delta2 <- 1 - inv_gamma_cdf(u, a, b) - 0.01  # 1% above upper bound
  
  return(c(delta1, delta2))
}

# Initialize parameters
# the lower bound should be above 20 and the upper bound is 600 for the length scale 
# This is a decision that the length scale should fall between these values - based on the distribution of distances from the data
l <- 20
u <- 600
theta <- c(l, u)

# Heuristic initial guesses for log(a) and log(b)
delta <- 1 # represent the desired difference between quarantines - 1 generates initial guesses for the parameters a and b of the inverse gamma distribution based on the specified bounds of l and u
a_guess <- log((delta * (u + l) / (u - l))^2 + 2)
b_guess <- log(((u + l) / 2) * ((delta * (u + l) / (u - l))^2 + 1))

# Use numerical solver to find log(a) and log(b)
# numeric solver is used to solve for the parameters a and b that satisfy the tail quantile constrains
# it attempts to find values of a and b such that the CDF at l is 0.01 and at u is 0.99 - implying that the inverse gamma prior for the p will concentrate between l and u
solver <- nleqslv::nleqslv(
  x = c(a_guess, b_guess),  # Initial guesses
  fn = function(params) tail_delta(params, theta)
)

# Extract results
a <- exp(solver$x[1])  # Shape parameter
b <- exp(solver$x[2])  # Scale parameter

cat("Solved parameters:\n")
cat("a (shape) =", a, "\n")
cat("b (scale) =", b, "\n")

# Validate: Plot the density
dinv_gamma <- function(x, a, b) {
  ifelse(x > 0, (b^a / gamma(a)) * x^(-a - 1) * exp(-b / x), 0)
}

# Plot the prior density
rho <- seq(0, 1000, length.out = 1000)  # Adjust range as needed
prior_density <- dinv_gamma(rho, a, b)

plot(rho, prior_density, type = "l", col = "blue", lwd = 2,
     xlab = "Length Scale (rho)", ylab = "Prior Density",
     main = "Inverse Gamma Prior for Length Scale")
abline(v = c(l, u), col = "red", lty = 2)  # Add vertical lines for bounds

#looking at default priors
hg.data <- hg.model %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(HGPPM = case_when(HGPPM <= 0.01 ~ HGPPM/2,
                           TRUE ~ HGPPM)) %>% 
  #species with more than 1,000 observations 
  filter(SPEC %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  #post method change and more recent 
  filter(YEARCOLL > 1999) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(WS_Lake_Ratio < 100) %>% 
  #linking data to lat/lon
  mutate(DOW = as.factor(DOW),
         DOW = str_pad(DOW, width = 8, pad = "0")) %>% 
  #removes lakes where lat/lon was not grabbed - worth checking in later
  filter(!is.na(lake_lat_decdeg)) %>% 
  filter(!is.na(clarity)) %>% 
  #transforming some variables for modeling
  group_by(SPEC) %>% 
  mutate(scaled_length = c(scale(LGTHIN))) %>% 
  ungroup() %>%
  mutate(log_hg = log(HGPPM),
         scaled_log_area = c(scale(log_area)),
         scaled_logit_wetlands = c(scale(logit_wetlands)),
         scaled_logit_urban = c(scale(logit_urban)),
         scaled_logit_ag = c(scale(logit_ag)),
         scaled_log_WS_Lake_Ratio = c(scale(log_WS_Lake_Ratio)),
         scaled_clarity = c(scale(clarity))) %>% 
  #filtering out super long or short fish
  filter(!(scaled_length > 3 | scaled_length < -3)) 
rm(hg, hg.model)


default_prior(log_hg ~ 
                scaled_length*SPEC + 
                SPEC + 
                scaled_logit_wetlands + 
                scaled_logit_urban +
                scaled_logit_ag + 
                scaled_log_WS_Lake_Ratio + 
                scaled_log_area + 
                zm_sample +
                gp(easting, northing, scale = F),
                     data = hg.data,
                     iter = 10000,
                     warmup = 500,
                     chains = 3,
                     family = gaussian(),
                     cores = 12)

inv_gamma_density <- function(x, shape, scale) {
  ifelse(x > 0, (scale^shape) * exp(-scale / x) / (gamma(shape) * x^(shape + 1)), 0)
}

# Define the parameters for the default prior
shape <- 1.494197
scale <- 37.243237

# Create a sequence of values for lscale
x_values <- seq(0.01, 600, length.out = 1000) # Avoid 0 since lscale > 0

# Compute the density
y_values <- inv_gamma_density(x_values, shape, scale)

# Create a data frame for plotting
prior_data <- data.frame(lscale = x_values, density = y_values)

# Plot the prior
ggplot(prior_data, aes(x = lscale, y = density)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title = "Default Prior for lscale",
    x = "lscale (length scale)",
    y = "Density"
  ) +
  geom_vline(xintercept = c(20, 600), color = "red")+
  theme_minimal()

#attempting to add prior to model
#lscale_prior <- prior(inv_gamma(3, 664), class = "lscale", coef = "gpeastingnorthing")
test <- brm(log_hg ~ 
                scaled_length*SPEC + 
                SPEC + 
                scaled_logit_wetlands + 
                scaled_logit_urban +
                scaled_logit_ag + 
                scaled_log_WS_Lake_Ratio + 
                scaled_log_area + 
                zm_sample +
                gp(easting, northing, scale = F),
            data = hg.data,
            prior = prior(inv_gamma(3, 664), class = "lscale", coef = "gpeastingnorthing"), 
                     iter = 100,
                     warmup = 5,
                     chains = 2,
                     family = gaussian(),
                     cores = 12)
prior_summary(test)
summary(test)
```
