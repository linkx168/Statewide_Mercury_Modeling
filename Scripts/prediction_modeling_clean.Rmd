---
title: "prediction_modeling_clean"
output: html_document
date: "2025-10-07"
editor_options: 
  chunk_output_type: console
---

# library
```{r}
library(tidyverse)
library(arrow)
library(LAGOSNE)
library(ggrepel)
library(mwlaxeref)
library(mnsentinellakes)
library(corrplot)
library(performance)
library(brms)
library(lme4)
library(Matrix)
library(car)
library(tidybayes)
library(rstanarm)
library(bayesplot)
library(loo)
library(arrow)
library(sf)
library(maps)
library(viridis)
library(tigris)
library(emmeans)
```

# data
Read in mercury and covariate data from the Google drive. The entire contaminate database, along with DNR lakeshed, glm lake metadata, infested waters list, and lake link to get predictors for modeling. We are left with merucry data and covarites. Things to note:

Mercury data: 
-filtered so all values have a length, mercury value, and DOW
-filtered to only include lakes
-lake superior is filtered out
-used the mwlaxref package to get from dow to nhdhrid

Lakeshed data:
-pulled from MN DNR 
-transform to the logit scale
  -I give it %'s but it recognizes and changes to props
  -for items that are 0 it returns 0.025 and for items that are 100 it returns 0.975

GLM data:
-just grabbing the clarity value and the area
-covert infinite clarity values to NA

Infested waters list:
-pulled from year 2024 
-create some variables based on designation year by the DNR

Lake Link:
-getting coordinates for these lakes 
-I also convert these to easting and northing (for Gaussian process modeling)

Final product:
-mercury data connected to covariate data with selected columns for modeling
-further filtering and scaling is needed prior to modeling 
```{r}
#mercury data
hg <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Mercury Data", "allfish_data_04042024_JLO.csv")) %>% 
  #remove any fish without length, mercury, or lake data
  filter(!(is.na(LGTHIN) | is.na(HGPPM) | is.na(DOWID))) %>%
  #only take data from lakes
  filter(TYPE %in% c("Lake", "LAKE")) %>% 
  #fixes dows using sentienalllakes package
  mutate(DOWID = fixlakeid(DOWID)) %>% 
  #removes lake supiror 
  filter(DOWID != "16000100") %>% 
  #gets nhdhr id for each dow
  local_to_nhdhr(from_colname = "DOWID", states = "mn") %>% 
  mutate(nhdhr.id = str_remove(nhdhr.id, "nhdhr_")) %>% 
  #selects columns for analysis 
  select(SAMPLENO,
         WATERWAY,
         TYPE,
         LOCATION,
         DOWID,
         nhdhr.id,
         DATECOL2,
         YEARCOLL,
         SPEC,
         ANATHG,
         NOFISH,
         LGTHIN,
         WTLB,
         AGE,
         SEX,
         HGPPM,
         HGCODE,
         HGLAB,
         HGCMMT) %>% 
  rename(DOW = DOWID,
         nhdid = nhdhr.id) %>% 
  ungroup()

# Minnesota lakeshed data
dnr_covary <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "MN_lakes_watershed_lulc_fromDNR.csv")) %>% 
  mutate(logit_wetlands = car::logit(Percent_Wetland),
         logit_urban = car::logit(Percent_Urban),
         logit_ag = car::logit(Percent_Cultivated_Crops),
         log_WS_Lake_Ratio = log(WS_Lake_Ratio))

#glm for clarity and area
glm <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "glm_lake_metadata.csv")) %>%
  filter(state == "MN") %>% 
  rename(nhdid = site_id) %>% 
  mutate(nhdid = gsub("^nhdhr_", "", nhdid)) %>% 
  mutate(clarity = case_when(is.infinite(clarity) ~ NA,
                             TRUE ~ clarity)) %>% 
  mutate(log_area = log(area))

#infested waters list
zm <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "ZM_MN_NHD.csv")) %>% 
  filter(!is.na(DOW))  %>% 
  mutate(DOW = as.character(gsub("-", "", DOW))) %>% 
  select(DOW,
         Year_Infested) %>% 
  mutate(zm_lag_year_infested = Year_Infested +2)

#lake link
lake_link <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "lake_link.csv")) %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

#converting coordinates to sf to get easting and northing
lake_link_sf <- lake_link %>% 
  st_as_sf(coords = c("lake_lon_decdeg", "lake_lat_decdeg"), crs = 4326) %>% 
  st_transform(crs = 32615)
utm_coords <- st_coordinates(lake_link_sf)
lake_link <- lake_link %>%
  mutate(easting = utm_coords[, 1]/1000,
         northing = utm_coords[, 2]/1000)
#utm values in km instead of m
rm(lake_link_sf, utm_coords)

######################add datasets together#####################################
hg <- hg %>% 
  left_join(dnr_covary)
rm(dnr_covary)


hg <- hg %>% 
  left_join(glm) 
rm(glm)

hg <- hg %>% 
  left_join(zm, by = c("DOW"))
rm(zm)

hg <- hg %>% 
  left_join(lake_link)
rm(lake_link)

##################selecting columns#############################################
hg <- hg %>%
  select(WATERWAY,
         TYPE,
         DOW,
         nhdid,
         DATECOL2,
         YEARCOLL,
         SPEC,
         ANATHG,
         NOFISH,
         LGTHIN,
         HGPPM,
         Total_Wetland_Hectares,
         Percent_Wetland,
         Percent_Urban,
         Percent_Cultivated_Crops,
         logit_wetlands,
         logit_urban,
         logit_ag,
         WS_Lake_Ratio,
         log_WS_Lake_Ratio,
         centroid_lat,
         centroid_lon,
         max_depth,
         area,
         log_area,
         elevation,
         clarity,
         Year_Infested,
         zm_lag_year_infested,
         lake_lat_decdeg,
         lake_lon_decdeg,
         easting,
         northing) %>% 
  mutate(zm_lake = case_when(is.na(Year_Infested) ~ "n",
                             TRUE ~ "y"),
         zm_sample = case_when(zm_lag_year_infested <= YEARCOLL ~ "y",
                               TRUE ~ "n"),
         time_since_invasion = YEARCOLL - zm_lag_year_infested)

hg <- hg %>% 
  janitor::clean_names()
```

# filtering data for predictive modeling
Biggest items:
-LOD handling (take half of the reported value)
-filter for species of interest 
-filter for years 2000-most recent
-filter any really big watersheds out (driving some skewed covariates), these are mostly reservoirs
```{r}
hg.model <- hg %>% 
  #dealing with LOD - taking half of the LOD - this method should hold up, but take closer look if LOD value is consistent
  mutate(hgppm = case_when(hgppm <= 0.01 ~ hgppm/2,
                           TRUE ~ hgppm)) %>% 
  #species with more than 1,000 observations 
  filter(spec %in% c("WAE", "NOP", "BLC", "YEP", "BLG", "LMB", "WTS")) %>% 
  #post method change and more recent 
  filter(yearcoll > 1999) %>% 
  #getting rid of some wonky ratios - worth coming back to
  filter(ws_lake_ratio < 100) %>% 
  #linking data to lat/lon
  mutate(dow = as.factor(dow),
         dow = str_pad(dow, width = 8, pad = "0")) %>% 
  #filter out any NA area (this will include some border lakes that don't have xwalk)
  filter(!is.na(area))
rm(hg)
```

# leave out data
Leaving out 15% of unique lake-species combos. This will entire lake leave outs. It will also include some leave outs where one species is removed and at least one remains
```{r}
set.seed(10)

#filtering out spec-lake combinations
loo_spec_lakes <- hg.model %>% 
  distinct(spec, dow) %>% 
  sample_frac(0.15)

hg.data.loo <- hg.model %>% 
  anti_join(loo_spec_lakes, by = c("spec", "dow"))

#summary of samples that were left out
hg.model %>% 
  semi_join(loo_spec_lakes, by = c("spec", "dow")) %>% 
  summarise(avg_hg = mean(hgppm),
            wae = sum(spec == "WAE"),
            nop = sum(spec == "NOP"),
            spec_else = sum(spec != "WAE" & spec != "NOP"),
            zm_samples = sum(zm_sample == "y"),
            n = n())

hg.model %>% 
  semi_join(loo_spec_lakes, by = c("spec", "dow")) %>% 
  distinct(dow, .keep_all = T) %>% 
  summarise(avg_size = mean(area, na.rm = T),
            avg_wetland = mean(percent_wetland),
            avg_urban = mean(percent_urban),
            zm_lake = sum(zm_sample == "y"),
            n_lakes = n_distinct(dow))

#are there any lakes totally removed?
#if multipule species were removed making removed completely, the lake will show up n times
hg.model %>% 
  distinct(dow, spec) %>% 
  anti_join(hg.data.loo, by = "dow") %>% 
  arrange(dow) %>% 
  print(n = nrow(.))

#how many times was a species removed but at least one still remained?
hg.model %>%
  distinct(dow, spec) %>%
  anti_join(hg.data.loo, by = c("spec", "dow")) %>% 
  semi_join(hg.data.loo, by = "dow") 

#write_csv(hg.data.loo, "hg_model_data_loo.csv")
rm(loo_spec_lakes)
```

# saved left out data
Here is a short cut for reading in the leave out data from the drive. We also generate a dataframe of the data that has been left out of the modeling
```{r}
# write_csv(hg.data.loo, file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Data", "hg_model_data_loo.csv"))

hg.data.loo <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Data", "hg_model_data_loo.csv"))

hg.data.loo_anti <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Data", "hg_model_data_loo.csv")) %>% 
  select(spec, yearcoll, dow, hgppm, lgthin)

left_out_data <- hg.model %>% 
  anti_join(hg.data.loo_anti)

rm(hg.data.loo_anti)
```

#scale data prior to model
-scale and make sure scalers are saved for prediction 
-scale length by species
-scale lake-level covarites
   (make sure that we scale for each unique lake, not at the fish level)
-filtering out any fish that are +/- 3 SD 
```{r}
#scale lake level data (have to get to one lake per row and then apply to fish data)
lake_level <- hg.data.loo %>% 
  distinct(dow, log_area, logit_wetlands, logit_urban, 
           logit_ag, log_ws_lake_ratio)

lake_scalers <- lake_level %>% 
  summarise(
    mean_log_area = mean(log_area),
    sd_log_area   = sd(log_area),

    mean_logit_wetlands = mean(logit_wetlands),
    sd_logit_wetlands   = sd(logit_wetlands),

    mean_logit_urban = mean(logit_urban),
    sd_logit_urban   = sd(logit_urban),

    mean_logit_ag = mean(logit_ag),
    sd_logit_ag   = sd(logit_ag),

    mean_lw_ratio = mean(log_ws_lake_ratio),
    sd_lw_ratio   = sd(log_ws_lake_ratio),
  )

#length scalers
length_scalers <- hg.data.loo %>% 
  group_by(spec) %>% 
  summarise(
    length_mean = mean(lgthin),
    length_sd   = sd(lgthin)
  )

######################apply scaling#################################
#lake level
hg.data.loo <- hg.data.loo %>%
    mutate(scaled_log_area = (log_area - lake_scalers$mean_log_area) /lake_scalers$sd_log_area,
           
           scaled_logit_wetlands = (logit_wetlands - lake_scalers$mean_logit_wetlands) / lake_scalers$sd_logit_wetlands,
           
           scaled_logit_urban =
      (logit_urban - lake_scalers$mean_logit_urban) / lake_scalers$sd_logit_urban,
      
      scaled_logit_ag =
      (logit_ag - lake_scalers$mean_logit_ag) / lake_scalers$sd_logit_ag,
      
      scaled_log_ws_lake_ratio =
      (log_ws_lake_ratio - lake_scalers$mean_lw_ratio) / lake_scalers$sd_lw_ratio) 

#length scaler
hg.data.loo <- hg.data.loo %>% 
  left_join(length_scalers, by = "spec") %>% 
  mutate(scaled_length = (lgthin - length_mean)/ length_sd) %>% 
  select(-length_mean,
         -length_sd)
rm(lake_level)

#now prep left out data to match the scaling used in prediction 
left_out_scaled <- left_out_data %>%
  left_join(length_scalers, by = "spec") %>% 
  mutate(
    # fish-level scaling (if grouped)
    scaled_length = (lgthin - length_mean) / length_sd,

    # lake-level predictors
    scaled_log_area =
      (log_area - lake_scalers$mean_log_area) / lake_scalers$sd_log_area,

    scaled_logit_wetlands =
      (logit_wetlands - lake_scalers$mean_logit_wetlands) / lake_scalers$sd_logit_wetlands,

    scaled_logit_urban =
      (logit_urban - lake_scalers$mean_logit_urban) / lake_scalers$sd_logit_urban,

    scaled_logit_ag =
      (logit_ag - lake_scalers$mean_logit_ag) / lake_scalers$sd_logit_ag,

    scaled_log_ws_lake_ratio =
      (log_ws_lake_ratio - lake_scalers$mean_lw_ratio) / lake_scalers$sd_lw_ratio
  ) %>% 
  mutate(log_hg = log(hgppm))
rm(left_out_data)

hg.data.loo <- hg.data.loo %>% 
  mutate(log_hg = log(hgppm))
```


# random effects model
```{r}
 # fit <- brm(log_hg ~
 #                 scaled_length*spec +
 #                 spec +
 #                 scaled_logit_wetlands +
 #                 scaled_logit_urban +
 #                 scaled_logit_ag +
 #                 scaled_log_ws_lake_ratio +
 #                 scaled_log_area +
 #                 zm_sample +
 #                 (1|dow),
 #                      data = hg.data.loo,
 #                      iter = 10000,
 #                      warmup = 500,
 #                     chains = 3,
 #                      family = gaussian())

#saveRDS(fit, "mn_random_leave_out.rds")

fit <- readRDS(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output", "mn_random_leave_out.rds"))

summary(fit)
```

# predictions
Now lets make predictions from the left out data and see how well we do. 500 draws should be sufficient for both point estimates and getting feel for uncertainty. So we end up with 500 different predictions for each point that was left out (one from each of the 500 draws). We will then have to summaries across draws from each point. We can gather information about point level uncertainty. We then can look at uncertainty to different levels 
```{r}
predictions <- left_out_scaled %>% 
  add_predicted_draws(fit, ndraws = 500, allow_new_levels = T)

#write_csv(predictions, "predictions_left_out.csv")

predictions <- read_csv(file.path("G:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Predictive Modeling", "Random Model Final Output",  "predictions_left_out.csv"))
```

# summary from predictions 
1. We do this on the log scale
2. We do this on the original scale (backtransformed)
```{r}
#summary of the draws for each prediction (on log scale)
predictions_summary <- predictions %>% 
  group_by(dow, hgppm, lgthin, spec, .row) %>%
  summarise(.pred_sd = sd(.prediction),
            .lower = quantile(.prediction, probs = c(0.025)),
            .upper = quantile(.prediction, probs = c(0.975)),
            .prediction = median(.prediction))

#connect these predictions back to the original data
predictions_summary <- left_out_scaled %>% 
  #this is needed due to maybe duplicate spec-length-hgppm-year-lake
  distinct(dow, spec, hgppm, lgthin, spec, .keep_all = T) %>% 
  left_join(predictions_summary, by = c("dow", "hgppm", "lgthin", "spec"))

##############################################################################

#summary of predictions in the original scale (back transformed)
predictions_summary_original <- predictions %>% 
  mutate(.prediction = exp(.prediction)) %>% 
  group_by(dow, hgppm, lgthin, spec, .row) %>%
  summarise(.pred_sd = sd(.prediction),
            .lower = quantile(.prediction, probs = c(0.025)),
            .upper = quantile(.prediction, probs = c(0.975)),
            .prediction = median(.prediction))

predictions_summary_original <- left_out_scaled %>% 
  #this is needed due to there being some duplicate spec-length-hgppm-year-lake
  distinct(dow, spec, hgppm, lgthin, spec, .keep_all = T) %>% 
  left_join(predictions_summary_original, by = c("dow", "hgppm", "lgthin", "spec"))
```

# summary of data that was left out
Here we get a summary of:
1. how many total lakes were left out
2. partial lakes with no walleye remaining
3. partial lakes with walleye or nop remaining
```{r}
train_lakes <- unique(hg.data.loo$dow)
left_out_scaled <- left_out_scaled %>% 
  mutate(lake_status = case_when(dow %in% train_lakes ~ "partial",
                                 TRUE ~ "new"))

train_species_by_lake <- hg.data.loo %>% 
  group_by(dow) %>% 
  summarise(species_present = list(unique(spec)), .groups = "drop")

left_out_scaled <- left_out_scaled %>% 
  left_join(train_species_by_lake, by = "dow") %>% 
  mutate(species_present = replace_na(species_present, list(character())))

left_out_scaled <- left_out_scaled %>% 
  mutate(species_case = case_when(lake_status == "new" ~ "new_lake",
                                  map_lgl(species_present, ~ !("WAE" %in% .x | "NOP" %in% .x)) ~ "partial_no_wae_nop",
                                  TRUE ~ "partial_wae_or_nop"))


left_out_scaled %>%
  distinct(dow, spec, .keep_all = T) %>% 
  group_by(species_case) %>% 
  count()

rm(train_lakes, train_species_by_lake)
```

# complete lake leave out - predictive metric summary
```{r}
#making grouping variable for lakes with complete leave out
loo_lakes <- hg.model %>% 
  distinct(dow) %>% 
  anti_join(hg.data.loo, by = "dow")

# assign a y or n to lakes based on if it was completely left out or not
predictions_summary <- predictions_summary %>% 
  mutate(lake_loo = case_when(dow %in% loo_lakes$dow ~ "y",
                              TRUE ~ "n"))
rm(loo_lakes)

#lets generate some predictive metrics from this
error_metrics <- predictions_summary %>%
  mutate(
    error = abs(log_hg - .prediction),
    squared_error = (log_hg - .prediction)^2,
    within_interval = ifelse(log_hg >= .lower & log_hg <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>%
  group_by(lake_loo) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE),
    avg_pred_sd = mean(.pred_sd)
  )
error_metrics

############################################################################
loo_lakes <- hg.model %>% 
  distinct(dow) %>% 
  anti_join(hg.data.loo, by = "dow")

predictions_summary_original <- predictions_summary_original %>% 
  mutate(lake_loo = case_when(dow %in% loo_lakes$dow ~ "y",
                              TRUE ~ "n"))
rm(loo_lakes)

error_metrics_backtransformed <- predictions_summary_original %>%
  mutate(
    error_original = abs(hgppm - .prediction),
    squared_error_original = (hgppm - .prediction)^2,
    within_interval_original = ifelse(hgppm >= .lower & hgppm <= .upper, 1, 0),
    interval_width_original = .upper - .lower,
  ) %>% 
  group_by(lake_loo) %>% 
  summarise(
    MAE_original = mean(error_original, na.rm = TRUE),
    RMSE_original = sqrt(mean(squared_error_original, na.rm = TRUE)),
    coverage_original = mean(within_interval_original, na.rm = TRUE),
    avg_interval_width_original = mean(interval_width_original, na.rm = TRUE),
    avg_pred_sd = mean(.pred_sd, na.rm = T)
  )
error_metrics_backtransformed

#confusion matrix
predictions_summary_original %>% 
  #filter(lake_loo == "y") %>% 
  mutate(obs_harmful = case_when(hgppm >= 0.22 ~ "y",
                                 TRUE ~ "n"),
         pred_harmful = case_when(.prediction >= .22 ~ "y",
                                  TRUE ~ "n")) %>% 
  group_by(obs_harmful,
           pred_harmful) %>% 
  count()
```

# compelte lake leave out - predictive plots
```{r}
# how well did we predict leave out lakes relative to non-leave out lakes? - log scale
predictions_summary %>% 
  ggplot() +
  geom_point(aes(hgppm, exp(.prediction), color = lake_loo)) +
  geom_abline(slope = 1, intercept = 0, color = "royalblue", linetype = "dashed") +
  scale_color_manual(values = c("navyblue", "orange")) +
  theme_minimal() +
  theme(legend.position = "bottom")
#ggsave("whole_lake_scatter.jpg")

################################################################################
# how well did we predict on the original scale?
#y-axis log transformed for display
predictions_summary_original %>%
  group_by(spec) %>% 
  arrange(hgppm) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = log(.lower), ymax = log(.upper)), width = 0.2, color = "lightblue") +
  geom_point(aes(y = log(.prediction)), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = log(.prediction)), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = log(0.22), linetype = "dashed") +
  facet_wrap(~spec, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")
#ggsave("loo_prediction_original_logged_for_display.png", dpi = 600, width = 11, height = 7, bg = "white")

predictions_summary_original %>%
  group_by(spec) %>% 
  arrange(hgppm) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = .prediction), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = .prediction), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = 0.22, linetype = "dashed") +
  facet_wrap(~spec, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")
#ggsave("loo_prediction_original.png", dpi = 600, width = 11, height = 7, bg = "white")

################################################################################
predictions_summary_original %>%
  group_by(spec) %>% 
  arrange(.prediction) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = log(.lower), ymax = log(.upper)), width = 0.2, color = "lightblue") +
  geom_point(aes(y = log(.prediction)), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = log(hgppm)), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = log(0.22), linetype = "dashed") +
  facet_grid(lake_loo~spec, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")

```

# species leave out - metrics
```{r}
#making grouping variable for lakes with complete leave out
loo_lakes <- left_out_scaled %>% 
  distinct(dow, lake_status, species_present, species_case) 

# assign a y or n to lakes based on if it was completely left out or not
predictions_summary_spec <- predictions_summary %>% 
  left_join(loo_lakes) %>% 
  filter(lake_status != "new")
rm(loo_lakes)

#lets generate some predictive metrics from this
error_metrics <- predictions_summary_spec %>%
  mutate(
    error = abs(log_hg - .prediction),
    squared_error = (log_hg - .prediction)^2,
    within_interval = ifelse(log_hg >= .lower & log_hg <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>%
  group_by(species_case) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE),
    avg_pred_sd = mean(.pred_sd)
  )
error_metrics

############################################################################
loo_lakes <- left_out_scaled %>% 
  distinct(dow, lake_status, species_present, species_case) 

# assign a y or n to lakes based on if it was completely left out or not
predictions_summary_spec_org <- predictions_summary_original %>% 
  left_join(loo_lakes) %>% 
  filter(lake_status != "new")
rm(loo_lakes)

error_metrics_backtransformed <- predictions_summary_spec_org %>%
  mutate(
    error_original = abs(hgppm - .prediction),
    squared_error_original = (hgppm - .prediction)^2,
    within_interval_original = ifelse(hgppm >= .lower & hgppm <= .upper, 1, 0),
    interval_width_original = .upper - .lower,
  ) %>% 
  group_by(species_case) %>% 
  summarise(
    MAE_original = mean(error_original, na.rm = TRUE),
    RMSE_original = sqrt(mean(squared_error_original, na.rm = TRUE)),
    coverage_original = mean(within_interval_original, na.rm = TRUE),
    avg_interval_width_original = mean(interval_width_original, na.rm = TRUE),
    avg_pred_sd = mean(.pred_sd, na.rm = T)
  )
error_metrics_backtransformed

predictions_summary_spec_org %>% 
  filter(species_case == "partial_wae_or_nop") %>% 
  mutate(obs_harmful = case_when(hgppm >= 0.22 ~ "y",
                                 TRUE ~ "n"),
         pred_harmful = case_when(.prediction >= .22 ~ "y",
                                  TRUE ~ "n")) %>% 
  group_by(obs_harmful,
           pred_harmful) %>% 
  count()
```

# species leave out - plots
```{r}
# Plot observed vs predicted
predictions_summary_spec %>% 
  filter(lake_status != "new") %>% 
ggplot(aes(x = hgppm, y = exp(.prediction))) +
  geom_point(alpha = 0.7, size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(
    x = "Observed log(Hg)",
    y = "Predicted log(Hg)",
  ) +
  facet_wrap(~species_case) +
  theme_minimal()
#ggsave("loo_spec_scatter.jpg", height = 5, width = 5, dpi = 600)
```

# predicitons at the lake-species level (not fish level)
Lets compare how well we predict the observed mean within a species-lake:
1. calculate the mean values within a species-lake (we will use the same length of fish for predictions so we won't have to worry about that)
2. from the predicted values, calculate the mean of value of each spec-lake-year. This would mean averaging across species, lake, .draw. (grouping by draw gets us each observation within lake-species-year and thus a mean mercury value for the grouping) - we will get 500 draws of each mean
3. compare the observed mean to the predicted mean
```{r}
# we group by .draw here so that we get an average of all fish in a lake-spec within a draw
#we essentially get 500 estimates of each lake-spec average value across all lengths
lake_species_preds <- predictions %>% 
  group_by(dow, spec, .draw) %>% 
  summarise(mean_pred = mean(.prediction))  %>% 
  ungroup() %>% 
  group_by(dow, spec) %>% 
  median_qi(mean_pred) %>% 
  select(-.width,
         -.point,
         -.interval)

lake_species_obs <- left_out_scaled %>% 
  group_by(dow, spec) %>% 
  summarise(mean_obs = mean(log_hg))

lake_species_compare <- left_join(lake_species_preds, lake_species_obs, by = c("dow", "spec"))
rm(lake_species_preds, lake_species_obs)


lake_species_metrics <- lake_species_compare %>%
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )
lake_species_metrics

#what about by various groupings?

loo_lakes <- left_out_scaled %>% 
  distinct(dow, spec, lake_status, species_present, species_case)

lake_species_compare <- lake_species_compare %>% 
  left_join(loo_lakes)

lake_species_compare %>%
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>%
  group_by(lake_status) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )

lake_species_compare %>%
  filter(lake_status != "new") %>% 
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>% 
  group_by(species_case) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )

lake_species_compare %>% 
ggplot(aes(x = mean_obs, y = mean_pred)) +
  geom_point(alpha = 0.7, size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(
    x = "Observed log(Hg)",
    y = "Predicted log(Hg)",
  ) +
  theme_minimal()

lake_species_compare %>%
  group_by(spec) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = log(0.22), linetype = "dashed") +
  facet_wrap(~spec, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")

lake_species_compare %>%
  group_by(spec) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = log(0.22), linetype = "dashed") +
  facet_grid(spec~lake_status, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")

#lets filter for where the partial miss to see how viz how badly 
lake_species_compare %>%
  filter(mean_obs <= .lower | mean_obs >= .upper) %>% 
  group_by(spec) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = log(0.22), linetype = "dashed") +
  facet_grid(spec~lake_status, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")

###############################################################################
#lets do this on the back transformed scale as well
lake_species_preds <- predictions %>% 
  group_by(dow, spec, .draw) %>% 
  summarise(mean_pred = mean(exp(.prediction)))  %>% 
  ungroup() %>% 
  group_by(dow, spec) %>% 
  median_qi(mean_pred) %>% 
  select(-.width,
         -.point,
         -.interval)

lake_species_obs <- left_out_scaled %>% 
  group_by(dow, spec) %>% 
  summarise(mean_obs = mean(hgppm))

lake_species_compare <- left_join(lake_species_preds, lake_species_obs, by = c("dow", "spec"))
rm(lake_species_preds, lake_species_obs)


lake_species_metrics <- lake_species_compare %>%
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )
lake_species_metrics

#what about by various groupings?
loo_lakes <- left_out_scaled %>% 
  distinct(dow, spec, lake_status, species_present, species_case)

lake_species_compare <- lake_species_compare %>% 
  left_join(loo_lakes)

#confusion matrix - overall
lake_species_compare %>% 
  mutate(obs_harmful = case_when(mean_obs >= 0.22 ~ "y",
                                 TRUE ~ "n"),
         pred_harmful = case_when(mean_pred >= .22 ~ "y",
                                  TRUE ~ "n")) %>% 
  group_by(obs_harmful,
           pred_harmful) %>% 
  count()

lake_species_compare %>% 
  filter(lake_status == "new") %>% 
  mutate(obs_harmful = case_when(mean_obs >= 0.22 ~ "y",
                                 TRUE ~ "n"),
         pred_harmful = case_when(mean_pred >= .22 ~ "y",
                                  TRUE ~ "n")) %>% 
  group_by(obs_harmful,
           pred_harmful) %>% 
  count()

lake_species_compare %>% 
  filter(species_case == "partial_no_wae_nop") %>% 
  mutate(obs_harmful = case_when(mean_obs >= 0.22 ~ "y",
                                 TRUE ~ "n"),
         pred_harmful = case_when(mean_pred >= .22 ~ "y",
                                  TRUE ~ "n")) %>% 
  group_by(obs_harmful,
           pred_harmful) %>% 
  count()


metrics <- lake_species_compare %>%
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>%
  group_by(lake_status) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE)
  )
metrics

metrics <- lake_species_compare %>%
  filter(lake_status != "new") %>% 
  mutate(
    error = abs(mean_obs - mean_pred),
    squared_error = (mean_obs - mean_pred)^2,
    within_interval = ifelse(mean_obs >= .lower & mean_obs <= .upper, 1, 0),
    interval_width = .upper - .lower
  ) %>% 
  group_by(species_case) %>% 
  summarise(
    MAE = mean(error, na.rm = TRUE),
    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),
    coverage = mean(within_interval, na.rm = TRUE),
    avg_interval_width = mean(interval_width, na.rm = TRUE),
    med_interval_width = median(interval_width)
  )
metrics

lake_species_compare %>% 
ggplot(aes(x = mean_obs, y = mean_pred, color = lake_status)) +
  geom_point(alpha = 0.7, size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_manual(values = c("orange", "navyblue")) +
  labs(
    x = "Observed Hg",
    y = "Predicted Hg",
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
#ggsave("lake_average_scatter.jpg")

lake_species_compare %>%
  group_by(spec) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = 0.22, linetype = "dashed") +
  facet_wrap(~spec, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")

lake_species_compare %>%
  group_by(spec) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = 0.22, linetype = "dashed") +
  facet_grid(spec~lake_status, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentration") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")
#ggsave("lake_average_loo_compare.jpg", height = 5, width = 6)

#lets filter for where the partial miss to see how viz how badly 
lake_species_compare %>%
  filter(mean_obs <= .lower | mean_obs >= .upper) %>% 
  group_by(spec) %>% 
  arrange(mean_pred) %>%  # Arrange within each species by observed_orig
  mutate(.row = row_number()) %>% 
  ggplot(aes(x = .row)) +
  geom_errorbar(aes(ymin = .lower, ymax = .upper), width = 0.2, color = "lightblue") +
  geom_point(aes(y = mean_pred), color = "lightblue", size = 2, shape = 16) +
  geom_point(aes(y = mean_obs), color = "black", size = 2, shape = 17, alpha = .5) +
  geom_hline(yintercept = 0.22, linetype = "dashed") +
  facet_grid(spec~lake_status, scales = "free") +
  labs(
    x = "Observation",
    y = "Mercury Concentrationn\n(log transformed for display)") +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  guides(color = guide_legend("Legend")) +
  theme(legend.position = "top")
#ggsave("lake_average_loo_misses.jpg", height = 5, width = 6)

#how many times do we estimate it is over?
#here we look at cases where the actual mean wasn't in the intervals
# we look at when the actual value was over and under the interval
# we look at cases where the predicted value was over 0.22 but it was obs under
# we look at cases where the obs
miss <- lake_species_compare %>%
  summarise(sum_under = sum(mean_obs <= .lower),
            sum_over = sum(mean_obs >= .upper),
            pred_error_over = sum(mean_pred >= 0.22 & mean_obs <= 0.22),
            pred_error_under = sum(mean_pred <= 0.22 & mean_obs >= 0.22),
            n = n(),
            prop_error_over = pred_error_over/n,
            prop_error_under = pred_error_under/n)
miss

#clean up work space
rm(metrics, miss, error_metrics, error_metrics_backtransformed, predictions_summary, predictions_summary_original, predictions_summary_spec, predictions_summary_spec_org, lake_species_compare, lake_species_metrics)
```

#visualize predictive metrics
```{r}
# hard coding the data

df <- tribble(
  ~level, ~leave_out_type, ~leave_out_value, ~MAE, ~RMSE, ~coverage, ~interval_width,
  "fish", "whole lake", "no", 0.10, 0.15, 0.91, 0.50,
  "fish", "whole lake", "yes", 0.16, 0.28, 0.98, 0.89,
  "fish", "species lake", "no WAE/NOP", 0.12, 0.19, 0.87, 0.59,
  "fish", "species lake", "WAE/NOP", 0.09, 0.14, 0.93, 0.47,
  "lake-spec average", "whole lake", "no", 0.05, 0.08, 0.78, 0.19,
  "lake-spec average", "whole lake", "yes", 0.14, 0.25, 0.95, 0.63,
  "lake-spec average", "species lake", "no WAE/NOP", 0.08, 0.12, 0.82, 0.31,
  "lake-spec average", "species lake", "WAE/NOP", 0.05, 0.07, 0.77, 0.16
)

df %>%
  pivot_longer(cols = c(MAE, RMSE, coverage, interval_width),
               names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = leave_out_type, y = value, fill = leave_out_value)) +
  geom_col(position = position_dodge()) +
  facet_grid(metric~level, scales = "free_y") +
  theme_minimal() +
  theme(legend.position = "bottom")
#ggsave("predictive_metrics.jpg", height = 8, width = 6)
```

# map of data
```{r}
library(arrow)
library(sf)
library(maps)
#first read in fisheries database to get coordinates
coords <- open_dataset(file.path("D:", 
                                  "Shared drives", 
                                  "Hansen Lab", 
                                  "RESEARCH PROJECTS", 
                                  "Fish Survey Data", 
                                  "Parquet files", 
                                  "hive_update"),partitioning = "state") %>% 
  filter(!is.na(nhdhr_id), !is.na(lake_id), !is.na(latitude_lake_centroid)) %>% 
  distinct(state, nhdhr_id, lake_id, latitude_lake_centroid, longitude_lake_centroid) %>% 
  collect() %>% 
  filter(state == "Minnesota") %>% 
  rename(dow = lake_id) %>% 
  mutate(dow = case_when(str_length(dow) == 7 ~ str_c("0", dow),
                         TRUE ~ dow))

hg.model <- hg.model %>% 
  left_join(coords %>% 
              select(dow, latitude_lake_centroid, longitude_lake_centroid), by = "dow")

hg.model %>% 
  filter(is.na(latitude_lake_centroid)) %>% 
  group_by(dow) %>% 
  count() %>% 
  print(n = nrow(.))
#there are three lakes here that never got a fisheries survey but have gotten mercury sampled, I guess let's just hard code them for now

hg.model <- hg.model %>% 
  mutate(latitude_lake_centroid = case_when(is.na(latitude_lake_centroid) & dow == "19007800" ~ 44.857618,
                                            is.na(latitude_lake_centroid) & dow == "03048900" ~ 46.752876,
                                            is.na(latitude_lake_centroid) & dow == "09003600" ~ 46.689051,
                                            TRUE ~ latitude_lake_centroid),
         longitude_lake_centroid = case_when(is.na(longitude_lake_centroid) & dow == "19007800" ~ -93.19346,
                                             is.na(longitude_lake_centroid) & dow == "03048900" ~ -95.936587,
                                             is.na(longitude_lake_centroid) & dow == "09003600" ~ -92.671952,
                                             TRUE ~ longitude_lake_centroid))

data_for_map <- hg.model %>% 
  group_by(dow, latitude_lake_centroid, longitude_lake_centroid) %>% 
  summarise(n_samples = n(),
            n_spec = n_distinct(spec))

# making map
states <- st_as_sf(map("state", plot = FALSE, fill = TRUE)) %>%
  filter(ID %in% c("minnesota"))

data_for_map <- data_for_map %>%
  st_as_sf(coords = c("longitude_lake_centroid", "latitude_lake_centroid"), 
           crs = 4326) 

ggplot() +
  geom_sf(data = states, fill = "white", color = "black") +
  geom_sf(data = data_for_map, aes(size = n_samples, color = n_spec)) +
  scale_size(name = "Number of Samples") +
  scale_color_viridis_c(option = "plasma", name = "Number of Species") +
  theme_minimal() +
  theme(legend.position = c(.78,.38),
        legend.title.align =  0.5,
        legend.key.height = unit(0.3, "cm"),
        legend.key.width  = unit(0.3, "cm"),
        legend.text = element_text(size = 7),
        legend.title = element_text(size = 8))
#ggsave("map_pred_samples.jpg", height = 6, width = 5, dpi = 600)
rm(states, data_for_map, coords)
```

# Hot Spot Map
1. create dataframe for lakes that we want to predict to 
    -read in dnr covaraite data and join other dataframes
    -filter out any NA values 
    -scale covar to what was modeled
    -change zm_lake to zm_sample so that predictions are for when the lake was invaded
```{r}
########dataframe for lakes we would like to predict########################
dnr_covary <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "MN_lakes_watershed_lulc_fromDNR.csv")) %>% 
  mutate(logit_wetlands = car::logit(Percent_Wetland),
         logit_urban = car::logit(Percent_Urban),
         logit_ag = car::logit(Percent_Cultivated_Crops),
         log_WS_Lake_Ratio = log(WS_Lake_Ratio))

zm <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "ZM_MN_NHD.csv")) %>% 
  filter(!is.na(DOW))  %>% 
  mutate(DOW = as.character(gsub("-", "", DOW))) %>% 
  select(DOW,
         Year_Infested) %>% 
  mutate(zm_lag_year_infested = Year_Infested +2)

glm <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "glm_lake_metadata.csv")) %>%
  filter(state == "MN") %>% 
  rename(nhdid = site_id) %>% 
  mutate(nhdid = gsub("^nhdhr_", "", nhdid)) %>% 
  mutate(clarity = case_when(is.infinite(clarity) ~ NA,
                             TRUE ~ clarity)) %>% 
  mutate(log_area = log(area))

#lake link
lake_link <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "lake_link.csv")) %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

#converting coordinates to sf to get easting and northing
lake_link_sf <- lake_link %>% 
  st_as_sf(coords = c("lake_lon_decdeg", "lake_lat_decdeg"), crs = 4326) %>% 
  st_transform(crs = 32615)
utm_coords <- st_coordinates(lake_link_sf)
lake_link <- lake_link %>%
  mutate(easting = utm_coords[, 1]/1000,
         northing = utm_coords[, 2]/1000)
#utm values in km instead of m
rm(lake_link_sf, utm_coords)
#############################################################################

pred_data <- left_join(dnr_covary, zm)
pred_data <- pred_data %>% 
  local_to_nhdhr(from_colname = "DOW", states = "mn") %>% 
  mutate(nhdid = str_remove(nhdhr.id, "nhdhr_")) %>% 
  select(-nhdhr.id) %>% 
  relocate(nhdid, .after = DOW)
pred_data <- left_join(pred_data, glm)
pred_data <- left_join(pred_data, lake_link)

pred_data <- pred_data %>% 
  select(DOW,
         nhdid,
         Total_Wetland_Hectares,
         Percent_Wetland,
         Percent_Urban,
         Percent_Cultivated_Crops,
         logit_wetlands,
         logit_urban,
         logit_ag,
         WS_Lake_Ratio,
         log_WS_Lake_Ratio,
         centroid_lat,
         centroid_lon,
         max_depth,
         area,
         log_area,
         elevation,
         clarity,
         Year_Infested,
         zm_lag_year_infested,
         lake_lat_decdeg,
         lake_lon_decdeg,
         easting,
         northing) %>% 
  mutate(zm_lake = case_when(is.na(Year_Infested) ~ "n",
                             TRUE ~ "y")) %>% 
  janitor::clean_names()
rm(dnr_covary, glm, lake_link, zm)
##############################################################################
#use scalers to put lakes in original data scale
pred_data <- pred_data %>% 
  filter(ws_lake_ratio < 100) %>% 
  filter(!is.na(area)) %>% 
  #linking data to lat/lon
  mutate(dow = as.factor(dow),
         dow = str_pad(dow, width = 8, pad = "0")) %>% 
  ungroup() %>%
  mutate(
    # lake-level predictors
    scaled_log_area =
      (log_area - lake_scalers$mean_log_area) / lake_scalers$sd_log_area,

    scaled_logit_wetlands =
      (logit_wetlands - lake_scalers$mean_logit_wetlands) / lake_scalers$sd_logit_wetlands,

    scaled_logit_urban =
      (logit_urban - lake_scalers$mean_logit_urban) / lake_scalers$sd_logit_urban,

    scaled_logit_ag =
      (logit_ag - lake_scalers$mean_logit_ag) / lake_scalers$sd_logit_ag,

    scaled_log_ws_lake_ratio =
      (log_ws_lake_ratio - lake_scalers$mean_lw_ratio) / lake_scalers$sd_lw_ratio
  )

pred_spec <- tibble(spec = c("WAE",
                             "NOP",
                             "BLC",
                             "LMB",
                             "BLG",
                             "WTS",
                             "YEP"),
                    length = c(15,
                               24,
                               8.1,
                               12,
                               6.1,
                               12,
                               8.1)) %>% 
  left_join(length_scalers,by = "spec") %>% 
  mutate(scaled_length = (length - length_mean)/length_sd)
pred_spec

#cross the datasets
pred_full <- crossing(pred_data, pred_spec)
rm(pred_data, pred_spec)

pred_full <- pred_full %>% 
  rename(zm_sample = zm_lake)
#############################################################################
#predicting at the lake-spec level 
lake_spec_pred <- pred_full %>% 
  add_predicted_draws(fit, ndraws = 500, allow_new_levels = TRUE)

#here we have 500 draw for each row because there is only one length within each lake-spec
lake_spec_sum <- lake_spec_pred %>% 
  group_by(dow, spec) %>% 
  median_qi(.prediction) %>% 
  select(-.width, -.point, -.interval)

#create four grouping for "risk" based on where the intervals and average sit relative to 0.22
lake_spec_org <- lake_spec_sum %>% 
  mutate(.prediction = exp(.prediction),
         .lower = exp(.lower),
         .upper = exp(.upper)) %>% 
  mutate(risk = case_when(.lower >= 0.22 ~ "high",
                          .prediction >= 0.22 & .lower < 0.22 ~ "worth a check",
                          .prediction <= 0.22 & .upper > 0.22 ~ "yellow flag",
                          .upper <= 0.22 ~ "low",
                          TRUE ~ NA)) %>% 
  local_to_nhdhr(from_colname = "dow", states = "mn") %>% 
  mutate(nhdid = str_remove(nhdhr.id, "nhdhr_")) %>% 
  select(-nhdhr.id)

lake_link <- read_csv(file.path("D:", "Shared drives", "Hansen Lab", "RESEARCH PROJECTS", "Statewide mercury modeling", "Data", "Covariate Data", "lake_link.csv")) %>% 
  distinct(lake_nhdid, .keep_all = T) %>% 
  rename(nhdid = lake_nhdid)

lake_spec_org <- lake_spec_org %>% 
  left_join(lake_link)
rm(lake_link, pred_full, lake_spec_pred, lake_spec_sum)

###############################################################################
#summary stuff
lake_spec_org %>% 
  group_by(spec) %>% 
  summarise(high = sum(risk == "high"),
            high_med = sum(risk == "worth a check"),
            low_med = sum(risk == "yellow flag"),
            low = sum(risk == "low"),
            n = n())

############################################################################
#map
lake_sf <- lake_spec_org %>%
  ####FILTERING OUT FOR NOW BUT I THINK THERE IS CODE ABOVE TO FIX
  filter(!is.na(lake_lat_decdeg)) %>% 
  st_as_sf(coords = c("lake_lon_decdeg", "lake_lat_decdeg"), crs = 4326) %>% 
  mutate(species = case_when(spec == "BLC" ~ "Black Crappie",
                             spec == "BLG" ~ "Bluegill",
                             spec == "LMB" ~ "Largemouth Bass",
                             spec == "NOP" ~ "Northern Pike",
                             spec == "WAE" ~ "Walleye",
                             spec == "YEP" ~ "Yellow Perch",
                             spec == "WTS" ~ "White Sucker")) %>% 
  mutate(risk = as.factor(risk),
         risk = factor(risk, levels = c("high", "yellow flag", "worth a check", "low")))

states <- st_as_sf(map("state", plot = FALSE, fill = TRUE)) %>%
  filter(ID %in% c("minnesota"))

ggplot() +
  geom_sf(data = states, fill = "white", color = "black") +
  geom_sf(
    data = lake_sf,
    aes(
      color = .prediction,
      shape = risk
    ),
    size = 2
  ) +
  scale_shape_manual("Risk for Consumption", values = c(15, 16, 17, 18)) +
  facet_wrap(~ species) +
  scale_color_viridis_c(option = "plasma",
                        name = "Mercury Concentration (ppm)") +
  theme_minimal() +
  theme(legend.box = "horizontal",
    legend.position = c(.65, .15),
    legend.key.size = unit(0.4, "lines"),
    legend.title = element_text(size = 6),
    legend.text  = element_text(size = 7),
    legend.spacing.x = unit(0.2, "cm"),
    legend.spacing.y = unit(0.2, "cm"))
ggsave("prediction_map_all_lakes.jpg", height = 5, width = 5, dpi = 600)

############################################################################
lake_spec_org %>% 
  group_by(spec, risk) %>% 
  summarise(n_lakes = n()) %>% 
  mutate(species = case_when(spec == "BLC" ~ "Black Crappie",
                             spec == "BLG" ~ "Bluegill",
                             spec == "LMB" ~ "Largemouth Bass",
                             spec == "NOP" ~ "Northern Pike",
                             spec == "WAE" ~ "Walleye",
                             spec == "YEP" ~ "Yellow Perch",
                             spec == "WTS" ~ "White Sucker")) %>% 
  mutate(risk = as.factor(risk),
         risk = factor(risk, levels = c("high", "yellow flag", "worth a check", "low"))) %>% 
  ggplot() +
  geom_col(aes(x = risk, y = n_lakes, fill = risk)) +
  labs(y = NULL,
       x = NULL) +
  scale_fill_manual(values = c("red", "yellow", "lightgreen", "green")) +
  facet_wrap(~species) +
  theme_minimal() +
  theme(legend.position = c(.65, .1),
        axis.text.x = element_text(angle = 45, hjust = 1))
ggsave("predicted_risk_count.jpg", height = 5, width = 5, dpi = 600)

lake_spec_org %>% 
  group_by(spec, risk) %>% 
  summarise(n_lakes = n()) %>% 
  mutate(species = case_when(spec == "BLC" ~ "Black Crappie",
                             spec == "BLG" ~ "Bluegill",
                             spec == "LMB" ~ "Largemouth Bass",
                             spec == "NOP" ~ "Northern Pike",
                             spec == "WAE" ~ "Walleye",
                             spec == "YEP" ~ "Yellow Perch",
                             spec == "WTS" ~ "White Sucker")) %>% 
  mutate(risk = as.factor(risk),
         risk = factor(risk, levels = c("high", "yellow flag", "worth a check", "low"))) %>% 
  ggplot() +
  geom_col(aes(x = risk, y = n_lakes, fill = species)) +
  labs(x = NULL,
       y = NULL) +
  theme_minimal() +
  theme(legend.position = c(.1, .7))
ggsave("risk_by_species.jpg", height = 5, width = 5, dpi = 600)
```

# fun summaries and plots we can make:
-risk by species (based on where the intervals and median sits)
-clean up the map
- compare how our predicted risk compares to consumption guidelines